{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# castorama.py\n",
    "\n",
    "#Фото со страницы получилось скачать только одно, правда, большое) Все остальные\n",
    "# ссылки на странице товара,ведущие на фотографии, в браузере выглядят нормально, а в scrapy ссылка превращается\n",
    "# в нечто вида \"data:image/png base64 , набор символов\". Это каcается всех ссылок на фото с тегом\n",
    "#src. Единственная работающая ссылка такого тега не имеет. Какие пути в scrapy не подставляй,\n",
    "#если в конце этого пути будет src - ссылка не обрабатывается((. Возможно, сайт блокирует?\n",
    "\n",
    "import scrapy\n",
    "from scrapy.http import HtmlResponse\n",
    "from cs.items import CsparserItem\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "\n",
    "class CsSpider(scrapy.Spider):\n",
    "    name = 'castorama'\n",
    "    allowed_domains = ['castorama.ru']\n",
    "\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.start_urls = [f\"https://www.castorama.ru/catalogsearch/result/?q={kwargs.get('query')}\"]\n",
    "\n",
    "    def parse(self, response: HtmlResponse):\n",
    "        next_page = response.xpath('//a[@class=\"product-list-show-more__link js-product-list-show-more\"]/@href').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "        links = response.xpath(\"//a[@class='product-card__img-link']\")\n",
    "        for link in links:\n",
    "            yield response.follow(link, callback=self.parse_ads)\n",
    "\n",
    "\n",
    "    def parse_ads(self, response: HtmlResponse):\n",
    "        loader = ItemLoader(item=CsparserItem(), response=response)\n",
    "        loader.add_xpath('name', '//h1[@itemprop=\"name\"]/text()')\n",
    "        loader.add_xpath('price', \"//span[@class='price']/span/span/text()\")\n",
    "        loader.add_xpath('photos', '//span[@itemprop = \"image\"]/@content')\n",
    "        loader.add_xpath('specs', '//div//*[contains(@class, \"specs-table__attribute\")]/text()')\n",
    "        loader.add_value('url', response.url)\n",
    "        yield loader.load_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items.py\n",
    "\n",
    "import scrapy\n",
    "from itemloaders.processors import MapCompose, TakeFirst\n",
    "\n",
    "\n",
    "def clear_price(value):\n",
    "    if value:\n",
    "        value = value.replace(' ','')\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except:\n",
    "            return value\n",
    "        return value\n",
    "\n",
    "def clear_specs(value):\n",
    "    value = [''.join(el.replace('\\n', ' ') for el in value)]\n",
    "    value = [el.strip() for el in value if el.isspace() == False]\n",
    "    #Хотел сделать список более читабельным, соединив в одной строке признак и его значение,\n",
    "    #попарно объединив элементы в списке. Однако почему-то ни одна из команд под комментарием\n",
    "    #в mapcompose не работает, хотя если их применить независимо, они дают нужный результат.\n",
    "    #В чем же может быть дело?\n",
    "\n",
    "    #value = [value[i] + ' - ' + value[i + 1] for i in range(0, len(value), 2)]\n",
    "\n",
    "    #si = iter(value)\n",
    "    #value = list(map(' - '.join, zip(si, si)))\n",
    "    list(range(0, len(value), 2))\n",
    "    return value\n",
    "\n",
    "\n",
    "class CsparserItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    name = scrapy.Field(output_processor=TakeFirst())\n",
    "    price = scrapy.Field(input_processor=MapCompose(clear_price), output_processor=TakeFirst())\n",
    "    url = scrapy.Field(output_processor=TakeFirst())\n",
    "    photos = scrapy.Field()\n",
    "    specs = scrapy.Field(input_processor=MapCompose(clear_specs))\n",
    "    _id = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "import scrapy\n",
    "from itemadapter import ItemAdapter\n",
    "from scrapy.pipelines.images import ImagesPipeline\n",
    "\n",
    "class CsparserPipeline:\n",
    "     def process_item(self, item, spider):\n",
    "         return item\n",
    "\n",
    "\n",
    "class CsPhotosPipeline(ImagesPipeline):\n",
    "    def get_media_requests(self, item, info):\n",
    "        if item['photos']:\n",
    "            for img in item['photos']:\n",
    "                try:\n",
    "                    yield scrapy.Request(img)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    def item_completed(self, results, item, info):\n",
    "        item['photos'] = [itm[1] for itm in results if itm[0]]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner.py\n",
    "\n",
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "from cs.spiders.castorama import CsSpider\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    configure_logging()\n",
    "    settings = get_project_settings()\n",
    "    runner = CrawlerRunner(settings)\n",
    "\n",
    "    runner.crawl(CsSpider, query='стул')\n",
    "\n",
    "    reactor.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "\n",
    "# Scrapy settings for avitoparser project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'castorama'\n",
    "\n",
    "IMAGES_STORE = 'photos'\n",
    "#IMAGES_THUMBS = {\n",
    "   #'medium': (55, 35)\n",
    "#}\n",
    "\n",
    "\n",
    "SPIDER_MODULES = ['cs.spiders']\n",
    "NEWSPIDER_MODULE = 'cs.spiders'\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "CONCURRENT_REQUESTS = 8\n",
    "\n",
    "LOG_ENABLED = True\n",
    "LOG_LEVEL = \"DEBUG\"\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "DOWNLOAD_DELAY = 1.5\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "COOKIES_ENABLED = True\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "# DEFAULT_REQUEST_HEADERS = {\n",
    "#   'cookie':'yandexuid=1259160151646070856; yuidss=1259160151646070856; yabs-sid=1937487681646070856; ymex=1961430856.yrts.1646070856#1961430856.yrtsi.1646070856; gdpr=0; _ym_uid=1646070893156269093; my=YwA=; amcuid=8340317491646422388; yandex_login=Orleon.ya; i=wdTJiryHb+LLnT5mcRSlLhkbNWoRkyWlPsQzRYQZNAx4MhNo6TM/BV8R51m0d0jIkpBJv0xmNtTmxeXkiTiWht9wDgw=; is_gdpr=0; is_gdpr_b=CNaZZBDuaigC; yandex_gid=44; cycada=Pu5H3ElFTWHQAAU7lrMwAL/hvOtk0vJNciYyzRBtN1E=; _ym_d=1649519883; yabs-frequency=/5/0000000000000000/LVTwO9j8Vb84HY6q2tDmb00000H68M-bN1mQii9h14Od8krF10oancu4HYC0/; yp=1665287886.szm.1_25:2560x1440:1990x1072#1962472582.udn.cDpPcmxlb24ueWE%3D#1651771282.ygu.1#1651771283.spcs.l#1651857692.csc.1; ys=udn.cDpPcmxlb24ueWE%3D#c_chck.8048816; Session_id=3:1649696374.5.0.1647112582511:GiqSWw:25.1.2:1|271833109.0.2|3:250771.178521.H07E8OxfqD1kCKQ9zmuC7qex_qY; sessionid2=3:1649696374.5.0.1647112582511:GiqSWw:25.1.2:1|271833109.0.2|3:250771.178521.H07E8OxfqD1kCKQ9zmuC7qex_qY'\n",
    "# }\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'avitoparser.middlewares.AvitoparserSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'avitoparser.middlewares.AvitoparserDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "   'cs.pipelines.CsparserPipeline': 300,\n",
    "   'cs.pipelines.CsPhotosPipeline': 200\n",
    "}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
