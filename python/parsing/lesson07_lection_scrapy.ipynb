{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90900f0",
   "metadata": {},
   "source": [
    "#  Фреймворк Scrapy. Скачивание файлов и фото\n",
    "\n",
    "C помощью модуля `requests` можно скачивать файлы. Помимо скрапи есть более лёгкие методы скачать файлы с помощью которых можно достаточно эффективно решить данную задачу. Импортируем модуль. Он умеет делать гет запросы. Процесс получения ответа от сервера практически идентичен скачиванию файла с какого либо ресурса. В случае, когда мы качаем файл мы скачиваем данные в этот файл. Когда мы скачиваем страницу - мы скачиваем html файл, просто он раскрывается в нашем браузере и мы получаем содержимое страницы. \n",
    "Делаем мы запрос на урл или на ссылку которая приведёт к скачиванию файла схожие процессы, технически никакой разницы нет. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93791e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41672519",
   "metadata": {},
   "source": [
    "Идём на яндекс картинки, выбираем понравившуюся, ПКМ - копировать адрес ссылки. https://kartinkin.net/uploads/posts/2022-12/1671751945_kartinkin-net-p-kot-kosmonavt-kartinki-krasivo-10.jpg \n",
    "откроем в отдельном окне. \n",
    "\n",
    "У открытой странички есть ссылка на неё. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec95e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://kartinkin.net/uploads/posts/2022-12/1671751945_kartinkin-net-p-kot-kosmonavt-kartinki-krasivo-10.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae2b03",
   "metadata": {},
   "source": [
    "Выполняем гет запрос через модуль `request` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547c6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579590be",
   "metadata": {},
   "source": [
    "Что дальше делать с этим респонсом. По факту в этом респонсе будет находиться тот самый скаченный файл, в нашем случае это картинка. И нам это нужно превратит в некоторое физическое проявление. \n",
    "\n",
    "Воспользуемся стандартной процедурой сохранения данных в файл. Открываем файл на запись причём в бинарном режиме. Мы будем писать сюда не текстовые данные а последовательность нулей и единиц, которые будут хрониться в нашем респонсе. Т.к открываем в бинарном режиме, кодировка никакая не нужна. \n",
    "\n",
    "Внутри функции `open` мы возьмём наш файл, вызовим метод `write()` и в него добавим содержимое респонса, но не текст, а контент? который содержит бинарное содержание контента и выполнение гет запроса на указанную ссылку. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66fad8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('image.jpg', 'wb') as picture:\n",
    "    picture.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6871a3",
   "metadata": {},
   "source": [
    "При наличии прямой ссылки на файл скачиваем его без проблем. Но у данного продемонстрированного подхода есть 2 недостатка:\n",
    "\n",
    "- наш файл целиком и полностью грузится в оперативку - response. И пока он дойдёт от `response = requests.get(url)` до `  picture.write(response.content)` он провисает в оперативке. И если файлов много скачивается в потоках оперативка может забиться. \n",
    "\n",
    "- мы делаем фиксированное имя файла. 'image.jpg'. И мы должны заботиться о том, чтобы это имя файла постоянно как то менялось, иначе будем затирать старое новым. \n",
    "\n",
    "Для решения проблем можем воспользоваться встроенными средствами. Напрмер скачивание файла по частям. Для имени распарсить ссылку и брать имя картинки оттуда. Можем воспользоваться модулем `wget`. Но изначально его нет, его надо установить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb8aa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9682 sha256=f6d2658b14e138a70f0ec07d3fba5cdb8c4b0adad1fc9ea3f1ecfd6f93696ca0\n",
      "  Stored in directory: c:\\users\\777\\appdata\\local\\pip\\cache\\wheels\\bd\\a8\\c3\\3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2a42225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04f750",
   "metadata": {},
   "source": [
    "Этот модуль предоставляет различные возможности. Воспользуемся методом download(). Он сохранит картинку с тем же именем, которое она имеет в ссылке. Также метод автоматически делит файлы на несколько частей, то есть для большого файла используется собственный алгоритм и он не закидывает целиком респонс в оперативку. Файл полюбому движится с сервера в оперативку, с оперативки в жёсткий диск. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b756c592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1671751945_kartinkin-net-p-kot-kosmonavt-kartinki-krasivo-10.jpg'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2cc5c9",
   "metadata": {},
   "source": [
    "### Для скрапи. создали проект. Создался каркас из файлов. \n",
    "\n",
    "В файле `settings.py`:\n",
    "`BOT_NAME = 'avitoparser'` - название нашего проекта.\n",
    "\n",
    "`ROBOTSTXT_OBEY = False` - роботы выключены. \n",
    "\n",
    "`ITEM_PIPELINES =` - пайплайн один включен.\n",
    "\n",
    "В самом пауке `spiders/avito.py` классический набор\n",
    "\n",
    "```\n",
    "class AvitoSpider(scrapy.Spider):\n",
    "    name = 'avito'\n",
    "    allowed_domains = ['avito.ru']\n",
    "    start_urls = [f'https://www.avito.ru']\n",
    "```\n",
    "\n",
    "в пайплайнах `pipelines.py` тоже пустота:\n",
    "```\n",
    "class AvitoparserPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        print()\n",
    "        return item\n",
    "```\n",
    "\n",
    "Раннер `runner.py` настроенный для одного паука \n",
    "```\n",
    "if __name__ == '__main__':\n",
    "    configure_logging() \n",
    "    \n",
    "# settings импортируется посредствам вызова функции. \n",
    "# когда мы вызываем функцию get_project_settings()\n",
    "# эта функция настроена на поиск в нашем виртуальном окружении \n",
    "# файла settings.py и импорт его внутрь нашего проекта, внутрь паука\n",
    "    \n",
    "    settings = get_project_settings()\n",
    "    runner = CrawlerRunner(settings)\n",
    "    \n",
    "    runner.crawl(AvitoSpider, query='bmw')\n",
    "\n",
    "    reactor.run()\n",
    "```\n",
    "\n",
    "\n",
    "Стандартный набор с которого мы начинаем нашу работу. \n",
    "Работать будем с сайтом Avito.ru. Наша задача автоматизировать сбор данных плюс сверху скачать фотографии ещё. Причём скачать так, чтобы скаченные данные были связаны с фотографиями. Чтобы получая информацию об одном объекте скаченном получали информацию и фотки. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0bd8b7",
   "metadata": {},
   "source": [
    "Начинаем всегда с того, что переходим на сайт, в нашем случае avito - вводим что ищем (например iphone), открывается страница с результатами поиска - это и будет наша точка входа. вставляем эту точка входа в файл `spiders/avito.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls = [f'https://www.avito.ru/moscow?q=iphone']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb7c39",
   "metadata": {},
   "source": [
    "Попробуем улучшить. `?q=iphone` - параметр зашит в ссылку и получается приложение настроено только на поиск iphone. А нам хочется передавать параметр поиска через какую нибудь переменную, при чтении из окна интерфейса или из стороннего файла. То есть влиять на работу приложения посредствам ввода запроса снаружи. Поэтому изменим начальную структуру нашего паука. \n",
    "\n",
    "Нам необходимо будет понять как внутрь класса `AvitoSpider` можем передать какие нибудь параметры. При создании объекта класса как передавать внутрь параметры? Что должно быть в самом классе, чтобы можно было снаружи параметры принимать и сохронять в свойствах объекта. НУЖЕН КОНСТРУКТОР. Здесь у spiders/avito.py конструктора нет, \n",
    "но мы наследуемся от класса `scrapy.Spider` можно в ИДЕ с зажатой клавишей CTRL перейти в этот класс (навелись мышкой на `scrapy.Spider` и с контролом переходим в код с классом.) и видим что в родительском классе конструктор всё таки есть. `def __init__(self, name=None, **kwargs)`. \n",
    "\n",
    "Внутри нашего класса `AvitoSpider(scrapy.Spider)` можем тоже определить этот самый конструктор `def __init__(self, )` и здесь в конструкторе мы создадим параметры, которые будут приниматься снаружи. Пока сделаем его по умолчанию. Если мы ничего не передадим будем искать то что указано по умолчанию. `def __init__(self, query = 'iphone'):`. Можно сделать и kwargs `def __init__(self, **kwargs):`, чтобы можно было передавать любой параметр. Если мы наследуемся от родительского класса и у него есть конструктор, мы его должны переопределять в дочернем классе. Поэтому внутрь класса обязательно `super().__init__(**kwargs)`\n",
    "\n",
    "```\n",
    "def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "```\n",
    "\n",
    "Теперь логика не нарушена, инициализация какой была такой и осталась. Внутри самого конструктора добавляем свой собственный функционал, ради которого мы и решили переопределить конструктор. В чвстности нам нужно через kwargs получить значение поиска который передаётся в `start_urls = [f'https://www.avito.ru/moscow?q=iphone']`. Поэтому мы `start_urls =` из атрибутов класса превратим в свойства будущих объектов. Мы убираем его из `class AvitoSpider` и вставляем внутрь конструктора через self. а также делаем f строку и вставляем kwargs `self.start_urls = [f'https://www.avito.ru/moscow?q={kwargs.get('query')}']` В параметре query будет значение для строки поиска. \n",
    "\n",
    "Как мы в чужом фреймворке, в чужом коде где мы сможем передать параметр query внутрь класса `AvitoSpider(scrapy.Spider)`. query стал обязательным параметром и мы теперь не сможем создать объект класса `AvitoSpider` без передачи параметра query внутрь. \n",
    "\n",
    "Все параметры которые лягут в kwargs мы указываем при инициализации объекта методом crawl в runner.py. `runner.crawl(AvitoSpider, query='bmw')`. \n",
    "\n",
    "Теперь параметр query получает значение и в момент создания объекта класса паука в конструкторе мы инициализируем свойство `start_urls =` который принимает в себя этот самый параметр query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c222bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/avito.py\n",
    "\n",
    "import scrapy\n",
    "from scrapy.http import HtmlResponse\n",
    "from avitoparser.items import AvitoparserItem\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "class AvitoSpider(scrapy.Spider):\n",
    "    name = 'avito'\n",
    "    allowed_domains = ['avito.ru']\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.start_urls = [f'https://www.avito.ru/moscow?q={kwargs.get('query')}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16096134",
   "metadata": {},
   "source": [
    "Запустили через раннер в режиме отладки. Посмотрели как отрабатывает паук при ручном запуске через ранер. Дальше нужно писать логику сбора данных в файле `spiders/avito.py` создаём метод `def parse()`\n",
    "мы проходим по ссылке `https://www.avito.ru/moscow?q={kwargs.get('query')}` и теперь нам нужно собрать ссылки на товары,  которые найдены на странице, чтобы войти внутрь каждого объявления. \n",
    "\n",
    "Через инструменты разработчика инспектируем элемент. у нас есть тег а в котором находится ссылка, также у тега находим атрибут 'data-maker'='item-title'. Составляем икс пас и проверяем `//a[@data-marker='item-title']`. \n",
    "\n",
    "Теперь в методе parse нашего класса создаём переменную, куда поместим список ссылок, который получим через Xpath. \n",
    "`links = response.xpath(\"//a[@data-marker='item-title']\")`. Получили контейнеры теги, хронящие в себе информацию. По хорошему чтобы получить ссылку мы должны писать `links = response.xpath(\"//a[@data-marker='item-title']/@href\")` потом должы написать метод getall() чтобы эти ссылки извлечь и превратить в список данных. `links = response.xpath(\"//a[@data-marker='item-title']/@href\").getall()`. Но в случае с сылками в скрапи есть одна очень интересная настройка. Мы можем передовать в методы для выполнения запросов не конечные финальные ссылки, а объекты которые их содержат. Скрапи умеет сам заходить внутрь и извлекать значение атрибута href.  И вот у нас на руках объекты со ссылками `links = response.xpath(\"//a[@data-marker='item-title']\")` и мы будем итеррироваться по этим объектам так, как будто они уже являются ссылками. \n",
    "```\n",
    "for link in links:\n",
    "    yield response.follow(link, callback=self.parse_ads)\n",
    "```\n",
    "создадим ещё один метод parse_ads `def parse_ads(self, response:HTMLResponse)` для того, чтобы заходить внутрь объявления. В верхнем методе мы используем генератор, для того чтобы возвращались нам результаты и мы шли к следующей ссылке.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48399741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/avito.py\n",
    "\n",
    "    def parse(self, response:HtmlResponse):\n",
    "        links = response.xpath(\"//a[@data-marker='item-title']\")\n",
    "        for link in links:\n",
    "            yield response.follow(link, callback=self.parse_ads)\n",
    "            \n",
    "    def parse_ads(self, response:HTMLResponse):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bcbe9f",
   "metadata": {},
   "source": [
    "Мы зашли внутрь объявления. Дальше собираем информацию внутри объявления. Наименование товара. Опять же инспектируем икс пас.\n",
    "```\n",
    "<h1 class=\"style-title-info-title-eHW9V style-title-info-title-text-CoxZd\"><span itemprop=\"name\" class=\"title-info-title-text\" data-marker=\"item-view/title-info\">Hyundai Solaris, 2014</span></h1>\n",
    "``` \n",
    "для нимменования товара должны взять текст тега спан, находящегося в теге h1. `name = response.xpath(\"//h1/span/text()\").get()`.\n",
    "\n",
    "Инспектируем цену\n",
    "```\n",
    "<span content=\"770000\" itemprop=\"price\" class=\"js-item-price style-item-price-text-_w822 text-text-LurtD text-size-xxl-UPhmI\">770&nbsp;000</span>\n",
    "```\n",
    "Видим что цена содержится в атрибуте тега content=\"770000\". \n",
    "дальше берём цену товара `price = response.xpath(\"//span[@itemprop='price'/@content]\").get()`.\n",
    "Но можно также взять и из текста `price = response.xpath(\"//span[@itemprop='price'/text()]\").get()`,\n",
    "но эти данные немного грязные. пробел в виде &nbsp; нужно будет дополнительно обрабатывать. \n",
    "\n",
    "Добавим также ссылку на объявление `url = response.url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12026ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def parse_ads(self, response:HTMLResponse):\n",
    "        name = response.xpath(\"//h1/span/text()\").get()\n",
    "        price = response.xpath(\"//span[@itemprop='price'/@content]\").get()\n",
    "        url = response.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848700d2",
   "metadata": {},
   "source": [
    "Следующий элемент, который нам надо собрать это фотографии товара. \n",
    "`photos = response.xpath(//)` чтобы сформировать правильный икс пас нам надо понимать как устроены галереии в вёрстке. \n",
    "Практически все галереии устроены одинаково. У нас есть мелкие картинки внизу в полном объёме представленные, и только одна большая картинка наверху. Проинспектируем элемент и посмотрим как это всё выглядит в коде. \n",
    "```\n",
    "<div class=\"gallery-root-n3_HK\" data-marker=\"item-view/gallery\"><div class=\"image-frame-root-vKeXJ\"><div class=\"image-frame-borderWrapper-wCbtU\"><div class=\"image-frame-controlButtonArea-_3TO9 image-frame-controlButton_left-i5XEe\" data-delta=\"-1\" data-marker=\"image-frame/left-button\"><button class=\"image-frame-controlButton-_vPNK\"></button></div><div class=\"image-frame-controlButtonArea-_3TO9 image-frame-controlButton_right-HeBIM\" data-delta=\"1\" data-marker=\"image-frame/right-button\"><button class=\"image-frame-controlButton-_vPNK\"></button></div><div class=\"image-frame-wrapper-_NvbY\" data-title=\"Фитолампа на полный цикл от 150Вт до 300Вт\" data-url=\"https://70.img.avito.st/image/1/1.NqqfYba4mkOpyFhG_xFYqq3DmEUhwBhL6cWYQS_Ikkkp.4G3SgDSvy8V6UgjJPuhP8xh4LRcHYBYibxfs77-DeWI\" data-image-id=\"0\" data-marker=\"image-frame/image-wrapper\"><span class=\"image-frame-cover-lQG1h\" style=\"background-image:url(https://70.img.avito.st/image/1/1.NqqfYba4mkOpyFhG_xFYqq3DmEUhwBhL6cWYQS_Ikkkp.4G3SgDSvy8V6UgjJPuhP8xh4LRcHYBYibxfs77-DeWI)\"></span><img src=\"https://70.img.avito.st/image/1/1.NqqfYba4mkOpyFhG_xFYqq3DmEUhwBhL6cWYQS_Ikkkp.4G3SgDSvy8V6UgjJPuhP8xh4LRcHYBYibxfs77-DeWI\" alt=\"Фитолампа на полный цикл от 150Вт до 300Вт\" class=\"desktop-1ky5g7j\"></div><div class=\"image-frame-hidden-XSrK5\"><button class=\"videoPlayer-button-CQ5Qh buttonSizeS\"></button><div class=\"videoPlayer-hidden-UR5_1\"></div></div></div></div><ul class=\"images-preview-previewWrapper-R_a4U images-preview-previewWrapper_newStyle-fGdrG\" data-marker=\"image-preview/preview-wrapper\"><li class=\"images-preview-previewImageWrapper-RfThd images-preview-previewImageWrapper_selected-OgdIL images-preview-previewImageWrapperWithPointer-NioZh\" data-marker=\"image-preview/item\" data-index=\"0\" data-type=\"image\"><img src=\"https://70.img.avito.st/image/1/1.NqqfYba3mkO_w_aX-gJzxSjCmkkhVJkvKcI.P6Kcq8tPOZ6ca1qz2TA7N-_EZwHWaRMeh4DtwAZG88A\" alt=\"Фитолампа на полный цикл от 150Вт до 300Вт\" srcset=\"\" class=\"desktop-1i6k59z\"></li><li class=\"images-preview-previewImageWrapper-RfThd images-preview-previewImageWrapperWithPointer-NioZh\" data-marker=\"image-preview/item\" data-index=\"1\" data-type=\"image\"><img src=\"https://70.img.avito.st/image/1/1.i86u67a3JyeOSUuJzYjOoRlIJy0Q3iRLGEg.VVdj-JZpB0z0SU1YctctOHbUyN2cpjopC1iLQYrSyow\" alt=\"Фитолампа на полный цикл от 150Вт до 300Вт\" srcset=\"\" class=\"desktop-1i6k59z\"></li><li class=\"images-preview-previewImageWrapper-RfThd images-preview-previewImageWrapperWithPointer-NioZh\" data-marker=\"image-preview/item\" data-index=\"2\" data-type=\"image\"><img src=\"https://20.img.avito.st/image/1/1.YKHGM7a3zEjmkaCwolAlznGQzEJ4Bs8kcJA.XKm8ftmBU8pFA4yGZwX1cTG-lQPSMZo9j6ObF7wV5mM\" alt=\"Фитолампа на полный цикл от 150Вт до 300Вт\" srcset=\"\" class=\"desktop-1i6k59z\"></li></ul></div>\n",
    "```\n",
    "Ссылки на маленькие картинки нам не интересны, оны не несут много информации из-за разрешения картинок. \n",
    "Инспектируем одну большую картинку. Идея собрать маленькие картинки под галереей сразу прогорает. \n",
    "```\n",
    "<img src=\"https://70.img.avito.st/image/1/1.NqqfYba4mkOpyFhG_xFYqq3DmEUhwBhL6cWYQS_Ikkkp.4G3SgDSvy8V6UgjJPuhP8xh4LRcHYBYibxfs77-DeWI\" alt=\"Фитолампа на полный цикл от 150Вт до 300Вт\" class=\"desktop-1ky5g7j\">\n",
    "```\n",
    "Наша цель не столько скачать картинки изначально, сколько получить ссылки на эти картинки. А скачать дальше это уже читсо технический несложный процесс. \n",
    "Выбирем картинку из галереии снизу, она станет полноразмерной и проинспектируем этот элемент.\n",
    "```\n",
    "<img src=\"https://20.img.avito.st/image/1/1.YKHGM7a4zEjwmg5NikIOofSRzk54kk5AsJfOSnaaxEJw.1AtxMPtL-3_KE2ab8mK04mUgxFu25E8OihFSx9T2VQA\" alt=\"Фитолампа на полный цикл от 150Вт до 300Вт\" class=\"desktop-1ky5g7j\">\n",
    "```\n",
    "идём вверх и смотрим в каком контейнере она лежит. div контейнер class=\"image-frame-wrapper-_NvbY\".\n",
    "\n",
    "Сейчас картинки подгружаются с помощью джаваскрипт. Поэтому сейчас их собрать становится проблематично. При наведении мышкой на картинку содержение одного и того же тега меняется. Динамическое подгружение. \n",
    "\n",
    "На примере маленьких картинок икс пас:`//li[contains(@class,'images-priview')]/img/@src` нам нужно взять все картинки .getall()\n",
    "получим ссылки, ведущие на маленькие картинки \n",
    "`photos = response.xpath(\"//li[contains(@class,'images-priview')]/img/@src\").getall()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dcc439",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def parse_ads(self, response:HTMLResponse):\n",
    "        name = response.xpath(\"//h1/span/text()\").get()\n",
    "        price = response.xpath(\"//span[@itemprop='price'/@content]\").get()\n",
    "        url = response.url\n",
    "        photos = response.xpath(\"//li[contains(@class,'images-priview')]/img/@src\").getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ed38f",
   "metadata": {},
   "source": [
    "Можно подключить селениум в скрапи и уже с помощью него подгружать динамические данные и дальше уже собирать. Прощёлкать картинки, они прогрузятся и собрать их уже полноразмерные. \n",
    "\n",
    "Дальше делаем импорт `from avitoparser.items import AvitoparserItem` и yield для сохранения порциями данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/avito.py\n",
    "\n",
    "import scrapy\n",
    "from scrapy.http import HtmlResponse\n",
    "from avitoparser.items import AvitoparserItem\n",
    "\n",
    "# ...\n",
    "\n",
    "    def parse_ads(self, response:HTMLResponse):\n",
    "        name = response.xpath(\"//h1/span/text()\").get()\n",
    "        price = response.xpath(\"//span[@itemprop='price'/@content]\").get()\n",
    "        url = response.url\n",
    "        photos = response.xpath(\"//li[contains(@class,'images-priview')]/img/@src\").getall()\n",
    "        yield AvitoparcerItem(name=name, price=price, url=url, photos=photos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6ea77",
   "metadata": {},
   "source": [
    "И не забываем в items добавить соответствующие поля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2abc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items.py\n",
    "\n",
    "class AvitoparserItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    name = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    photos = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "\n",
    "class AvitoparserPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        print()\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2dd73",
   "metadata": {},
   "source": [
    "Айтомы все скомплектованы, данные все записаны и нам теперь необходимо скачать файлы по представленным в photos ссылкам. В скрапи есть отдельно созданный функционал для этих целей. Этот функционал заложен внутри отдельного pipeline. Пайплайнов несколько для работы с файлами, мы будем пользоваться пайплайном для работы с фотографиями. \n",
    "\n",
    "Из соответствующего раздела пайплайнс `from scrapy.pipelines.images import ImagesPipeline`. Теперь создадим класс - ещё один пайплайн. Это нормальный подход, когда есть несколько пайплайнов для обработки наших айтомов.  Удобно с точки зрения структурности, потому что всё разложено по полочкам, функциональность обработки данных не смешивается и в целом для дальнейшей масштабируемости проекта удобно.  создаём класс `class AvitoPhotoPipeline(ImagesPipeline)` и чтобы не придумывать свой собственный функционал будем наследоваться от `ImagesPipeline`. отдельный класс созданный на базе ImagesPipeline даст нам возможность использоовать его методы. \n",
    "\n",
    "У нашего пайплайна есть точка входа - это `process_item`. У любого пайплайна должен быть этот самый process_item. Исключения составляют мультимидийные пайплайны, у которых точка входа немного другая. она называется get_media_requests. Вызывается этот метод самым первым и внутри этого метода мы должны написать логику сбора фотографий. ` get_media_requests(self, item, info)` в метод приходит item и ещё создаётся объект info, который содержит информацию о процессе скачивания фотографий: данные о том сколько файлы стоят в осереди на скачивание, сколько сейчас уже скачиваются, сколько осталось ещё скачать. \n",
    "\n",
    "сначала проверим есть ли у нашего айтома в поле photos хоть что то.\n",
    "`if item['photos']:` - если там что то есть и если есть то начинаем скачивать\n",
    "```\n",
    "if item['photos']:\n",
    "    for img in item['photos']:\n",
    "```\n",
    "\n",
    "для каждой ссылки мы должны попытаться (try) выполнить запрос по этой ссылке. Если мы делаем без try-except тем более в потоке, так как одновременно можем скачивать несколько фотографий, у нас вероятность ошибки повышается. Поэтому страхуемся, чтобы приложение не упало. Внутри try мы как раз производим скачивание.\n",
    "```\n",
    "if item['photos']:\n",
    "    for img in item['photos']:\n",
    "        try:\n",
    "            yield \n",
    "```\n",
    "\n",
    "Процедура скачивания. мы создаём новый объект запроса. Первичный объект запроса у нас уже создавался `def parse(self, response:HtmlResponse)` - объект response. Дальше мы работаем в рамках этого объекта. при переходе на следующую ссылку опять делаем через response. Условно говоря это наша сессия в браузере - наша открытая вкладка. \n",
    "\n",
    "А процесс качивания фотографий это новая открытая вкладка у нас в браузере.  Технически каждый процесс скачивания - это отдельный процесс, который не зависит от основоного.  Здесь это выражено в качестве создания нового объекта запроса, поэтому ещё к нашему пайплайну подключаем объект скрапи так как именно в модуле скрапи у нас находится объект Request. `import scrapy ` внутрь класса Request(img) передаём ссылку для скачивания картинки. Просто создаём по отдельному объекту запроса для каждой ссылки ведущей нас на фотографию. \n",
    "\n",
    "Дальше делаем except. Выведим сообщение об ошибке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fd7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class AvitoPhotoPipeline(ImagesPipeline):\n",
    "    def get_media_requests(self, item, info):\n",
    "        if item['photos']:\n",
    "            for img in item['photos']:\n",
    "                try:\n",
    "                    yield scrapy.Request(img)\n",
    "                except Exeption as c:\n",
    "                    print(c)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f4ace",
   "metadata": {},
   "source": [
    "Для того чтобы всё сработало в полной мере ещё необходимо сделать 3 действия. первое действие касается только лишь картинок. Мы должны установить дополнительно в комплект с модулем скрапи ещё модуль pillow - модуль для обработки фотографий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11202c91",
   "metadata": {},
   "outputs": [],
   "source": [
    " ! pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2ace5",
   "metadata": {},
   "source": [
    "Второй момент. В settings.py нужно указать параметр `IMAGES_STORE = 'photos'`. Если этого не сделать всё будет хорошо работать, но скачиваться ничего не будет, потому что данное значение будет пустым, а скачивание по значению None не даёт ничего. Физически файл будет скачиваться, но не будет сохроняться на жёсткий диск. \n",
    "\n",
    "Третий момент. Мы создали отдельновзятый пайплайн `AvitoPhotoPipeline`. И раз мы это сделали у нас информации об его существовании нет нигде вообще. Информация об AvitoparserPipeline создаётся когда мы генерируем проект, а об AvitoPhotoPipeline информации нету. У нас нет входа в AvitoPhotoPipeline. Это также делается в settings.py \n",
    "```\n",
    "ITEM_PIPELINES = {\n",
    "   'avitoparser.pipelines.AvitoparserPipeline': 300,\n",
    "   'avitoparser.pipelines.AvitoPhotosPipeline': 200\n",
    "}\n",
    "```\n",
    "\n",
    "В этом словаре может храниться несколько пайплайнов. Указываем начальный путь до пайплайна `avitoparser.pipelines.AvitoPhotosPipeline` в ключах указываем пути до класса с пайплайнами. Паук должен знать о всех наших пайплайнах. \n",
    "Встаёт вопрос в следующем, раз пайплайнов несколько, в каком порядке по ним ходить. \n",
    "\n",
    "Цифра, которая является значением ключа задаёт приоритет прохождения. Чем меньше значение, тем более высокий приоритет. Нам надо, чтобы фотки скачиались раньше, чем всё складывается в базу данных. Теперь паук будет точно знать, что item надо сперва закинуть в `AvitoPhotosPipeline` а потом в `AvitoparserPipeline`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd51d1",
   "metadata": {},
   "source": [
    "### Привьюшки\n",
    "В settings.py можно указать параметр \n",
    "```\n",
    "IMAGES_THUMBS = {\n",
    "   'medium': (55, 35)\n",
    "}\n",
    "```\n",
    "в словаре мы говорим, что хотим привьюшки другого размера, например medium и указываем размер 55 на 35 пикселей. Для них создастся папка медиум, в которую они будут помещены соответствующего размера. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf5c4e",
   "metadata": {},
   "source": [
    "### как картинки подогнать под объявления\n",
    "Теперь нам осталось решить проблему как сопоставить фотографии скаченные и айтомы. Наша задача сделать связи между текстовыми данными  name, price, url и ссылки на фотографии скаченные, которые где то на диске хронятся (в папке full). \n",
    "\n",
    "Для этого есть специальный метод, который мы заберём у ImagesPipeline. Метод  `def item_completed(self, results, item, info)` в этом методе есть объект `results`. Это тоже список и каждый объект этого списка это картеж. Для каждого скаченного объекта в списке results создаётся картеж, внутри которого 2 элемента: булиево значение, означающее успешно ли скачалась фотка и второй элемент кортежа - словарь с четеримя полезнейшими ключами. `checksum` - контрольная сумма файла  (чтобы скрапи не скачивал уже скаченный ранее файл).  `path` - удрес и название скаченной фотки, еперь у нас есть путь к скаченной фотке. `status` - говорит о том был уже файл скачен ранее или нет (download - только скачен, updated - скачен был ранее). `url` - ссылка откуда скачен файл. \n",
    "\n",
    "Теперь мы это хотим сохранить в свой item. мы возьмём второй элемент кортежа `itm[1]` для каждого `itm` в `results`, но при условии если первый элемент кортежа True (`if itm[0]`)\n",
    "\n",
    "`item['photos'] = [itm[1] for itm in results if itm[0]]`\n",
    "\n",
    "Теперь в каждый `item['photos']` запишится этот словарь.  И теперь вместо списка ссылок который раньше был в `item['photos']` записывается более полная информация и мы можем по адресу обратиться к скаченной картинке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3056bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvitoPhotosPipeline(ImagesPipeline):\n",
    "    def get_media_requests(self, item, info):\n",
    "        if item['photos']:\n",
    "            for img in item['photos']:\n",
    "                try:\n",
    "                    yield scrapy.Request(img)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    def item_completed(self, results, item, info):\n",
    "        item['photos'] = [itm[1] for itm in results if itm[0]]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebfbd1",
   "metadata": {},
   "source": [
    "После того как завершили работу с items в рамках одного пайплайна мы обязательно должны этот самый item вернуть. `return item`. \n",
    "\n",
    "теперь не важно что все собранные фото лежат в одной папке. мы можем обратиться к словарю и получить ссылку на адрес хранения фотографии на жёстком диске. Технически фотографии не хронятся в вамой базе данных, в базе данных хронятся локальные ссылки на эту фотографию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9328cec",
   "metadata": {},
   "source": [
    "### Ещё о скрапи\n",
    "У нас есть паук. \n",
    "```\n",
    "# spiders/avito.py\n",
    "\n",
    "import scrapy\n",
    "from scrapy.http import HtmlResponse\n",
    "from avitoparser.items import AvitoparserItem\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "class AvitoSpider(scrapy.Spider):\n",
    "    name = 'avito'\n",
    "    allowed_domains = ['avito.ru']\n",
    "\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.start_urls = [f\"https://www.avito.ru/izhevsk?q={kwargs.get('query')}\"]\n",
    "\n",
    "    def parse(self, response: HtmlResponse):\n",
    "        links = response.xpath(\"//a[@data-marker='item-title']\")\n",
    "        for link in links:\n",
    "            yield response.follow(link, callback=self.parse_ads)\n",
    "\n",
    "\n",
    "    def parse_ads(self, response: HtmlResponse):\n",
    "        loader = ItemLoader(item=AvitoparserItem(), response=response)\n",
    "        loader.add_xpath('name', '//h1/span/text()')\n",
    "        loader.add_xpath('price', \"//span[@itemprop='price']/text()\")\n",
    "        loader.add_xpath('photos', \"//li[contains(@class,'images-preview-preview')]/img/@src\")\n",
    "        loader.add_value('url', response.url)\n",
    "        yield loader.load_item()\n",
    "\n",
    "\n",
    "        # name = response.xpath(\"//h1/span/text()\").get()\n",
    "        # price = response.xpath(\"//span[@itemprop='price']/text()\").get()\n",
    "        # url = response.url\n",
    "        # photos = response.xpath(\"//li[contains(@class,'images-preview-preview')]/img/@src\").getall()\n",
    "        # yield AvitoparserItem(name=name, price=price, url=url, photos=photos)\n",
    "```\n",
    "\n",
    "Как сделать так, чтобы паук не занимался извлечением данных вообще. Метод `def parse_ads(self, response: HtmlResponse):`. Процесс itemLoader. Добавим в класс нашего паука ещё один класс. `from scrapy.loader import ItemLoader`. Он нам даёт возможность освободить функциональность паука на конечном методе, который заключается в том, чтобы чисто технически собрать с помощью специальных селекторов данные со страницы.  По сути задача паука - только парсинг. Он не должен заниматься обработкой. Он просто переходит по страницам и собирает данные, а мы эти данные собираем и отправляем в другой модуль. \n",
    "\n",
    "В последнем методе, который занимается сбором данных с финальной страницы, полученной на предыдущем этапе ` def parse_ads(self, response: HtmlResponse):`мы должны создать объект этого loader. `loader = ItemLoader()` и внутри в конструктор ему передать 2 параметра: item. по сути здесь мы должны создать объект нашего класса, AvitoparserItem() `loader = ItemLoader(item=AvitoparserItem(), )` вторым моментом мы передаём параметром наш response `loader = ItemLoader(item=AvitoparserItem(), response=response)` - наш объект загрузчика создан. \n",
    "\n",
    "Так внутри этого объекта загрузчика мы имеем объект класса AvitoparserItem() то есть структура нашего будущего айтома хронится теперь в loader. Мы теперь должны заполнить эту самую структуру. Мы создавали 4 поля:\n",
    "```\n",
    "loader.add_xpath('name', '//h1/span/text()')\n",
    "        loader.add_xpath('price', \"//span[@itemprop='price']/text()\")\n",
    "        loader.add_xpath('photos', \"//li[contains(@class,'images-preview-preview')]/img/@src\")\n",
    "        loader.add_value('url', response.url)\n",
    "```\n",
    "\n",
    "Теперь мы работаем не напрямую с какими то свойствами, а работаем через loader. У него есть отдельные методы очень похожие на методы response.  add_Xpath - добавить элемент собранный с помощью xpath , add_CSS - добавить элемент собранный с помощью css и add_value - добавить готовое значение.  В готовых методах первым параметром идёт имя поля (ключ), второй параметр значение этого поля - селектор. \n",
    "\n",
    "Через loader мы заполнили все наши поля. И в финале когда мы уже знаем все селекторы применённые для всех полей делаем через yield вызов метода load_item() `yield loader.load_item()`. \n",
    "\n",
    "Мы убрали у паука функциональность по обработке данных. Мы на этом этапе передаём всю обработку и извлечение данных уже в другой модуль. В обёртку над нашим классом AvitoparserItem().\n",
    "```\n",
    "class AvitoparserItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    name = scrapy.Field(output_processor=TakeFirst())\n",
    "    price = scrapy.Field(input_processor=MapCompose(clear_price), output_processor=TakeFirst())\n",
    "    url = scrapy.Field(output_processor=TakeFirst())\n",
    "    photos = scrapy.Field()\n",
    "    _id = scrapy.Field()\n",
    "\n",
    "```\n",
    "\n",
    "Тем самым паук может заниматься своими делами дальше, а ItemLoader будет заниматься уже первичной обработкой данных и их структурированием. Мы тем самым распараллеливаем процессы и можем выигрывать в скорости. \n",
    "\n",
    "Польза также в том, что мы можем делать мелкие обработки до пайплайн. ItemLoader забрал часть паука и часть пайплайнов. Мы можем здесь добавить обработчики. у него есть свои собственные. `from itemLoaders.processors import MapCompose, TakeFirst` мы можем импортировать 6 обработчиков из которых реально используются только 2-3. \n",
    "\n",
    "обработчики нужны, чтобы собранные данные когда они попадут в соответствующие поля их можно было по мелкому ка книбудь обработать. В частности не забываем что методы xpath и css селекторы всегда нам возвращают список. И нам нужно этот список раскрыть, достав оттуда первый элемент, когда нам это необходимо. Для этого используем обработчик `TakeFirst`. \n",
    "\n",
    "Все обработчики делятся на постобработчики и предобработчики. Они жёстко зафиксированы и изменять их нельзя. Каждое поле позволяет использовать один постобработчик и один предобработчик. `TakeFirst` - постобработчик. Этот постобработчик мы передаём в конструкторе класса AvitoparserItem в соответствующее поле в значении параметра output_processor= `name = scrapy.Field(output_processor=TakeFirst())`. Когда сработает этот обработчик он извлечёт из полученного списка первое значение и его сохранит в поле name. \n",
    "\n",
    "Для обработки цены нужен `MapCompose`. У нас приходит цена с пробелами, которые заменены спецсимволами и нам надо эти спецсимволы убрать. И эту мелкую предобработку можно производить внутри ItemLoader. Мы используем предобработчик `input_processor=MapCompose`. `price = scrapy.Field(input_processor=MapCompose(clear_price), output_processor=TakeFirst())`. Суть - предобработчик берёт список, который находится внутри данного поля и к каждому элементу списка применяет уазанную в параметрах функцию. \n",
    "\n",
    "Создадим функцию обработки `def clear_price()` Чтобы эта функция могла что то обрабатывать, она должна уметь что то принять. `def clear_price(value):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_price(value):\n",
    "    if value:\n",
    "        value = value.replace('\\xa0','')\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except:\n",
    "            return value\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7eddf2",
   "metadata": {},
   "source": [
    "`price = scrapy.Field(input_processor=MapCompose(clear_price), output_processor=TakeFirst())` так как нам возвращается список из 2ух элементов, то возьмём лишь одну цену с помощью постобработчика `output_processor=TakeFirst()`. \n",
    "\n",
    "input_processor всегда срабатывает раньше output_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8fb6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f379070",
   "metadata": {},
   "source": [
    "# Файлы с архива урока "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/avito.py\n",
    "\n",
    "import scrapy\n",
    "from scrapy.http import HtmlResponse\n",
    "from avitoparser.items import AvitoparserItem\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "class AvitoSpider(scrapy.Spider):\n",
    "    name = 'avito'\n",
    "    allowed_domains = ['avito.ru']\n",
    "\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.start_urls = [f\"https://www.avito.ru/izhevsk?q={kwargs.get('query')}\"]\n",
    "\n",
    "    def parse(self, response: HtmlResponse):\n",
    "        links = response.xpath(\"//a[@data-marker='item-title']\")\n",
    "        for link in links:\n",
    "            yield response.follow(link, callback=self.parse_ads)\n",
    "\n",
    "\n",
    "    def parse_ads(self, response: HtmlResponse):\n",
    "        loader = ItemLoader(item=AvitoparserItem(), response=response)\n",
    "        loader.add_xpath('name', '//h1/span/text()')\n",
    "        loader.add_xpath('price', \"//span[@itemprop='price']/text()\")\n",
    "        loader.add_xpath('photos', \"//li[contains(@class,'images-preview-preview')]/img/@src\")\n",
    "        loader.add_value('url', response.url)\n",
    "        yield loader.load_item()\n",
    "\n",
    "\n",
    "        # name = response.xpath(\"//h1/span/text()\").get()\n",
    "        # price = response.xpath(\"//span[@itemprop='price']/text()\").get()\n",
    "        # url = response.url\n",
    "        # photos = response.xpath(\"//li[contains(@class,'images-preview-preview')]/img/@src\").getall()\n",
    "        # yield AvitoparserItem(name=name, price=price, url=url, photos=photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb28c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "\n",
    "# Scrapy settings for avitoparser project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'avitoparser'\n",
    "\n",
    "IMAGES_STORE = 'photos'\n",
    "IMAGES_THUMBS = {\n",
    "   'medium': (55, 35)\n",
    "}\n",
    "\n",
    "\n",
    "SPIDER_MODULES = ['avitoparser.spiders']\n",
    "NEWSPIDER_MODULE = 'avitoparser.spiders'\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "USER_AGENT = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:98.0) Gecko/20100101 Firefox/98.0'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "CONCURRENT_REQUESTS = 8\n",
    "\n",
    "LOG_ENABLED = True\n",
    "LOG_LEVEL = \"DEBUG\"\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "DOWNLOAD_DELAY = 1.5\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "COOKIES_ENABLED = True\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "# DEFAULT_REQUEST_HEADERS = {\n",
    "#   'cookie':'yandexuid=1259160151646070856; yuidss=1259160151646070856; yabs-sid=1937487681646070856; ymex=1961430856.yrts.1646070856#1961430856.yrtsi.1646070856; gdpr=0; _ym_uid=1646070893156269093; my=YwA=; amcuid=8340317491646422388; yandex_login=Orleon.ya; i=wdTJiryHb+LLnT5mcRSlLhkbNWoRkyWlPsQzRYQZNAx4MhNo6TM/BV8R51m0d0jIkpBJv0xmNtTmxeXkiTiWht9wDgw=; is_gdpr=0; is_gdpr_b=CNaZZBDuaigC; yandex_gid=44; cycada=Pu5H3ElFTWHQAAU7lrMwAL/hvOtk0vJNciYyzRBtN1E=; _ym_d=1649519883; yabs-frequency=/5/0000000000000000/LVTwO9j8Vb84HY6q2tDmb00000H68M-bN1mQii9h14Od8krF10oancu4HYC0/; yp=1665287886.szm.1_25:2560x1440:1990x1072#1962472582.udn.cDpPcmxlb24ueWE%3D#1651771282.ygu.1#1651771283.spcs.l#1651857692.csc.1; ys=udn.cDpPcmxlb24ueWE%3D#c_chck.8048816; Session_id=3:1649696374.5.0.1647112582511:GiqSWw:25.1.2:1|271833109.0.2|3:250771.178521.H07E8OxfqD1kCKQ9zmuC7qex_qY; sessionid2=3:1649696374.5.0.1647112582511:GiqSWw:25.1.2:1|271833109.0.2|3:250771.178521.H07E8OxfqD1kCKQ9zmuC7qex_qY'\n",
    "# }\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'avitoparser.middlewares.AvitoparserSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'avitoparser.middlewares.AvitoparserDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "   'avitoparser.pipelines.AvitoparserPipeline': 300,\n",
    "   'avitoparser.pipelines.AvitoPhotosPipeline': 200\n",
    "}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51def14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner.py \n",
    "\n",
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "from avitoparser.spiders.avito import AvitoSpider\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    configure_logging()\n",
    "    settings = get_project_settings()\n",
    "    runner = CrawlerRunner(settings)\n",
    "    # query = input('')\n",
    "    runner.crawl(AvitoSpider, query='bmw')\n",
    "\n",
    "    reactor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "import requests\n",
    "import wget\n",
    "\n",
    "url = 'https://www.ap22.ru/netcat_files/multifile/2546/90766/palms_3242342_960_720.jpg'\n",
    "wget.download(url)\n",
    "\n",
    "# response = requests.get(url)\n",
    "#\n",
    "# with open('palms.jpg', 'wb') as f:\n",
    "#     f.write(response.content)\n",
    "#\n",
    "# response = requests.get(url, stream=True)\n",
    "# handle = open(target_path, \"wb\")\n",
    "# for chunk in response.iter_content(chunk_size=512):\n",
    "#     if chunk:  # filter out keep-alive new chunks\n",
    "#         handle.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "import scrapy\n",
    "from itemadapter import ItemAdapter\n",
    "from scrapy.pipelines.images import ImagesPipeline\n",
    "\n",
    "class AvitoparserPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        print()\n",
    "        return item\n",
    "\n",
    "\n",
    "class AvitoPhotosPipeline(ImagesPipeline):\n",
    "    def get_media_requests(self, item, info):\n",
    "        if item['photos']:\n",
    "            for img in item['photos']:\n",
    "                try:\n",
    "                    yield scrapy.Request(img)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    def item_completed(self, results, item, info):\n",
    "        item['photos'] = [itm[1] for itm in results if itm[0]]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "from itemloaders.processors import MapCompose, TakeFirst\n",
    "\n",
    "def clear_price(value):\n",
    "    if value:\n",
    "        value = value.replace('\\xa0','')\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except:\n",
    "            return value\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "class AvitoparserItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    name = scrapy.Field(output_processor=TakeFirst())\n",
    "    price = scrapy.Field(input_processor=MapCompose(clear_price), output_processor=TakeFirst())\n",
    "    url = scrapy.Field(output_processor=TakeFirst())\n",
    "    photos = scrapy.Field()\n",
    "    _id = scrapy.Field()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90b515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
