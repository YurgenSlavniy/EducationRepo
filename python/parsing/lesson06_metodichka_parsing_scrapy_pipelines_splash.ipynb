{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6df42ed",
   "metadata": {},
   "source": [
    "# Урок 6. Парсинг данных. Scrapy, pipelines, Splash\n",
    "### Методы сбора и обработки данных при помощи Python\n",
    "\n",
    "### На этом уроке\n",
    "1. Познакомимся с фреймворком Splash\n",
    "2. Научимся сохранять данные из Scrapy в mongoDB и Sqlite\n",
    "\n",
    "Если мы хотим импортировать все данные в базу данных, всю логику этого процесса надо описывать\n",
    "в файле pipelines.py.\n",
    "\n",
    "Посмотрим на этот файл из нашего предыдущего проекта `books`. Как видите, тут уже есть встроенный класс BooksItemPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookPipeLine:\n",
    "    def process_item(self, item, spider):\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca06c5",
   "metadata": {},
   "source": [
    "И как видите, тут уже есть встроенный метод process_item(). В этом классе моггу быть еще два метода: open_spider() и close_spider(), оба принимают аргументы self и spider. Open вызывается один раз, когда паук начинает свою работу, то есть открывается, close вызывается тогда, когда паук\n",
    "закончил свою работу и закрылся.\n",
    "\n",
    "Теперь нам надо перейти в настройки - файл settings.py.\n",
    "Там находим строчку ITEM_PIPELINES и раскомментируем ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d850121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure item pipelines\n",
    "# see https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {'books.pipelines.BooksPipeline': 300}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceebe69",
   "metadata": {},
   "source": [
    "В этом словаре мы можем указать все пайплайны, которые мы хотим использовать, прописав путь к\n",
    "каждому на подобие того, что у нас уже записано. То есть будет взят класс BooksPipeline из папки\n",
    "pipelines, которая находится внутри директории books. Цифра отвечает за приоритет.\n",
    "Допустим, мы создадим еще один пайплайн, который будет отвечать за фильтрацию дубликатов.\n",
    "Назовем его books.pipelines.FilterDuplicates. И поставим ему значение 100ю Это значит, что этот\n",
    "пайплайн будет обрабатываться в первую очередь. Чем меньше цифра, тем выше приоритет.\n",
    "Теперь давайте попробуем сохранять наши данные в MongoDB. На предыдущих уроках мы уже\n",
    "обсуждали, как скачать, установить и запустить локальный сервер MongoDB. Запускаем его еще раз.\n",
    "для этого идем в папку с mongodb/bin и вводим команду для старта сервера:\n",
    "`brew services start mongodb-community@5.0`\n",
    "\n",
    "Наш сервер запустился. теперь идем в файл pipelines.py. Меняем название класса на\n",
    "MongoDBPipeline, затем импортируем библиотеку pymongo: `from pymongo import MongoClient`\n",
    "\n",
    "Затем создаем метод `open_spider(self, spider)`: и внутри него создаем клиент `self.client =\n",
    "MongoClient(‘localhost’, 27017)`\n",
    "\n",
    "Затем создаем объект базы данных: `self.db = self.client[‘Books’]`\n",
    "Теперь создаем метод `close_spider(self, spider)`: и внутри мы должны закрыть соединение с базой\n",
    "данных: `self.client.close()`\n",
    "\n",
    "Внутри метода `process_item()` мы будем вставлять в базу данных каждый элемент, который был\n",
    "спарсен нашим пауком. Для начала мы должны указать название коллекции, в которую будут\n",
    "сложены данные. Давайте создадим переменную в начале нашего класса `collection_name =\n",
    "‘all_books’`. И внутри функции `process_item` укажем эту коллекцию: `self.db[self.collection_name]` и\n",
    "добавим метод `insert_one()`. Как вы видите, у нас в функции `insert_item()` уже есть аргумент item,\n",
    "который нам и надо сохранять в базу. Укажем его `self.db[self.collection_name].insert_one(item)`.\n",
    "\n",
    "Переименовываем pipeline в файле с настройками: MongoDBPipeline и закомментируем переход на\n",
    "следующую страницу в пауке, чтобы убедиться, что всё у нас работает правильно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353767f",
   "metadata": {},
   "source": [
    "Запускаем парсер. Как видите, все выполнилось, никаких ошибок нет. Теперь давайте посмотрим, что\n",
    "же у нас сохранилось и куда. Есть два варианта, мы можем пойти в терминал, набрать там mongosh,\n",
    "а затем show dbs. Каквидите, у нас есть есть база данных Books.\n",
    "Перейдем в нее: use Books и посмотрим, что там у нас лежит db.all_books.find(). Как\n",
    "видите, у нас показан список элементов в базе данных. Посмотрим, сколько у нас всего элементов:\n",
    "db.all_books.countDocuments()\n",
    "\n",
    "Так как мы отключили переход на следующую страницу, то у нас всего 20 книг - всё верно. Теперь\n",
    "давайте удалим коллекцию и базу данных, включим переход на следующую страницу в парсере,\n",
    "данные будем собирать в другую базу, назовем ее scraped_books и посмотрим, что получится. Но\n",
    "уже немного другим способом.\n",
    "\n",
    "Парсер закончил работать. Проверяем: собралось 1000 книг. Так, теперь открываем MongoDB\n",
    "Compass, советую вам скачать эту программу, она бесплатная и удобная для исследования ваших баз данных. При открытии\n",
    "необходимо указать ссылку, по которой соединиться с вашей базой. Так как у нас запущен локальный\n",
    "сервер, указываем: mongodb:/ / localhos t :27017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3b175",
   "metadata": {},
   "source": [
    "Как видите, у нас тут есть база данных scraped_books. Кликаем на нее, тут всего одна коллекция -\n",
    "all_books. И вы видите статистику: 1К документов, общий объем и так далее:\n",
    "При клике коллекцию попадаем в данные, просматривать и фильтровать данные, посмотреть схему, индексы, копировать и удалять данные.\n",
    "\n",
    "Теперь давайте научимся сохранять данные в sqlite3. Для этого импортируем (import sqlite3) в файле\n",
    "pipelines и скопируем уже существующий пайплайн, чтобы было удобнее работать, назовем его\n",
    "SQLitePipeline.\n",
    "\n",
    "Создаем переменную connection в методе open_spider(): self.connection =\n",
    "sqlite3.connect(‘books.db’). В методже connect передадим название базы данных. Пусть будет books.\n",
    "Теперь создадим курсор: self.cursor = self.connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb271b",
   "metadata": {},
   "source": [
    "Теперь напишем запрос для создания таблицы. Для простоты сделаем все поля текстовые.\n",
    "```\n",
    "create_table_query = '''\n",
    "CREATE TABLE all_books(\n",
    "image TEXT,\n",
    "title TEXT,\n",
    "price TEXT,\n",
    "instock TEXT\n",
    ")\n",
    "'''\n",
    "```\n",
    "\n",
    "Вообще при создании таблиц со схемами надо уделять большое внимание типам данных, которые вы\n",
    "собираетесь хранить, а так же первичным ключам. Но это отдельные темы для изучения. Теперь\n",
    "выполним запрос на создание таблицы: self.cursor.execute(self.create_table_query)\n",
    "Сохраняем изменения в базу: self.connection.commit()\n",
    "\n",
    "В методе close_spider мы меняем client на connection, так как нам тоже надо закрыть соединение с\n",
    "базой, как и в случае с MongoDB.\n",
    "\n",
    "Теперь создадим запрос на запись данных в таблицу:\n",
    "```\n",
    "insert_query = '''\n",
    "INSERT INTO all_books(image, title, price, instock)\n",
    "VALUES(?,?,?,?)\n",
    "‘''\n",
    "```\n",
    "\n",
    "И выполним этот запрос в методе insert_item(). Только вместо простой переменно item укажем кортеж\n",
    "следующим способом self.cursor.execute(self.insert_query, (item.get(‘image’), item.get(‘title’), item.get(‘price’),\n",
    "item.get(‘instock’)))\n",
    "\n",
    "И сохраним данные: self.connection.commit()\n",
    "\n",
    "Копируем и меняем имя пайплайна в настройках. И запускаем нашего паука. Смотрите, у нас появился\n",
    "файл books.db: Просмотреть базу данных можно с помощью любой программы. У меня это DBeaver. Открываем и нажимаем на создать новое соединение, находим нашу базу данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb92c2",
   "metadata": {},
   "source": [
    "Теперь давайте немного изменим наш код, так как если вы запустите его еще раз, то у вас будет\n",
    "ошибка, что таблица с таким именем уже существует. Поэтому запрос на создание таблицы нам надо\n",
    "обернуть в try except:\n",
    "\n",
    "```\n",
    "try:\n",
    "self.cursor.execute(self.create_table_query)\n",
    "self.connection.commit()\n",
    "except sqlite3.OperationalError:\n",
    "pass\n",
    "```\n",
    "\n",
    "Теперь, если мы запустим паука еще раз, ошибки не будет, а данные будут записаны в\n",
    "существующую таблицу. Давайте остановим выполнение кода. И посмотрим, сколько книг у нас\n",
    "теперь в таблице: `select count(*) from all_books ab`\n",
    "\n",
    "На этом наше знакомство с пайплайнами в Scrapy завершено. Приступаем к знакомству с\n",
    "фреймворком Splash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ebe59",
   "metadata": {},
   "source": [
    "# Splash\n",
    "\n",
    "Часто при парсинге вы можете столкнуться с тем, что, прописав все пути до нужных элементов, вы\n",
    "получите пустые списки и массивы. Если вы проинспектируете html-ответ, который вам вернулся, вы\n",
    "можете увидеть, что разметка существенно отличается от той, которую вы только что изучали на\n",
    "сайте. Это всё связано с работой javascipt’a. Давайте разберемся на примере.\n",
    "\n",
    "Идем на сайт quotes.toscrape.com/js. Вот у нас тут такая разметка. Теперь давайте отключим\n",
    "выполнение javascript’ов на сайте. Для этого достаточно нажать F1 и выбрать «Отключить\n",
    "JavaScript». сайт перезагрузится, если нет, перезагрузите его сами. И смотрите, у нас получился совсем другой сайт - в нем нет никакого наполнения. То есть при загрузке сайта выполняется какой-то\n",
    "скрипт, который загружает все данные на страницу. Это сделано в том числе и для защиты от\n",
    "парсинга.\n",
    "\n",
    "К сожалению, ни requests, ни scrapy это обойти не могут, они загружают html ссылки без выполнения\n",
    "скриптов. Но есть чудесная библиотека - Splash Открываем ее домашнюю страницу -\n",
    "https://github.com/scrapy-plugins/scrapy-splash\n",
    "\n",
    "Эта библиотека позволяет загружать ссылку с выполнением javascript’ов и возвращать html. Сейчас\n",
    "мы научимся это делать. У меня сплеш предустановлен, вам необходимо следовать инструкции.\n",
    "После запуска докера запустится и сплеш. Мы можем открыть его по этому адресу. Давайте для\n",
    "начала попробуем получить html страницы на страничке splash, а потом поговорим про его\n",
    "использование вместе со scrapy.\n",
    "\n",
    "Итак, переходим по ссылке - localhost:8050 - откроется страничка splash. Здесь вы видите пример\n",
    "работы на языке lua, на котором написан Splash.\n",
    "Давайте для начала попробуем получить html обычной страницы quotes.toscrape.com. Для этого в\n",
    "окошко вместо google.com вставляем нашу ссылку на сайт. Затем стираем здесь всё и пишем:\n",
    "```\n",
    "url = args.url\n",
    "splash:go(url)\n",
    "return splash:html()\n",
    "```\n",
    "\n",
    "И нажимаем кнопку Render! Как видите, нам вернулся ответ, мы можем скопировать его в редактор или\n",
    "просмотреть здесь. В принципе, и так видно, что нам вернулся html с цитатами. Теперь давайте\n",
    "изменим нашу ссылку, добавив к ней слеш js. Снова запускаем код. Смотрите, тут нам вернулся точно\n",
    "такой же html, как и с первой ссылкой. Значит, сплеш смог выполнить скрипты и получить\n",
    "полноценную страницу с кодом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35613818",
   "metadata": {},
   "source": [
    "Давайте напишем еще несколько команд. Во-первых, splash:go(url) завернем в assert - это\n",
    "предотвратит splash от выполнения дальнейших строчек кода, если url не откроется. Так же мы\n",
    "можем добавить таймаут вручную, для этого напишем: assert(splash:wait(1)) - в таком случае сплеш\n",
    "будет ждать 1 секунду и затем выполнит оставшуюся часть кода.\n",
    "\n",
    "Теперь, при выполнении скрипта сплеш использует свой юзер агент, многие сайты могут\n",
    "заблокировать выполнение скриптов и в принципе работу сплеша, как только увидят, что юзер агент -\n",
    "сплеш. Давайте поменяем это в нашем коде. Мы можем сделать это двумя способами. Давайте\n",
    "найдем какой-нибудь юзер агент.\n",
    "\n",
    "Первый способ: splash:set_user_agent(\"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\n",
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36»)\n",
    "\n",
    "Второй способ:\n",
    "```\n",
    "headers = {\n",
    "['User-Agent'] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like\n",
    "Gecko) Chrome/98.0.4758.80 Safari/537.36\"\n",
    "}\n",
    "splash:set_custom_headers(headers)\n",
    "```\n",
    "\n",
    "Так же сайт может не возвращать html, потому что splash использует по умолчанию настройку\n",
    "инкогнито, сайт это распознает и блокирует сплеш от парсинга. Отключить эту настройку можно\n",
    "следующим образом:\n",
    "splash.private_mode_enabled = false\n",
    "Мы с вами будем парсить простой сайт, так что все эти модификации нам не нужны, но о них полезно\n",
    "помнить.\n",
    "\n",
    "Теперь, когда мы познакомились со сплешем, давайте научимся запускать его из скрейпи. Создадим\n",
    "новый проект, назовем его splash_quotes\n",
    "```\n",
    "scrapy startproject splash_quotes\n",
    "cd splash_quotes\n",
    "scrapy genspider quotes quotes.toscrape.com\n",
    "```\n",
    "\n",
    "Старт юрлс нам не нужен, пока закомментируем, потому что ссылка отсюда нам еще понадобится.\n",
    "Теперь импортируем библиотеку scrapy_splash: from scrapy_splash import SplashRequest. И с\n",
    "главной страницы splash добавляем в настройки следующие строчки кода. Давайте скопируем их\n",
    "просто. Splash url можем поменять на localhost. Обратите внимание, что для правильно работы сплеш\n",
    "в скрейпи, у вас должен быть запущен сплеш на докере, иначе ничего не сработает.\n",
    "\n",
    "Так, теперь возвращаемся к нашему пауку. Сначала создадим переменную script, в которую\n",
    "скопируем наш код из локального сплеша:\n",
    "```\n",
    "script = '''function main(splash, args)\n",
    "url = args.url\n",
    "assert(splash:go(url))\n",
    "assert(splash:wait(1))\n",
    "return splash:html()\n",
    "end\n",
    "```\n",
    "\n",
    "Теперь создадим функцию start_requests, которая будет запускать выполнение скрипта на сплеше, а\n",
    "ответ передавать в следующую функцию. Итак:\n",
    "```\n",
    "def start_requests(self):\n",
    "yield SplashRequest(\n",
    "url='http://quotes.toscrape.com/js', - указываем ссылку на сайт, который хотим спарсить и\n",
    "удаляем start_urls\n",
    "callback=self.parse, - после выполнения скрипта передаем ответ в функцию parse\n",
    "endpoint=‘execute', - выполнить скрипт\n",
    "args={\n",
    "'lua_source': self.script - указываем, какой скрипт выполнить\n",
    "```\n",
    "\n",
    "Теперь давайте соберем цитату и автора в функции parse:\n",
    "quotes = response.xpath(\"//div[@class='quote']\")\n",
    "for quote in quotes:\n",
    "yield {\n",
    "'author': quote.xpath(\".//small[@class='author']/text()\").get(),\n",
    "'quote': quote.xpath(\".//span[@class='text']/text()\").get()\n",
    "Запускаем скрипт. Как видите, у нас собрались первые 10 цитат с сайта. Используя сплеш можно\n",
    "обходить такие вот блокировки. В последнем уроке мы с вами вернемся к сплешу, когда будем\n",
    "говорить о логине на сайт при его помощи. На этом сегодняшний урок завершен.\n",
    "\n",
    "### Домашнее задание\n",
    "1) Собрать все цитаты с сайта quotes.toscrape.com/js, плюс провалиться в информацию о каждом авторе и собрать информацию о нем: день рождения и описание.\n",
    "2) Полученные данные сохранить в БД на ваш выбор (mongoDB, SQLite3) с использованием пайплайна. Скриншот монги или .db файл приложить вместе с кодом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609de45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
