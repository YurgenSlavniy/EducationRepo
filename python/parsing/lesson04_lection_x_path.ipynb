{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f065ff7c",
   "metadata": {},
   "source": [
    "# Урок 4. \n",
    "## Парсинг HTML. XPath\n",
    "\n",
    "1) Структура страницы HTML - DOM\n",
    "\n",
    "2) Язык запросов Xpath\n",
    "\n",
    "3) Парсинг HTML в Python - библиотека lxml\n",
    "\n",
    "XPath - достаточно старый, борадатый инструмент. Этот язык запросов помогает выбирать нужные нам данные из голого чистого html. Предварительно этот код преобразуется к DOM. Точно такой же DOM как был в Beautiful_Soup.\n",
    "\n",
    "DOM (от англ. Document Object Model - объектная модель документа) - программный интерфейс, позволяющий программам скриптам получать доступ к содержимому html документов, а также изменять их содержимое, структуру и оформление таких документов. \n",
    "\n",
    "типы узлов:\n",
    "- документ\n",
    "- элементы\n",
    "- текстовые узлы\n",
    "- комментарии\n",
    "- атрибуты\n",
    "- текст\n",
    "- пространство имён\n",
    "- инструкция обработки\n",
    "\n",
    "В XPath  другой парсер в отличии от Beautiful_Soup. С другим механизмом работы. сами классы по другому называются, объекты. связи. Другие методы. \n",
    "\n",
    "Связи между объектами в доме схожи со связями Beautiful_Soup. Родительские, дочерние, родственные. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb321de",
   "metadata": {},
   "source": [
    "Навигация по DOM \n",
    "```\n",
    "document\n",
    "    |\n",
    "childNodes[0]   (<HTML>)\n",
    "    |\n",
    "childNodes[2]   (<BODY>)\n",
    "    |\n",
    "childNodes[1]   (<p>)\n",
    "    |\n",
    "childNodes[0]   (<input>)  \n",
    "    |\n",
    "value\n",
    "```\n",
    "\n",
    "Если нам необходимо выставить какую то цепочку зависимостей одного объекта от другого чтобы не запускать заново метод поиска, просто переходим по этим узлам от изночально корневого элемента, до нужного нам, используя связи по средствам обращения к каждому встречному узлу на пути от точки А до точки Б пока не дойдём до нужной нам части. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.childNodes[0].childNodes[2].childNodes[1].childNodes[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5bd9d",
   "metadata": {},
   "source": [
    "## Язык запросов XPath\n",
    "XPath (XML Path Language) - Язык запросов к элементам XML-документа. Разработан для организации доступа к частям документа XML в файлах трансформации XSLT и является стандартом консорциума W3C. XPath призван реализовать навигацию по DOM в XML. в Xpath используется компактный синтаксис, отличный от принятого в XLM.\n",
    "\n",
    "XML - стандарт формата данных (похожий на HTML, т.к и там и там есть теги). Используется в некоторых организациях. Некоторые API могут выдавать XML формат. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c0f39",
   "metadata": {},
   "source": [
    "#### Алгоритмы работы XPath\n",
    "у нас есть некий HTML код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "<html>\n",
    "    <body>\n",
    "        <div>Первый слой\n",
    "            <span>блок текста в первом слое </span>\n",
    "        </div>\n",
    "        <div>Второй слой</div>\n",
    "        <div>Третий слой\n",
    "            <span class='text'>первый блок в третьем слое</span>\n",
    "            <span class='text'>второй блок в третьем слое</span>\n",
    "            <span>третий блок в третьем слое</span>\n",
    "        </div>\n",
    "        <span>четвёртый слой</span>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1495436",
   "metadata": {},
   "source": [
    "Нас в данном коде интересуют 2 тега:\n",
    "```\n",
    "<span class='text'>первый блок в третьем слое</span>\n",
    "<span class='text'>второй блок в третьем слое</span>\n",
    "```\n",
    "\n",
    "Это должен будет быть наш конечный результат. В принципе мы достигаем этой цели с ипоьзованием текущего Xpath пути. Пройдёмся по нему пошагово\n",
    "\n",
    "#### Xpath путь: `/html/body/*/span[@class]`\n",
    "\n",
    "будет соответствовать в нём двум элементам исходного документа.\n",
    "```\n",
    "<span class='text'>\n",
    "<span class='text'>\n",
    "```\n",
    "\n",
    "Xpath  можно сравнить с путём к файлу в нашей операционной системе. \n",
    "Каждый узел есть директория, и в конечном счёте нужный нам элемент это файл с каким то расширением. Если проводить грубую анологию с ОС. \n",
    "\n",
    "Для того чтобы дойти до нужного файла, нам надо указать путь, относительно корня диска операционной системы или корня раздела. В целом так всё и происходит. \n",
    "\n",
    "Мы берём относительно корня. (корнем является document). Корень -  Базовый объект от которого наследуются все остальные объекты, являющиеся элементами дерева. Изначально весь парсинг происходит отталкиваясь от этого корня. Поэтому первым мы ставим \"/\". \n",
    "\n",
    "\"/\" в Xpath указывает отношение элемента справа к элементу слева. \"/\" указывает, что элемент который справа является непосредственно ребёнком элемента который слева. \n",
    "\n",
    "1. шаг `/html`  после него снова идёт одиночный слэш \"/\". \n",
    "\n",
    "2. шаг `/body` благодоря слэшу мы будем понимать что ищем детей тега html\n",
    "\n",
    "3. шаг `/*` - звёздочка означает, что там может быть любой тэг. Всё что угодно. Следует применять с большой осторожностью. все дети body будут подходить под третий шаг.\n",
    "\n",
    "4. шаг `/span[@class]` Мы явно задаём что у тэга span должен быть класс\n",
    "\n",
    "Парсинг заканчивается когда доходим до закрывающего тэга `</html>`\n",
    "Результатом работы будут 2 элемента, которые возвратятся списком. \n",
    "Рекурсивно обходит весь html без исключений. \n",
    "\n",
    "Результатом работы метода всегда будет список. Если элементов нет нам вернётся пустой список. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0545fc8",
   "metadata": {},
   "source": [
    "### Язык запросов XPath\n",
    "\n",
    "`выражение: messages` - выбирает все узлы с именем 'messages'\n",
    "\n",
    "`выражение: /messages` - выбирает корневой элемент сообщений. (Если путь начинается с косой серты (\"/\") то он всегда представляет абсолютный путь к элементу\n",
    "\n",
    "`выражение: messages/note` - выбирает все элементы 'note', являющиеся потомками элемента 'messages'\n",
    "\n",
    "`выражение: //note`- выбирает все элементы 'note', независимо от того, где они находятся\n",
    "\n",
    "`выражение: messages//note` -  выбирает все элементы 'note', являющиеся потомками элемента 'messages' независимо от того, где они находятся от элемента 'messages'\n",
    "\n",
    "`выражение: //@date` - выбирает все атрибуты с именем 'date'\n",
    "\n",
    "\n",
    "У  XPath есть дополнительные инструменты, котрые позволяют очень эффективно быстро без написания кода тестить пути. \n",
    "\n",
    "С помощью инструмента разработчика открываем html код ресурса, где надо данные. Мы можем к уже загруженному html применить какой нибудь селектор.  нажимаем сочитание клавиш `CTRL + F` и в открывшейся строчке (внизу страницы) ввести XPath селектор.  Это не нагружает сервер, не отправляет запрос на сервер, это работает с локальными данными html которые сейчас в браузере отображаются. Те элементы, которые попадают под селекцию в html коде подсвечиваются жёлтым и также подсвечиваются на странице. Это стандартные инструменты любого браузера. \n",
    "\n",
    "В браузере можно установить дополнительно chroPATH. ужно смотреть в настройках браузера дополнения (магазин расширений). Появится это расширение в панели разработчика. В окне со стилями, слоями и другими инструментами. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29175747",
   "metadata": {},
   "source": [
    "#### Как работать с XPath\n",
    "\n",
    "Предположим нам надо собрать топ 5 новостей по версии яндекса. \n",
    "Начинаем с анализа HTML кода. Нам нужны контейнеры, которые содержат необходимые нам данные.  Мы собираем 5 новостей и видим что они находятся каждый в своём контейнере и мы попробуем найти, что есть общее у этих контейнеров.  Навожу на нужный элемент мышкой, ПКМ, и выбираю copy -> xpath. \n",
    "```\n",
    "//[@id=\"news_552134667988.6136\"]/article/div[2]/ul/li[2]\n",
    "```\n",
    "Смотрим контейнер, где находится интересующая нас новость. \n",
    "\n",
    "`<li class=\"news-story__story-2M\" data-testid=\"news-item\"></li>`\n",
    "\n",
    "У нас есть 5 таких контейнеров, в которых находится 5 новостей.  Нам должно быть удобно работать с этими тегами. Надо написать такой селектор, который бы выбрал эти элементы сразу на старнице.  Смотрим на внешний контейнер, где находятся интересующие нас контейнеры с новостями. Ищем родителя тэгов `<li class=\"news-story__story-2M\"`\n",
    "\n",
    "Это будет тег `<ul class=\"card-news__stories-Bu\" style=\"height:auto\"> </ul>`\n",
    "\n",
    "пишем селектор xpath: //ul и проверяем его в открывшейся поисковой строке в инструментах разработчика `CTRL + F`. В результате жёлтым подсветился контейнер  `<ul class=\"card-news__stories-Bu\" ` Без указания класса просто находим тэг `<ul>`. К классу привяжемся позже. \n",
    "Причём мы видим что он нашл 16 таких тэгов.  сейчас подсвечен 2 of 16.\n",
    "\n",
    "/ - слэш задаёт соотношение элемента справа к элементу слева. Используя одиночный слэш, мы говорим, что это ребёнок элемента слева.  Используя двойной слэш // - мы говорим, что это потомок n-ого поколения.  Задача xpath ограничившись элементом слева найти внутри него потомка с указанными параметрами. В частности параметров у нас немного, это должен быть тэг `<ul>`. \n",
    "\n",
    "Нам нужно максимально отвязаться от вёрстки. Когда ставим два слэша - говорим ищи по всему документу. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b3c25e",
   "metadata": {},
   "source": [
    "Когда у нас есть этот контейнер с нужной нам информацией, мы можем войти в этот тег. Смотрим, что у него есть дети `<li class=\"news-story__story-2M\"`? в которых находится нужная нам информация. \n",
    "коллектор имеет теперь вид:`//ul/li` Ищем всех детей li внутри ul.\n",
    "\n",
    "Зайдём в ребёнка и посмотрим где непосредственно находится нужная нам информация. \n",
    "\n",
    "```<span class=\"news-story__text-2H\" aria-label=\"Заголовок\">Число погибших в&nbsp;результате землетрясения в&nbsp;Турции увеличилось до&nbsp;284<!-- -->&nbsp;&nbsp;&nbsp;&nbsp;<!-- -->&nbsp;&nbsp;&nbsp;</span>\n",
    "```\n",
    "\n",
    "Анализируем уровни вложенности:\n",
    "`ul -> li -> a -> div -> span `\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033404f",
   "metadata": {},
   "source": [
    "Воспользуемся ещё раз двойным слэшем чтобы невилировать уровни вложенности. `//ul/li//span ` \n",
    "\n",
    "Если идти по структуре дерева, то коллектор будет иметь вид:\n",
    "`//ul/li/a/div/span`\n",
    "\n",
    "Если заменим например тег `<a>` на звёздочку, будет означать, что вместо звёздоки может быть любой тег `//ul/li/*/div/span`\n",
    "\n",
    "например сколько тегов и узлов на странице с атрибутом класс `//*[@class]`. Можем более конкретно узнать сколько тегов div с атрибутом класс `//div[@class]`. Все div у которых есть атрибут но не знаем какой `//div[@*]` - вернутся все дивы у которых есть какой то атрибут. Также есть отрицание `//div[not(@)*]` . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943b002",
   "metadata": {},
   "source": [
    "##### Уточняющие параметры\n",
    "эти параметры пишутся в квадратных скобках `[ ]`. Если что то уточняет нашу выборку, это что то будет в квадратных скобках.  Можно указать индекс элемента. `//ul/li/a/div/span[1]`.  В xpath нумирация начинается не с 0, а с 1.  В xpath даже есть свои собственные функции. `//ul/li/a/div[position()<4]` например функция position. Можно добавлять логические операторы `//ul/li/a/div[position()<4 and position()>1]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abdb875",
   "metadata": {},
   "source": [
    " # Практическая часть.\n",
    " \n",
    "- собрать погоду с сайта яндекс. https://dzen.ru\n",
    "- собрать 5 новостей с сайта яндекс https://dzen.ru\n",
    "- собрать 5 новостей с новостного ресурса  например https://ria.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314b68e",
   "metadata": {},
   "source": [
    "### Погода\n",
    "Заходим на сайт в браузере и ищем элемент на странице, смотрим через инструмент разработчика его код. Нас интересует значение температуры. \n",
    "```\n",
    "<div class=\"geoblock-weather__currentWeather-1P\">-1<!-- -->°</div>\n",
    "```\n",
    "Дльше открываем поиск XPath `CTRL + F` и вводим:\n",
    "\n",
    "`//div[@class='geoblock-weather__currentWeather-1P']`\n",
    "\n",
    "Подводный камень заключается в том, что если у элемента более чем один класс, например:\n",
    "`<div class=\"geoblock-weather__currentWeather-1P end_one_class\">-1<!-- -->°</div>`\n",
    "\n",
    "у нас 2 класса у тега div:\n",
    "- geoblock-weather__currentWeather-1P\n",
    "- end_one_class\n",
    "\n",
    "поиск в XPATH по селектору `//div[@class='geoblock-weather__currentWeather-1P']` ничего не даст, потому что не умеет различать классы. Чтобы XPATH понял надо вводить атрибут класс целиком, он не разделяет класссы по пробелу."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4b6c0",
   "metadata": {},
   "source": [
    "##### Рассмотрим функцию contains\n",
    "`//div[contains()]` передаём 2 параметра:\n",
    "- значение узла в котором мы будем искать (атрибут, класс)\n",
    "- значение этого самого атрибута\n",
    "\n",
    "`//div[contains(@class, 'end_one_class')]` \n",
    "\n",
    "привязываемся только к одному классу. Если он эту последовательность символов находит, он возвращает результат. \n",
    "```\n",
    " <div class=\"geoblock-weather__currentWeather-1P end_one_class\" data-tid=\"dehddjwx\">-1<!-- -->°</div>\n",
    "```\n",
    "Предположим что у элемента есть ещё атрибуты и можем искать ширше:\n",
    "\n",
    "`//div[contains(@class, 'end_one_class' and @data-tid)]`\n",
    "\n",
    "или ещё ширше, возьмём тег выше `<main>`:\n",
    "\n",
    "`//main/div[contains(@class, 'end_one_class' and @data-tid)]`\n",
    "\n",
    "ещё накрутим какое нибудь условие:\n",
    "\n",
    "`//main/div[contains(@class, 'end_one_class') and @data-tid and position()>3]`\n",
    "\n",
    "Мы выстраиваем наш XPATH, чтобы он не привязывался к динамическим классам. \n",
    "\n",
    "`//main/div[contains(@class, 'end_one_class') and @data-tid and position()>3 and not(position()=last())]`\n",
    "\n",
    "Position() - определяет какие теги по счёту. Из списка найденных тегов. Ограничение позиции выбрасывает лишние найденные элементы.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73a163",
   "metadata": {},
   "source": [
    "Иногда нужные нам элементы находятся на одной странице но в разных блоках и мы одним XPATH не можем их выбрать. В данной ситуации можно объединить 2 XPATH в одно выражение поставив объединяющее или - \" | \".  \n",
    "\n",
    "`//body/note | //body/day` - выбираем в документе body \n",
    "\n",
    "`//body | //div` - выбираем во всём документе\n",
    "\n",
    "Результат работы будет один список объектов полученных от 2 икс пасов. \n",
    "Получить один список, который потом надо будет обработать. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8da13",
   "metadata": {},
   "source": [
    "# ПРАКТИЧЕСКИЙ ПРИМЕР\n",
    "\n",
    "Начинаем мы с установки модуля. Парсер который позволяет получать доступ к xpath селекторам и с их помощью обрабатывать ДОМ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32a9718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-4.9.2-cp38-cp38-win_amd64.whl (3.9 MB)\n",
      "     ---------------------------------------- 3.9/3.9 MB 625.7 kB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-4.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "! pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad6884",
   "metadata": {},
   "source": [
    "Подключаем модуль отвечающий за html и остальные необходимые части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0fd90fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6dbf85",
   "metadata": {},
   "source": [
    "Создаём ссылку на которую мы сейчас пойдём"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_ebay = ''\n",
    "url_ya = ''\n",
    "url_ria = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a0ff7",
   "metadata": {},
   "source": [
    "выбираем категорию товара которую будем искать. Комплектующие для компьютера. Зашли на страницу входа список товаров, комплектующих. Эта страничка будет точкой входа и скопируем её себе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d64ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_ebay = 'https://www.ebay.com/b/Computer-Parts-Components/175673/bn_1643095'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f73ff",
   "metadata": {},
   "source": [
    "Юзер агент критически обязателен! Когда используем икс пас работаем с вёрсткой, которая была непосредственно выдана браузеру. В каком браузере работаем его юзер агента и используем. В браузере `chrom://version` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b9ba821",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da35037",
   "metadata": {},
   "source": [
    "Параметров в ссылке нет - больше заголовки не заполняем, есть - можно указать в словаре headers\n",
    "\n",
    "А дальше делаем запрос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93e916f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url_ebay, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49dcc668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [400]>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374700ec",
   "metadata": {},
   "source": [
    "Мы должны дальше из response получить DOM. Вызовим метод `fromstring()` а внутрь передаём хтмл код который содержится внутри нашего ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64212c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = html.fromstring(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6719aac3",
   "metadata": {},
   "source": [
    "Дальше работать непосредственно с вёрсткой. Результатом работы икс пас является список. Поэтому в половине случаев хорошо использовать иной подход по сбору данных. Получим сразу списком наиминование всех товаров. исследуем страничку, кликнем по наименованию товара и посмотрим на то где он находится в коде. \n",
    "\n",
    "`<h3 class=\"s-item__title\">Crucial 16GB 8GB 4GB 2Rx8 PC3L-12800S DDR3-1600MHz SODIMM Laptop Memory GS</h3>`\n",
    "\n",
    "вызываем в инструментах разработчика `CTRL + F` xpath строку и выбираем селектор `//h3`\n",
    "результат поиска 58 элементов но при анализе смотрим что первые 10 нам не нужны. Можем использовать класс дополнительно для селектора `//h3[@class=\"s-item__title\"]` и вот уже в результате поиска 48 нужных нам штук. \n",
    "\n",
    "`//h3[position()>10]` - не сработает, так как ищут детей. Разный родитель и результат поиска ноль. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd5ef8",
   "metadata": {},
   "source": [
    "теперь получаем список товаров, используя икс пас селектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_stock = dom.xpath(\"//h3[@class='s-item__title']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb9e10",
   "metadata": {},
   "source": [
    "Попробуем изменить точку входа. Просто в поиске пишем наименование товара, которое будем искать. Например в поисковой строке ибея ввожу: `fluke` и отправляюсь на новую точку входа. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5d12b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [400]>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_ebay = 'https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2380057.m570.l1313&_nkw=fluke&_sacat=0'\n",
    "\n",
    "headers = {'user agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "response = requests.get(url_ebay, headers=headers)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3cc61",
   "metadata": {},
   "source": [
    "Получаем ДОМ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = html.fromstring(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933ba7d",
   "metadata": {},
   "source": [
    "Из Дома дальше с помощью селектора находим все узлы нам нужные (блоки, теги) и считываем с них информацию - текст. Получаем список. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb82643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = dom.xpath(\"//h3[@class='s-item__title']/text()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808aa58",
   "metadata": {},
   "source": [
    "Дальше мы хотим собрать например ссылку. \n",
    "```\n",
    "<a data-interactions=\"[{&quot;actionKind&quot;:&quot;NAVSRC&quot;,&quot;interaction&quot;:&quot;wwFVrK2vRE0lhQQ0MDFHUlkzU1BaWUM5UEQ0SlYzMDZGQjNNR0o0MDFHUlkzU1BGRlZNMENCVDVaMUQwUDA1MkMAAAg3NDAwDE5BVlNSQwA=&quot;}]\" target=\"_blank\" data-s-jufo690=\"{&quot;eventFamily&quot;:&quot;LST&quot;,&quot;eventAction&quot;:&quot;ACTN&quot;,&quot;actionKind&quot;:&quot;NAVSRC&quot;,&quot;actionKinds&quot;:[&quot;NAVSRC&quot;],&quot;operationId&quot;:&quot;2351460&quot;,&quot;flushImmediately&quot;:false,&quot;eventProperty&quot;:{&quot;$l&quot;:&quot;138092160120784&quot;}}\" _sp=\"p2351460.m1686.l7400\" class=\"s-item__link\" href=\"https://www.ebay.com/itm/255966678916?hash=item3b98cd8f84:g:izUAAOSwundj4atr&amp;amdata=enc%3AAQAHAAAA0C6UkOD5JwlyEjIHm3BG3eeyDBIMUdzg%2FzqFTf%2Bgrt5ZGPL8BoL%2FgDnYFdJhbtbWOHkCYYL%2BGrjKIjD54fFqQmzESh%2FKNf1MJbAhlZg6CIfQ%2FyILQISb9JAQs%2B4ZvDqydPtb6Tm3QLLJCSkBiUb0A3OH6GuccERuzC3XgwnQZGAAivxsR5XEKwtID1xvNUi0T1qkAQLjlq3RiLUJUgexB6JGT3Fw4rY3ucT7XUcbQa%2BbmTjRIfRppCoCRVege1VLuviNnkWqnN%2FRGW5DPnbLlUM%3D%7Ctkp%3ABk9SR_7n5sPHYQ\"><div class=\"s-item__title\"><span role=\"heading\" aria-level=\"3\">Fluke 54 II B термометр</span></div><span class=\"clipped\">Открывается в новом окне или вкладке</span></a>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b127b",
   "metadata": {},
   "source": [
    "вот она в коде спряталась `href=\"https://www.ebay.com/itm/255966678916?`. \n",
    "\n",
    "Для демонстрации возможностей икс паса продемонстрируем следующий подход. В теге `<a>` есть класс `class=\"s-item__link\"`. \n",
    "в икс пас поиске в панеле разработчиков собираем теги просто `//h3[@class='s-item__link']`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb524f",
   "metadata": {},
   "source": [
    "Обратим внимание что ранее используемый тег `<h3>` и `<a>` находятся близко друг к другу. мы можем в принципе тег `<a>`  взять отталкиваясь от тега `<h3>`.  Берём ранее использованный селектор от которого надо будет подняться до родителя `//h3[@class='s-item__title']` и поднимаемся до родителя используя синтаксис '/..' : `//h3[@class='s-item__title'/..]`  и в \\том контейнере нас интересует значение атрибута `href`: `//h3[@class='s-item__title'/../@href]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = dom.xpath(\"//h3[@class='s-item__title'/../@href]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36efdd",
   "metadata": {},
   "source": [
    "можно было через класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca37d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = dom.xpath(\"a[@class='s-item__link'/@href]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c98a58",
   "metadata": {},
   "source": [
    "Подняться до дедушки: `//h3[@class='s-item__title'/../..]` \n",
    "\n",
    "Используя икс пас мы формируем списки данных. И потом эти списки мы потом сольём. \n",
    "\n",
    "Теперь возьмём цену. Бурём для этого `<span class='s-item__price'>` Но мы сталкиваемся с тем, что объектов больше чем нам нужно `//span[@class='s-item__price']` , так как есть некоторые товары с аукционной стоимостью и там указаны 2 цены. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = dom.xpath(\"span[@class='s-item__price']/text()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60ca12",
   "metadata": {},
   "source": [
    "Если links и markets у нас одинаковые по количеству элементов - 61, то prices нам вернулся список в 71 элемент. Потому что есть товары разной комплектации и цена указано от и до диапазоном.\n",
    "\n",
    "Но кажется собран не весь текст, пробуем так использовать для текста двойной слэш :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = dom.xpath(\"span[@class='s-item__price']//text()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd685364",
   "metadata": {},
   "source": [
    "результат нам приходит список в 91 позицию. Когда мы используем выборку этого элемента, контейнера с классом `span[@class='s-item__price']` и прописывая один слэш текст - мы ищем текст среди детей. Но во вложенных узлах мы текста не соберём. Цена в разных элементах на разных уровнях вложенности. Где то цена на уровне ребёнка, а где то на уровне внуков. Двойной слэш поэтому используем, чтобы собрать по всем уровням вложенности тексты. Там где разная комплектация на один товар приходит 4 единицы в списке: 'от', '100', 'до', '200'. Поэтому длинный список надо теперь обработать после сбора данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4251a",
   "metadata": {},
   "source": [
    "Например  'от', '100', 'до', '200' объединить в один элемент списка, чтобы для начала добиться совпадение длины списков. Идём по списку и если встречаем 'от' то три следующих элемента аппендим к нашему списку, делаем join по пробелу и делаем в одну строчку. Со структурой щас заморачиваться не будем - это постобработка, а щас надо собрать данные, чтобы они потом во едино слились. Длины списков нужны равные. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9b6b5",
   "metadata": {},
   "source": [
    "Соберём ещё информацию о ссылке на отзывы. И начинаем всё с инспекции элемента в инструментах разработчика. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1f1df",
   "metadata": {},
   "source": [
    "ссылочка находится во внуках тега `<div class='s_item'>` спускаемся к детям `//div[@class='s_item'/a/href]`  - ссылки на отзывы. Получили список ссылок в 24 объекта. А остальные списки иные по размеру. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f61e7",
   "metadata": {},
   "source": [
    "Подход показанный выше не всегда эффективен. Не везде применим. В данной ситуации эффективний получать контейнеры с объявлениями и проходить их в цикле. обрабатывая каждый контейнер. На одной итеррации цикла в рамках одного контейнера мы получаем набор данных по одному товару и сформировать словарь по одному товару. Если каких то данных нету мы будем ставить наны. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdee6a6",
   "metadata": {},
   "source": [
    "Для начала получим с помощью икс пас кнтейнер с данными по товару. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ae94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = dom.xpath(//li[contains(@class, 'item_lin')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b2c18",
   "metadata": {},
   "source": [
    "Получили контейнеры с объявлениями и уже в цикле перебираем их. Причём если есть лишний контейнеры можем их исключить взяв срез. `item_info` - создадим словарик внутри цикла в который будем добавлять данные по одному товару. В качестве ДОМа передаём item.  `item_info` заполняем по соответствующим ключам. И потом добавляем всё в итоговый список `list_items.append(item_info)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d146c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_items = []\n",
    "\n",
    "for item in items[1:]:\n",
    "    item_info = {}\n",
    "    price = item.xpath(\"//span[@class='s-item__price']//text()\")\n",
    "    link = item.xpath(\"//a[@class='s-item__link'/@href]\")\n",
    "    market = item.xpath(\"//h3[@class='s-item__title']/text()\")\n",
    "    \n",
    "    item_info['name'] = name\n",
    "    item_info['price'] = price\n",
    "    item_info['link'] = link\n",
    "    item_info['market'] = market\n",
    "    \n",
    "    list_items.append(item_info)\n",
    "    \n",
    "pprint(list_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2be74",
   "metadata": {},
   "source": [
    "\"CTRL\" + D - позволяет скопировать строчку, которая выше. в пайчарме."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d3dc9",
   "metadata": {},
   "source": [
    "Результат получился отличным от ожидаемого. Икс пас - такой товарищь...Ему пофиг что происходит снаружи. Когда он видит селектор `//span[@class='s-item__price']//text()` и видит объект к которому его прмменять `item`. Он игнорирует его рамки. Он выходит наружу до корня и начинает применять селектор сверх до низу.  Чтоби икс пас работал в рамках одного объекта используем магический символ. Ставим точку \".\" в начале икс паса, говоря о том, что наш путь теперь не обсалютный, а относительный. `.//span[@class='s-item__price']//text()` Точка - символ в операционной системе, который указывает на текущую директорию. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6bf4d",
   "metadata": {},
   "source": [
    "Результат тот который нужен получился. \n",
    "Дальше постобработка если необходима. Отдельной функцией к каждому элементу списка чистим и обрабатываем данные как надо.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd085c",
   "metadata": {},
   "source": [
    "./modul.py - значит что мы ищем modul.py в текущей папке. указываем относительный путь и при переносе проекта файл найдётся, а абсолютный путь сломается. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e36916",
   "metadata": {},
   "source": [
    "Дальше всё зависит от поставленной задачи и от той структуры которую хотим получить. Постобработкой данных. Хорошо подойдёт Монго. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a783715",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa422d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd8a6390",
   "metadata": {},
   "source": [
    "### собрать 5 новостей с сайта яндекс https://dzen.ru\n",
    "Ищу контейнер со всеми пятью новостями через инструмент разработчика: `<ul class='card-news__stories-Bu'>` и через селектор в XPATH: `//ul[@class='card-news__stories-Bu']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1936eef",
   "metadata": {},
   "source": [
    "берём первую новость и вторую новость через XPATH. Объединяем 2 XPATH:\n",
    "`//ul/li[1] | //ul/li[2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a52b384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "url_ya = 'https://dzen.ru/'\n",
    "headers = {'User Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url_ya, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ae625f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [400]>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = html.fromstring(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f658ca63",
   "metadata": {},
   "source": [
    "В результате мы получаем объекты. И у каждого этого объекта имеются свои параметры. и нужный нам текст находится в одном из параметров. Чтобы получить нужный нам текст, дальше надо парсить до конца. \n",
    "\n",
    "Есть контейнер - это тег. Внутри этого тега есть информация. По сути контейнеры - это опорные элементы по пути к данным. Нам нужны сами данные. Так как по концепции дома - всё есть узел, то и текст и атрибуты тоже узлы нашего дерева. Когда простраиваем икс пас можем дойти до внутреннего узла выбранного нами объекта.  В частности мы говорим что хотим собрать все теги `<ul>` с классом `'card-news__stories-Bu'`:\n",
    "\n",
    "`//ul[@class='card-news__stories-Bu']`\n",
    "\n",
    "Да мы собираем теги но по сути нам нужна информация из этих тегов. Чтобы извлечь текстовые узлы из конечного результата нам нужно сделать ещё шаг от нашего объекта - добавить '/' и будем искать среди детей текст. \n",
    "\n",
    "`//ul[@class='card-news__stories-Bu'/text()']`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949167f",
   "metadata": {},
   "source": [
    "XPath позволяет решать более комплексные задачи более простыми вещами. Более тактический инструмент в плане постройки селекторов для выбора элементов страницы. \n",
    "\n",
    "Также есть CSS селлекторы. Они рассматриваются на другом курсе. Эти селекторы предназначены для выборки элементов.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78267f2",
   "metadata": {},
   "source": [
    "### Файл с урока"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f04aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "url = 'https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2380057.m570.l1313&_nkw=iphone&_sacat=0'\n",
    "headers = {'user-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36'}\n",
    "\n",
    "with open('./test.py') as f:\n",
    "    pass\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "dom = html.fromstring(response.text)\n",
    "\n",
    "items = dom.xpath(\"//li[contains(@class,'s-item')]\")\n",
    "\n",
    "list_items = []\n",
    "for item in items[1:]:\n",
    "    item_info = {}\n",
    "    name = item.xpath(\".//h3[@class='s-item__title']/text()\")\n",
    "    link = item.xpath(\".//h3[@class='s-item__title']/../@href\")\n",
    "    price = item.xpath(\".//span[@class='s-item__price']//text()\")\n",
    "    review = item.xpath(\".//div[@class='s-item__reviews']/a/@href\")\n",
    "\n",
    "\n",
    "    item_info['name'] = name\n",
    "    item_info['link'] = link\n",
    "    item_info['price'] = price\n",
    "    item_info['review'] = review\n",
    "    list_items.append(item_info)\n",
    "\n",
    "pprint(list_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336af6a",
   "metadata": {},
   "source": [
    "# Разбор домашней работы\n",
    "\n",
    "### https://github.com/belkanov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7938db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/belkanov\n",
    "\n",
    "\"\"\"\n",
    "Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы получаем должность)\n",
    "с сайтов HH(обязательно) и/или Superjob(по желанию).\n",
    "Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы).\n",
    "Получившийся список должен содержать в себе минимум:\n",
    "Наименование вакансии.\n",
    "Предлагаемую зарплату (разносим в три поля: минимальная и максимальная и валюта. цифры преобразуем к цифрам).\n",
    "Ссылку на саму вакансию.\n",
    "Сайт, откуда собрана вакансия.\n",
    "По желанию можно добавить ещё параметры вакансии (например, работодателя и расположение).\n",
    "Структура должна быть одинаковая для вакансий с обоих сайтов.\n",
    "Общий результат можно вывести с помощью dataFrame через pandas.\n",
    "Сохраните в json либо csv.\n",
    "\"\"\"\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "import json\n",
    "\n",
    "RE_SALARY = re.compile(r'(?:\\d+\\s*){1,}')\n",
    "RE_CURRENCY = re.compile(r'\\D+')\n",
    "MAIN_URL = 'https://hh.ru'\n",
    "VACANCY_URL = f'{MAIN_URL}/search/vacancy'\n",
    "HEADERS = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'\n",
    "}\n",
    "vacancy_name = 'python'\n",
    "PARAMS = {\n",
    "    'text': vacancy_name\n",
    "}\n",
    "\n",
    "Salary = namedtuple('Salary', (\n",
    "    'min',\n",
    "    'max',\n",
    "    'currency'\n",
    "))\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger('job_scraper')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# если включить - можно увидеть редиректы\n",
    "# requests_log = logging.getLogger(\"urllib3\")\n",
    "# requests_log.setLevel(logging.DEBUG)\n",
    "# requests_log.propagate = True\n",
    "\n",
    "\n",
    "def get_response(url, headers, params=None):\n",
    "    timeouts = (5, 5)  # conn, read\n",
    "    response = requests.get(url,\n",
    "                            headers=headers,\n",
    "                            params=params,\n",
    "                            timeout=timeouts)\n",
    "    if response.ok:\n",
    "        # на случай редиректа hh.ru -> rostov.hh.ru\n",
    "        splitted_response_url = response.url.split('/')\n",
    "        new_main_url = f'{splitted_response_url[0]}//{splitted_response_url[2]}'\n",
    "        return response, new_main_url\n",
    "\n",
    "\n",
    "def get_int(re_match):\n",
    "    return int(re_match.group().replace(' ', ''))\n",
    "\n",
    "\n",
    "def get_salary(tag):\n",
    "    if tag is None:\n",
    "        return Salary(None, None, None)\n",
    "\n",
    "    text = clear_tag_text(tag)\n",
    "    # получалось неплохо через всякие сплиты/слайсы/джоины,\n",
    "    # но потом пришли 'бел. руб.' и все сломалось =)\n",
    "    # поэтому регулярки\n",
    "    if 'от' in text:\n",
    "        re_salary = RE_SALARY.search(text)\n",
    "        salary = get_int(re_salary)\n",
    "        re_currency = RE_CURRENCY.search(text, re_salary.end())\n",
    "        return Salary(salary, None, re_currency.group())\n",
    "    elif 'до' in text:\n",
    "        re_salary = RE_SALARY.search(text)\n",
    "        salary = get_int(re_salary)\n",
    "        re_currency = RE_CURRENCY.search(text, re_salary.end())\n",
    "        return Salary(None, salary, re_currency.group())\n",
    "    else:\n",
    "        re_min_salary = RE_SALARY.search(text)\n",
    "        min_salary = get_int(re_min_salary)\n",
    "        re_max_salary = RE_SALARY.search(text, re_min_salary.end())\n",
    "        max_salary = get_int(re_max_salary)\n",
    "        re_currency = RE_CURRENCY.search(text, re_max_salary.end())\n",
    "        return Salary(min_salary, max_salary, re_currency.group())\n",
    "\n",
    "\n",
    "def clear_tag_text(tag):\n",
    "    text = tag.getText()\n",
    "    text = text.replace('\\n', '')\n",
    "    # внезапно вылезло много пробелов\n",
    "    splitted = [word for word in text.split() if word]\n",
    "    text = ' '.join(splitted)\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_vacancy_link(tag):\n",
    "    link = tag.get('href')\n",
    "    return f'{MAIN_URL}{link}'\n",
    "\n",
    "\n",
    "def save_to_file(data, file_name):\n",
    "    with open(file_name, 'w', encoding='utf8') as f:\n",
    "        f.write(data)\n",
    "\n",
    "\n",
    "def parse_response(response, main_url):\n",
    "    vacancies_info = []\n",
    "\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    anchor = soup.find('div', {'class': 'vacancy-serp-content'})\n",
    "\n",
    "    vacancy_results = anchor.find('div', {'data-qa': 'vacancy-serp__results'})\n",
    "    vacancies = vacancy_results.find_all('div', {'class': 'vacancy-serp-item'})\n",
    "    logger.info('Нашел %d вакансий. Обрабатываю...', len(vacancies))\n",
    "    for vacancy in vacancies:\n",
    "        title_tag = vacancy.find('a', {'data-qa': 'vacancy-serp__vacancy-title'})\n",
    "        salary_tag = vacancy.find('span', {'data-qa': 'vacancy-serp__vacancy-compensation'})\n",
    "\n",
    "        vacancy_info = {\n",
    "            'name': clear_tag_text(title_tag),\n",
    "            'salary': get_salary(salary_tag),\n",
    "            'link': title_tag.get('href'),\n",
    "            'site': main_url,\n",
    "        }\n",
    "        vacancies_info.append(vacancy_info)\n",
    "\n",
    "    return vacancies_info, anchor\n",
    "\n",
    "\n",
    "def main():\n",
    "    all_vacancies = []\n",
    "\n",
    "    page_cnt = 1\n",
    "    url = VACANCY_URL\n",
    "    headers = HEADERS\n",
    "    params = PARAMS\n",
    "    while True:\n",
    "        logger.info('Parse page #%d', page_cnt)\n",
    "        response, main_url = get_response(url, headers=headers, params=params)\n",
    "        if not response:\n",
    "            logger.error('NO response from %s', url)\n",
    "            raise SystemExit(1)\n",
    "\n",
    "        try:\n",
    "            vacancies_info, anchor = parse_response(response, main_url)\n",
    "        except ValueError as e:\n",
    "            logger.exception(e)\n",
    "            save_to_file(response.text, 'error_response.html')\n",
    "            raise SystemExit(1)\n",
    "\n",
    "        all_vacancies.extend(vacancies_info)\n",
    "\n",
    "        logger.info('Ищу следующую страницу...')\n",
    "        next_link = anchor.find('a', {'data-qa': 'pager-next'})\n",
    "        if next_link:\n",
    "            logger.info('Нашел.')\n",
    "            url = f'{main_url}{next_link.get(\"href\")}'\n",
    "            params = None\n",
    "            page_cnt += 1\n",
    "            sleep(1)  # не будем спамить запросами\n",
    "        else:\n",
    "            logger.info('Видимо это последняя =) Всего обработано %d страниц',\n",
    "                        page_cnt)\n",
    "            break\n",
    "\n",
    "    with open('vacancies.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_vacancies, f, ensure_ascii=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger.info('--- START')\n",
    "    main()\n",
    "    logger.info('--- END')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add48229",
   "metadata": {},
   "source": [
    "Добавление монго "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0879ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/belkanov\n",
    "\n",
    "\"\"\"\n",
    "1. Развернуть у себя на компьютере/виртуальной машине/хостинге MongoDB и\n",
    "реализовать функцию, которая будет добавлять только новые вакансии в вашу базу.\n",
    "2. Написать функцию, которая производит поиск и выводит на экран вакансии с\n",
    "заработной платой больше введённой суммы (необходимо анализировать оба поля зарплаты).\n",
    "Для тех, кто выполнил задание с Росконтролем - напишите запрос для поиска продуктов\n",
    "с рейтингом не ниже введенного или качеством не ниже введенного\n",
    "(то есть цифра вводится одна, а запрос проверяет оба поля)\n",
    "\"\"\"\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pymongo import MongoClient\n",
    "from pymongo.collection import Collection\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "from constants import *\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger('job_scraper')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# если включить - можно увидеть редиректы\n",
    "# requests_log = logging.getLogger(\"urllib3\")\n",
    "# requests_log.setLevel(logging.DEBUG)\n",
    "# requests_log.propagate = True\n",
    "\n",
    "\n",
    "def get_response(url, headers, params=None):\n",
    "    timeouts = (5, 5)  # conn, read\n",
    "\n",
    "    for i in range(5):\n",
    "        response = requests.get(url,\n",
    "                                headers=headers,\n",
    "                                params=params,\n",
    "                                timeout=timeouts)\n",
    "        if response.ok:\n",
    "            logger.debug('response - OK')\n",
    "            break\n",
    "        else:\n",
    "            logger.debug('response - NOT OK (%s)', response.status_code)\n",
    "            sleep_time = i + 1\n",
    "            logger.warning(f'Не смог получить ответ для %s. Подождем %d сек.',\n",
    "                           response.url,\n",
    "                           sleep_time)\n",
    "            sleep(sleep_time)\n",
    "    else:\n",
    "        raise SystemExit(1, f'Так и не смог получить ответ для {response.url}')\n",
    "\n",
    "    # на случай редиректа hh.ru -> rostov.hh.ru\n",
    "    splitted_response_url = response.url.split('/')\n",
    "    new_main_url = f'{splitted_response_url[0]}//{splitted_response_url[2]}'\n",
    "    return response, new_main_url\n",
    "\n",
    "\n",
    "def get_int(re_match):\n",
    "    return int(re_match.group().replace(' ', ''))\n",
    "\n",
    "\n",
    "def get_salary(tag):\n",
    "    if tag is None:\n",
    "        return Salary(None, None, None)\n",
    "\n",
    "    text = clear_tag_text(tag)\n",
    "    # получалось неплохо через всякие сплиты/слайсы/джоины,\n",
    "    # но потом пришли 'бел. руб.' и все сломалось =)\n",
    "    # поэтому регулярки\n",
    "    if 'от' in text:\n",
    "        re_salary = RE_SALARY.search(text)\n",
    "        salary = get_int(re_salary)\n",
    "        re_currency = RE_CURRENCY.search(text, re_salary.end())\n",
    "        return Salary(salary, None, re_currency.group())\n",
    "    elif 'до' in text:\n",
    "        re_salary = RE_SALARY.search(text)\n",
    "        salary = get_int(re_salary)\n",
    "        re_currency = RE_CURRENCY.search(text, re_salary.end())\n",
    "        return Salary(None, salary, re_currency.group())\n",
    "    else:\n",
    "        re_min_salary = RE_SALARY.search(text)\n",
    "        min_salary = get_int(re_min_salary)\n",
    "        re_max_salary = RE_SALARY.search(text, re_min_salary.end())\n",
    "        max_salary = get_int(re_max_salary)\n",
    "        re_currency = RE_CURRENCY.search(text, re_max_salary.end())\n",
    "        return Salary(min_salary, max_salary, re_currency.group())\n",
    "\n",
    "\n",
    "def clear_tag_text(tag):\n",
    "    text = tag.getText()\n",
    "    text = text.replace('\\n', '')\n",
    "    # внезапно вылезло много пробелов\n",
    "    splitted = [word for word in text.split() if word]\n",
    "    text = ' '.join(splitted)\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_vacancy_link(tag):\n",
    "    link = tag.get('href')\n",
    "    return f'{MAIN_URL}{link}'\n",
    "\n",
    "\n",
    "def save_to_file(data, file_name):\n",
    "    with open(file_name, 'w', encoding='utf8') as f:\n",
    "        f.write(data)\n",
    "\n",
    "\n",
    "def parse_response(response, main_url):\n",
    "    vacancies_info = []\n",
    "\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    anchor = soup.find('div', {'class': 'vacancy-serp-content'})\n",
    "\n",
    "    vacancy_results = anchor.find('div', {'data-qa': 'vacancy-serp__results'})\n",
    "    vacancies = vacancy_results.find_all('div', {'class': 'vacancy-serp-item'})\n",
    "    logger.info('Нашел %d вакансий. Обрабатываю...', len(vacancies))\n",
    "    for vacancy in vacancies:\n",
    "        title_tag = vacancy.find('a', {'data-qa': 'vacancy-serp__vacancy-title'})\n",
    "        salary_tag = vacancy.find('span', {'data-qa': 'vacancy-serp__vacancy-compensation'})\n",
    "\n",
    "        vacancy_info = {\n",
    "            'name': clear_tag_text(title_tag),\n",
    "            'salary': get_salary(salary_tag),\n",
    "            'link': title_tag.get('href'),\n",
    "            'site': main_url,\n",
    "        }\n",
    "        vacancies_info.append(vacancy_info)\n",
    "\n",
    "    return vacancies_info, anchor\n",
    "\n",
    "\n",
    "def get_mongo_collection(collection_name: str) -> Optional[Collection]:\n",
    "    client = MongoClient(host=MONGODB_HOST,\n",
    "                         port=MONGODB_PORT,\n",
    "                         username='gb_mongo_root',\n",
    "                         password='gb_mongo_root_pass')\n",
    "    db = client[MONGODB_DB_NAME]\n",
    "\n",
    "    collection = getattr(db, collection_name, None)\n",
    "    if collection is None:\n",
    "        raise ValueError(1, f\"Коллекция {collection_name} не найдена\")\n",
    "\n",
    "    return collection\n",
    "\n",
    "# ***************************************************************************\n",
    "# Комментарий преподавателя по функции ниже:\n",
    "\n",
    "# Придя на сайт хед хантер видим в названии ссылки присутствует некое числовое значение - 54960009\n",
    "# Это айдишник который даётся в объявлении с вакансией. \n",
    "# Уникальный у каждого объявления. Хед хантер управляет объектами с помощью айдишников\n",
    "# Это значение можно брать в качестве своего значения ай ди для документа\n",
    "# _id = ObjectId(f'{vacancy_id:0>24}')\n",
    "# В итоге 2 одинаковые вакансии к нам в базу не попадут. \n",
    "# Объект значения ай ди в монго может быть любого типа. кроме словарей. \n",
    "# Здесь к этому объекту приводят ObjectId(f'{vacancy_id:0>24}')\n",
    "\n",
    "# Для создания уникального поля ай ди у каждой вакансии уникальная ссылка\n",
    "# Можно взять эту ссылку и прогнать через функцию из библиотеки hashlib\n",
    "# она формирует последовательности для поданных данных. \n",
    "# И для одинаковых данных она сформирует одинаковые последовательности\n",
    "# можно ссылку в хэш перевести и этот хэш взять уникальным айдишником.\n",
    "\n",
    "# Монго проверяет на дубли \n",
    "# except DuplicateKeyError as e:\n",
    "\n",
    "# ***************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_to_mongo(data):\n",
    "    hh_vacancies = get_mongo_collection('hh_vacancies')\n",
    "    # вероятно это не самый эффективный вариант вставки большого кол-ва данных\n",
    "    # (по аналогии с одиночной вставкой в обычном SQL)\n",
    "    # наверное, лучше бы было обогатить ИДшниками data и вставлять через insert_many...\n",
    "    # хз - надо гуглить\n",
    "    # но так проще работать с уже существующими вакансиями\n",
    "    for row in data:\n",
    "        # скорее всего ИД уникален только в рамках города (домена) - надо тестить\n",
    "        # пока норм. всегда можно переделать =)\n",
    "        try:\n",
    "            # https://city.hh.ru/vacancy/54960009?fro...\n",
    "            # -> 54960009\n",
    "            vacancy_id = row['link'].split('/')[4].split('?')[0]\n",
    "            # mongo любит только 24-знаковые ИД\n",
    "            # может это и не best practice, зато не надо новый индекс делать =)\n",
    "            _id = ObjectId(f'{vacancy_id:0>24}')\n",
    "            hh_vacancies.insert_one({'_id': _id, **row})\n",
    "        except DuplicateKeyError as e:\n",
    "            logger.debug('Для ID=%s уже есть запись. Пропускаем', vacancy_id)  # noqa\n",
    "            pass\n",
    "\n",
    "        \n",
    "        \n",
    "# ***************************************************************************\n",
    "# Комментарий преподавателя по функции ниже:\n",
    "\n",
    "# Оператор '$elemMatch': Он берёт значение 'salary' и сравнивает \n",
    "# со всеми значениями salary_value - списка состоящего из зарплат. \n",
    "# и если '$elemMatch' пропусая значение 'salary' по всему списку 'salary_value'\n",
    "# находит элементы большие или равные '$gte'\n",
    "# данный документ вернётся в качестве результата поиска\n",
    "# \n",
    "# ***************************************************************************\n",
    "\n",
    "def filter_vacancies_by_salary(salary_value):\n",
    "    sleep(1)  # дадим логам отлежаться, а то может получиться каша при выводе\n",
    "    print(f'--- Вакансии с ЗП {salary_value:,}+')\n",
    "    hh_vacancies = get_mongo_collection('hh_vacancies')\n",
    "    # запросы конечно смотрятся страшно на фоне обычных SQL =)\n",
    "    for row in hh_vacancies.find({\n",
    "        # либо у нас есть значение больше искомого\n",
    "        '$or': [{'salary': {'$elemMatch': {'$gte': salary_value}}},\n",
    "                {\n",
    "                    # либо у нас указана только начальная зарплата\n",
    "                    '$and': [\n",
    "                        {'salary.0': {'$ne': None}},\n",
    "                        {'salary.1': None},\n",
    "                    ]\n",
    "                }\n",
    "                ]\n",
    "    }):\n",
    "        pprint(row)\n",
    "\n",
    "\n",
    "def main():\n",
    "    page_cnt = 1\n",
    "    url = VACANCY_URL\n",
    "    headers = HEADERS\n",
    "    params = PARAMS\n",
    "    while True:\n",
    "        logger.info('Parse page #%d', page_cnt)\n",
    "        response, main_url = get_response(url, headers=headers, params=params)\n",
    "        if not response:\n",
    "            logger.error('NO response from %s', url)\n",
    "            raise SystemExit(1)\n",
    "\n",
    "        try:\n",
    "            vacancies_info, anchor = parse_response(response, main_url)\n",
    "        except ValueError as e:\n",
    "            logger.exception(e)\n",
    "            save_to_file(response.text, 'error_response.html')\n",
    "            raise SystemExit(1)\n",
    "\n",
    "        logger.info('Сохраняю их в Mongo')\n",
    "        save_to_mongo(vacancies_info)\n",
    "\n",
    "        logger.info('Ищу следующую страницу...')\n",
    "        next_link = anchor.find('a', {'data-qa': 'pager-next'})\n",
    "        if next_link:\n",
    "            logger.info('Нашел.')\n",
    "            url = f'{main_url}{next_link.get(\"href\")}'\n",
    "            params = None\n",
    "            page_cnt += 1\n",
    "            sleep(1)  # не будем спамить запросами\n",
    "        else:\n",
    "            logger.info('Видимо это последняя =) Всего обработано %d страниц',\n",
    "                        page_cnt)\n",
    "            break\n",
    "\n",
    "    filter_vacancies_by_salary(100_000)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger.info('--- START')\n",
    "    main()\n",
    "    logger.info('--- END')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0420b9",
   "metadata": {},
   "source": [
    "### https://github.com/Androkotey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833153c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# мои модули\n",
    "from parser_hh import get_vacancies\n",
    "from mongo_queries import salary_filter\n",
    "\n",
    "\n",
    "# Будем хранить ссылки на вакансии в множестве\n",
    "PRIMARY_KEYS = set()\n",
    "# заменить проверку на\n",
    "# if collection.find_one('link', doc['link']):\n",
    "#     print('Документ существует в базе')\n",
    "# Но лучше сделать update_one с параметром upsert=True\n",
    "\n",
    "\n",
    "def pprint_cursor_object(cursor):\n",
    "    for doc in cursor:\n",
    "        pprint(doc)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def connect_to_mongodb_collection(database_name, collection_name, delete=False):\n",
    "    \"\"\" Открывает соединение, удаляет коллекцию после отработки, если необходимо, и закрывает соединение \"\"\"\n",
    "\n",
    "    client = MongoClient('127.0.0.1', 27017)\n",
    "    db = client[database_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    yield collection\n",
    "\n",
    "    count_documents = collection.count_documents({})\n",
    "    if delete:\n",
    "        db.drop_collection(collection_name)\n",
    "        print(f\"Удалена коллеция с {count_documents} документами\")\n",
    "    client.close()\n",
    "\n",
    "\n",
    "def add_data_to_collection(collection, data):\n",
    "    \"\"\" Добавляет вакансии в коллекцию \"\"\"\n",
    "\n",
    "    for doc in data:\n",
    "        try:\n",
    "            if doc['link'] in PRIMARY_KEYS:\n",
    "                raise DuplicateKeyError('err')\n",
    "            PRIMARY_KEYS.add(doc['link'])\n",
    "            collection.insert_one(doc)\n",
    "        except DuplicateKeyError:\n",
    "            print(f'Вакансия {doc[\"name\"]} в {doc[\"address\"]} уже существует в базе')\n",
    "\n",
    "\n",
    "def get_sample(collection, num=1):\n",
    "    \"\"\" Выбирает несколько записей из коллекции и убирает поля _id (чтобы честно по ссылке проверять дубликаты)\"\"\"\n",
    "\n",
    "    return collection.aggregate([{'$sample': {'size': num}}, {'$project': {\"_id\": 0}}])\n",
    "\n",
    "\n",
    "def main():\n",
    "    with connect_to_mongodb_collection(database_name='hw3_database',\n",
    "                                       collection_name='vacancies',\n",
    "                                       delete=True) as vacancies:\n",
    "        list_of_vacancies = get_vacancies(text='data scientist')  # получили вакансии от парсера\n",
    "        add_data_to_collection(vacancies, list_of_vacancies)  # добавили вакансии в базу\n",
    "        sample = get_sample(vacancies, 4)  # получили набор из случайных вакансий\n",
    "        add_data_to_collection(vacancies, sample)  # попытались добавить набор в коллекцию\n",
    "        pprint_cursor_object(salary_filter(vacancies, 500000))  # фильтруем и выводим вакансии\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "#************************************\n",
    "\n",
    "# Mongo_queries.py\n",
    "\n",
    "# Запросы к базе монго\n",
    "#  суть зпроса ограничивается одной строчкой \n",
    "#  '$match':  # Фильтрация\n",
    "#               {\n",
    "#                   '$or': [{'salary_min_rub': {option: value}}, {'salary_max_rub': {option: value}}]\n",
    "#\n",
    "# Либо 'salary_min_rub' больше введёного значения 'option: value' \n",
    "# либо 'salary_max_rub' больше введёного значения 'option: value'\n",
    "# с помощтю оператора '$or' это достигается. \n",
    "\n",
    "def salary_filter(collection, value, option='$gte'):\n",
    "    \"\"\" Переводит валюту в рубли, создаёт соответствующие поля и фильтрует результат \"\"\"\n",
    "\n",
    "    return collection.aggregate([\n",
    "        {\n",
    "            '$fill':  # Заполнение None\n",
    "                {\n",
    "                    'output':\n",
    "                        {\n",
    "                            'salary_min': {'value': 0},\n",
    "                            'salary_max': {'value': 0},\n",
    "                        }\n",
    "                }\n",
    "        },\n",
    "        {\n",
    "            '$addFields':  # Конвертация валюты\n",
    "                {\n",
    "                    'salary_min_rub': {'$switch': {\n",
    "                        'branches': [\n",
    "                            {'case': {'$eq': ['$salary_currency', 'бел.руб.']}, 'then':\n",
    "                                {'$multiply': ['$salary_min', 30]}},\n",
    "                            {'case': {'$eq': ['$salary_currency', 'руб.']}, 'then':\n",
    "                                {'$multiply': ['$salary_min', 1]}},\n",
    "                            {'case': {'$eq': ['$salary_currency', 'USD']}, 'then':\n",
    "                                {'$multiply': ['$salary_min', 65]}},\n",
    "                            {'case': {'$eq': ['$salary_currency', 'EUR']}, 'then':\n",
    "                                {'$multiply': ['$salary_min', 70]}},\n",
    "                            {'case': {'$eq': ['$salary_currency', 'сум']}, 'then':\n",
    "                                {'$multiply': ['$salary_min', 0.0053]}},\n",
    "                            {'case': {'$eq': ['$salary_currency', 'KZT']}, 'then':\n",
    "                                {'$multiply': ['$salary_min', 0.14]}}\n",
    "                        ],\n",
    "                        'default': None}},\n",
    "                    'salary_max_rub': {'$switch': {\n",
    "                        'branches': [\n",
    "                            {'case': {'$eq': ['$salary_currency', 'бел.руб.']}, 'then':\n",
    "                                {'$multiply': ['$salary_max', 30]}},\n",
    "                            {'case': {'$eq': ['$salary_currency', 'руб.']}, 'then':\n",
    "                                {'$multiply': ['$salary_max', 1]}},\n",
    "                            {'case': {'$eq': ['$salary_currency', 'USD']}, 'then':\n",
    "                                {'$multiply': ['$salary_max', 65]}},\n",
    "                            {'case': {'$eq': ['$salary_currency', 'EUR']}, 'then':\n",
    "                                {'$multiply': ['$salary_max', 70]}},\n",
    "                            {'case': {'$eq': ['$salary_currency', 'сум']}, 'then':\n",
    "                                {'$multiply': ['$salary_max', 0.0053]}},\n",
    "                            {'case': {'$eq': ['$salary_currency', 'KZT']}, 'then':\n",
    "                                {'$multiply': ['$salary_max', 0.14]}}\n",
    "                        ],\n",
    "                        'default': None}}}},\n",
    "        {\n",
    "            '$match':  # Фильтрация\n",
    "                {\n",
    "                    '$or': [{'salary_min_rub': {option: value}}, {'salary_max_rub': {option: value}}]\n",
    "                }\n",
    "        }])\n",
    "#************************************\n",
    "\n",
    "# Parcer_hh.py\n",
    "\n",
    "\"\"\" Немного изменённый код из 2-ой домашней работы \"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "\n",
    "def salary_extraction(vacancy_salary):\n",
    "    salary_dict = {'min': None, 'max': None, 'cur': None}\n",
    "\n",
    "    if vacancy_salary:\n",
    "        raw_salary = vacancy_salary.getText().replace(' – ', ' ').replace(' ', '').split()\n",
    "        if raw_salary[0] == 'до':\n",
    "            # до 380 000 руб.\n",
    "            salary_dict['max'] = int(raw_salary[1])\n",
    "        elif raw_salary[0] == 'от':\n",
    "            # от 50 000 руб.\n",
    "            salary_dict['min'] = int(raw_salary[1])\n",
    "        else:\n",
    "            # 50 000 – 100 000 руб.\n",
    "            salary_dict['min'] = int(raw_salary[0])\n",
    "            salary_dict['max'] = int(raw_salary[1])\n",
    "        salary_dict['cur'] = ''.join(raw_salary[2:])  # решение проблемы бел. руб.\n",
    "\n",
    "    return salary_dict\n",
    "\n",
    "\n",
    "def get_vacancies(text):\n",
    "    main_url = 'https://spb.hh.ru/'\n",
    "    page_link = '/search/vacancy'  # ссылка на первую страницу поиска\n",
    "\n",
    "    params = {'search_field': ['name', 'company_name', 'description'], 'items_on_page': 20,\n",
    "              'text': text}\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                             'Chrome/101.0.4951.67 Safari/537.36'}\n",
    "\n",
    "    vacancies = []\n",
    "    while True:\n",
    "        response = requests.get(main_url+page_link,\n",
    "                                params=params,\n",
    "                                headers=headers)\n",
    "        html = response.text\n",
    "        soup = bs(html, 'html.parser')\n",
    "        vacancies_soup = soup.find_all('div', {'class': ['vacancy-serp-item-body__main-info']})\n",
    "\n",
    "        for vacancy in vacancies_soup:\n",
    "\n",
    "            vacancy_data = {'website': 'hh.ru'}\n",
    "\n",
    "            vacancy_title = vacancy.find('a')\n",
    "            vacancy_name = vacancy_title.getText()\n",
    "            vacancy_link = vacancy_title['href'][: vacancy_title['href'].index('?')]\n",
    "            vacancy_salary = salary_extraction(vacancy.find('span', {'class': ['bloko-header-section-3']}))\n",
    "            try:\n",
    "                vacancy_employer = vacancy.find('a', {'data-qa': 'vacancy-serp__vacancy-employer'}).getText().replace('\\xa0', ' ')\n",
    "                vacancy_address = vacancy.find('div', {'data-qa': 'vacancy-serp__vacancy-address'}).getText().replace('\\xa0', ' ')\n",
    "            except AttributeError:\n",
    "                continue\n",
    "\n",
    "            vacancy_data['name'] = vacancy_name\n",
    "            vacancy_data['link'] = vacancy_link\n",
    "            vacancy_data['salary_min'] = vacancy_salary['min']\n",
    "            vacancy_data['salary_max'] = vacancy_salary['max']\n",
    "            vacancy_data['salary_currency'] = vacancy_salary['cur']\n",
    "            vacancy_data['employer'] = vacancy_employer\n",
    "            vacancy_data['address'] = vacancy_address\n",
    "\n",
    "            vacancies.append(vacancy_data)\n",
    "\n",
    "        next_page = soup.find('a', {'data-qa': 'pager-next'})\n",
    "        if not next_page:\n",
    "            break\n",
    "\n",
    "        page_link = next_page['href']\n",
    "    return vacancies\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(len(get_vacancies('data scientist')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51a2c0",
   "metadata": {},
   "source": [
    "##### Комментарий от преподавателя "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if collection.find_one('link', doc['link']):\n",
    "    print('Документ существует в базе')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917934d7",
   "metadata": {},
   "source": [
    "Для разбора другой работы. про используемую функцию `update_one({'ссылка': vac['ссылка']},{'$setOnInsert': {словарь того что вставляем}} , upsert=True)` \n",
    "\n",
    "если документ по критерию будет найден `({'ссылка': vac['ссылка']})`  Апдейт уан записывает на старые заниси новые. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0892ada",
   "metadata": {},
   "source": [
    "Если вакансия найдена не будет то сработает insert_one() т.к у нас указан параметр upsert=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6737d0",
   "metadata": {},
   "source": [
    "### https://github.com/LittleFox26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa699e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "client = MongoClient('127.0.0.1', 27017)\n",
    "\n",
    "db = client['vacDB']\n",
    "# db.drop_collection(db.vacDB)\n",
    "vacDB = db.vacDB\n",
    "\n",
    "# https://hh.ru/search/vacancy?text=Data+scientist&area=1&salary=&currency_code=RUR&experience=doesNotMatter&order_by=relevance&search_period=0&items_on_page=50&no_magic=true&L_save_area=true&from=suggest_post\n",
    "main_url = 'https://hh.ru'\n",
    "vacancy = 'Data Scientist'\n",
    "page = 0\n",
    "all_vacancies = []\n",
    "params = {'text': vacancy,\n",
    "          'area': 2,\n",
    "          'experience': 'doesNotMatter',\n",
    "          'order_by': 'relevance',\n",
    "          'search_period': 0,\n",
    "          'items_on_page': 19,\n",
    "          'page': page}\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "                         'AppleWebKit/537.36 (KHTML, like Gecko)'\n",
    "                         'Chrome/98.0.4758.141 YaBrowser/22.3.4.731 Yowser/2.5 Safari/537.36'}\n",
    "response = requests.get(main_url + '/search/vacancy', params=params, headers=headers)\n",
    "\n",
    "while True:\n",
    "\n",
    "    with open('page.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    with open('page.html', 'r', encoding='utf-8') as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    vacancies = soup.find_all('div', {'class': 'vacancy-serp-item'})\n",
    "\n",
    "    for vacancy in vacancies:\n",
    "\n",
    "        vacancy_info = {}\n",
    "        vacancy_anchor = vacancy.find('a', {'data-qa': \"vacancy-serp__vacancy-title\"})\n",
    "        vacancy_name = vacancy_anchor.getText()\n",
    "        vacancy_info['name'] = vacancy_name\n",
    "\n",
    "        vacancy_link = vacancy_anchor['href']\n",
    "        vacancy_info['link'] = vacancy_link\n",
    "\n",
    "        vacancy_info['site'] = main_url + '/'\n",
    "\n",
    "        vacancy_salary = vacancy.find('span', {'data-qa': \"vacancy-serp__vacancy-compensation\"})\n",
    "        if vacancy_salary is None:\n",
    "            min_salary = None\n",
    "            max_salary = None\n",
    "            currency = None\n",
    "        else:\n",
    "            vacancy_salary = vacancy_salary.getText()\n",
    "            if vacancy_salary.startswith('до'):\n",
    "                max_salary = int(\"\".join([s for s in vacancy_salary.split() if s.isdigit()]))\n",
    "                min_salary = None\n",
    "                currency = vacancy_salary.split()[-1]\n",
    "\n",
    "            elif vacancy_salary.startswith('от'):\n",
    "                max_salary = None\n",
    "                min_salary = int(\"\".join([s for s in vacancy_salary.split() if s.isdigit()]))\n",
    "                currency = vacancy_salary.split()[-1]\n",
    "\n",
    "            else:\n",
    "                max_salary = int(\"\".join([s for s in vacancy_salary.split('–')[1] if s.isdigit()]))\n",
    "                min_salary = int(\"\".join([s for s in vacancy_salary.split('–')[0] if s.isdigit()]))\n",
    "                currency = vacancy_salary.split()[-1]\n",
    "\n",
    "        vacancy_info['max_salary'] = max_salary\n",
    "        vacancy_info['min_salary'] = min_salary\n",
    "        vacancy_info['currency'] = currency\n",
    "\n",
    "        all_vacancies.append(vacancy_info)\n",
    "\n",
    "    next_button = soup.find('a', {'data-qa': \"pager-next\"})\n",
    "    if next_button is None:\n",
    "        break\n",
    "    else:\n",
    "        response = requests.get(main_url + next_button['href'], headers=headers)\n",
    "\n",
    "print(len(all_vacancies))\n",
    "# pprint(all_vacancies)\n",
    "\n",
    "# ДОБАВЛЕНИЕ ТОЛЬКО НОВЫХ ВАКАНСИЙ В ДБ:\n",
    "\n",
    "\n",
    "def db_update(database, vac_list):\n",
    "\n",
    "# ***************************************************************************\n",
    "# Комментарий преподавателя по функции ниже:\n",
    "\n",
    "# Запрос к базе данных делается безусловный это ПЛОХО \n",
    "# database.find({}) - мы всю массу которая у нас лежит мы поднимаем наверх\n",
    "# в медленные тормознутые пайтоновские списки. \n",
    "# И эти списки ещё начинаем обрабатывать по нескольким значениям\n",
    "# el['name'], el['max_salary'] и т.д.\n",
    "# Нам наружу поднимать не надо всё, а лишь поднять нужный результат. \n",
    "\n",
    "\n",
    "    x = True\n",
    "    for vac in vac_list:\n",
    "        for el in database.find({}):\n",
    "            if el['name'] == vac['name'] and \\\n",
    "               el['max_salary'] == vac['max_salary'] and \\\n",
    "               el['min_salary'] == vac['min_salary'] and \\\n",
    "               el['currency'] == vac['currency']:\n",
    "                x = False\n",
    "                break\n",
    "            else:\n",
    "                x = True\n",
    "        if x:\n",
    "            database.insert_one(vac)\n",
    "    return database\n",
    "\n",
    "\n",
    "db_update(vacDB, all_vacancies)\n",
    "# for i in vacDB.find({}):\n",
    "#     print(i)\n",
    "\n",
    "# Подбор вакансий из ДБ по ЗП:\n",
    "\n",
    "\n",
    "def vacancy_by_salary(database):\n",
    "\n",
    "    try:\n",
    "        salary = int(input('Insert desired salary: '))\n",
    "        s = []\n",
    "        for el in database.find({'$and': [\n",
    "            {'$or': [{'min_salary': {'$type': 'number'}}, {'max_salary': {'$type': 'number'}}]},\n",
    "            {'$or': [{'min_salary': {'$gt': salary}}, {'max_salary': {'$gt': salary}}]}\n",
    "                                         ]}):\n",
    "            s.append(el)\n",
    "        return pprint(s)\n",
    "    except ValueError:\n",
    "        print('Salary should be an integer number.')\n",
    "\n",
    "\n",
    "vacancy_by_salary(vacDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b5c7c",
   "metadata": {},
   "source": [
    "### https://github.com/Templl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError as dke\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "import re\n",
    "\n",
    "#загружаем вакансии\n",
    "with open('data.txt') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "#добавляем индетификатор вакансии\n",
    "for vacancy in data:\n",
    "    link = vacancy['link']\n",
    "    id = re.split('/', link)[4]\n",
    "    vacancy['_id'] = id[:8]\n",
    "\n",
    "\n",
    "###\n",
    "#подключаемся к базе монго\n",
    "client = MongoClient('127.0.0.1', 27017)\n",
    "db = client['vacncy2005']  # база данных\n",
    "vacancy = db.vacancy  # название коллекции\n",
    "\n",
    "#загружаем в базу вакансии\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        vacancy.insert_one(data[i])\n",
    "    except dke:\n",
    "        #print(f'Вакансия \"{data[i][\"name\"]}\" уже есть в базе')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError as dke\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "#current_course =\n",
    "\n",
    "#подключаемся к базе монго\n",
    "client = MongoClient('127.0.0.1', 27017)\n",
    "db = client['vacncy2005']  # база данных\n",
    "vacancy = db.vacancy  # название коллекции\n",
    "\n",
    "# вводе переменных для поиска\n",
    "salary = input('Желаемая зарплата:')\n",
    "currency = input('В какой валюте зарплата:')\n",
    "\n",
    "#salary = 220000\n",
    "#currency = 'руб.'\n",
    "\n",
    "#result = list(vacancy.find({}))\n",
    "#pprint(result)\n",
    "\n",
    "for doc in vacancy.find(\n",
    "        {'мин': {'$gte': salary},\n",
    "         'макс': {'$gte': salary},\n",
    "         'валюта': {'$eq': currency}}\n",
    "):\n",
    "    if doc == None:\n",
    "        print('вакансии с такими параметрами не найдены')\n",
    "    else:\n",
    "        pprint(doc)\n",
    "\n",
    "result = list(vacancy.find(\n",
    "                            {'мин': {'$gte': salary},\n",
    "                             'макс': {'$gte': salary},\n",
    "                             'валюта': {'$eq': currency}}\n",
    "))\n",
    "\n",
    "if len(result):\n",
    "    pprint(result)\n",
    "else:\n",
    "    print('Подходящих вакансий не найдено')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42a846",
   "metadata": {},
   "source": [
    "Есть метод подсчитывающий найденные документы (количество найденных документов): persons.count_documrnts({'age': {'$gte': 30}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fc5c14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9837a7fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1f35274",
   "metadata": {},
   "source": [
    "### собрать 5 новостей с новостного ресурса например https://ria.ru\n",
    "\n",
    "Ищу контейнер со всеми пятью новостями через инструмент разработчика:\n",
    "`<div class=\"cell-list__list\">` и через селектор  XPATH:\n",
    "`//div[@class=\"cell-list__list\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "110b0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "url_ria = 'https://ria.ru/'\n",
    "headers = {'User Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url_ria, headers=headers)\n",
    "\n",
    "dom_ria = html.fromstring(response.text)\n",
    "\n",
    "news = dom_ria.xpath(\"//span[@class='cell-list__item-title']/text()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "835cb7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f707693",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_ria = html.fromstring(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526581e4",
   "metadata": {},
   "source": [
    "Посмотрел через инструмент разработчика интересующий элемент и составил икс пас запрос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b622e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = dom_ria.xpath(\"//span[@class='cell-list__item-title']/text()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d165f",
   "metadata": {},
   "source": [
    "Вывожу первые 5 новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a6a75bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Число жертв землетрясений в Турции превысило 16,5 тысячи ',\n",
       " 'Офис президента Украины отредактировал пост о скорых поставках истребителей ',\n",
       " 'МИД назвал реакцию Запада на очередной расстрел российских пленных постыдной ',\n",
       " 'На Купянском и Краснолиманском направлениях уничтожили более 140 украинских солдат ',\n",
       " 'В Новосибирске задержали сотрудников газовой компании, осматривавших дом перед взрывом ']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bde2a9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcc91c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "url = 'https://www.thesun.co.uk/'\n",
    "headers = {'User Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "dom = html.fromstring(response.text)\n",
    "\n",
    "news = dom.xpath(\"//a[@class='teaser-anchor']/data-headline\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00bac6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b672f74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faef723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
