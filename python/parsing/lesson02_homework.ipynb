{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7644e908",
   "metadata": {},
   "source": [
    "# Методы сбора и обработки данных из сети Интернет\n",
    "### Урок 2. Парсинг HTML. Библиотека Beautiful soup.\n",
    "Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы получаем должность) с сайтов HH(обязательно) и/или Superjob(по желанию). Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы). Получившийся список должен содержать в себе минимум:\n",
    "\n",
    "- Наименование вакансии.\n",
    "- Предлагаемую зарплату (разносим в три поля: минимальная и максимальная и валюта. цифры преобразуем к цифрам).\n",
    "- Ссылку на саму вакансию.\n",
    "- Сайт, откуда собрана вакансия.\n",
    "\n",
    "По желанию можно добавить ещё параметры вакансии (например, работодателя и расположение). Структура должна быть одинаковая для вакансий с обоих сайтов. Общий результат можно вывести с помощью dataFrame через pandas. Сохраните в json либо csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d66c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# https://hh.ru/search/vacancy?text=Data+scientist&area=1&salary=&currency_code=RUR&experience=doesNotMatter&order_by=relevance&search_period=0&items_on_page=50&no_magic=true&L_save_area=true&from=suggest_post\n",
    "main_url = 'https://hh.ru'\n",
    "vacancy = 'Data Scientist'\n",
    "page = 0\n",
    "all_vacancies = []\n",
    "params = {'text': vacancy,\n",
    "          'area': 1,\n",
    "          'experience': 'doesNotMatter',\n",
    "          'order_by': 'relevance',\n",
    "          'search_period': 0,\n",
    "          'items_on_page': 20,\n",
    "          'page': page}\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "                         'AppleWebKit/537.36 (KHTML, like Gecko)'\n",
    "                         'Chrome/98.0.4758.141 YaBrowser/22.3.4.731 Yowser/2.5 Safari/537.36'}\n",
    "response = requests.get(main_url + '/search/vacancy', params=params, headers=headers)\n",
    "soup = bs(response.text, 'html.parser')\n",
    "try:\n",
    "    # Получаем последнюю страничку\n",
    "    last_page = int(soup.find_all('a',{'data-qa':'pager-page'})[-1].text)\n",
    "except:\n",
    "    last_page = 1\n",
    "\n",
    "for i in range(last_page):\n",
    "\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "\n",
    "    vacancies = soup.find_all('div', {'class': 'vacancy-serp-item'})\n",
    "\n",
    "    for vacancy in vacancies:\n",
    "\n",
    "        vacancy_info = {}\n",
    "        vacancy_anchor = vacancy.find('a', {'data-qa': \"vacancy-serp__vacancy-title\"})\n",
    "        vacancy_name = vacancy_anchor.getText()\n",
    "        vacancy_info['name'] = vacancy_name\n",
    "\n",
    "        vacancy_link = vacancy_anchor['href']\n",
    "        vacancy_info['link'] = vacancy_link\n",
    "\n",
    "        vacancy_info['site'] = main_url + '/'\n",
    "\n",
    "        vacancy_salary = vacancy.find('span', {'data-qa': \"vacancy-serp__vacancy-compensation\"})\n",
    "        if vacancy_salary is None:\n",
    "            min_salary = None\n",
    "            max_salary = None\n",
    "            currency = None\n",
    "        else:\n",
    "            vacancy_salary = vacancy_salary.getText()\n",
    "            if vacancy_salary.startswith('РґРѕ'):\n",
    "                max_salary = int(\"\".join([s for s in vacancy_salary.split() if s.isdigit()]))\n",
    "                min_salary = None\n",
    "                currency = vacancy_salary.split()[-1]\n",
    "\n",
    "            elif vacancy_salary.startswith('РѕС‚'):\n",
    "                max_salary = None\n",
    "                min_salary = int(\"\".join([s for s in vacancy_salary.split() if s.isdigit()]))\n",
    "                currency = vacancy_salary.split()[-1]\n",
    "\n",
    "            else:\n",
    "                max_salary = int(\"\".join([s for s in vacancy_salary.split('вЂ“')[1] if s.isdigit()]))\n",
    "                min_salary = int(\"\".join([s for s in vacancy_salary.split('вЂ“')[0] if s.isdigit()]))\n",
    "                currency = vacancy_salary.split()[-1]\n",
    "\n",
    "        vacancy_info['max_salary'] = max_salary\n",
    "        vacancy_info['min_salary'] = min_salary\n",
    "        vacancy_info['currency'] = currency\n",
    "\n",
    "        all_vacancies.append(vacancy_info)\n",
    "\n",
    "    params['page'] += 1\n",
    "    response = requests.get(main_url + '/search/vacancy', params=params, headers=headers)\n",
    "    print(len(all_vacancies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216b712",
   "metadata": {},
   "source": [
    "#### Момент с `user-Agent`\n",
    "В хед хантер (HH) если не передать юзер агент, то он нам даст 404 ошибку. Обязательно нужно передовать. Второй момент - идёт редирект по региону в котором мы живём. Это не является фильтром поиска вакансии. Это нормально. Это разгрузка нагрузки по серверам HH. \n",
    "\n",
    "https://hh.ru/search/vacancy?text=python&area=1 -ссылка на поиск по вакансии python\n",
    "\n",
    "Хед хантер ограничивает нас выдачью 2000 вакансий. Как бы мы к нему не обращались, хоть через апи, но больше 2000 вакансий хед хантер не выдаёт. Это особенность ресурса. Больше 2000 вакансий за один запрос не полуить!\n",
    "\n",
    "Собрать информацию о том сколько страниц с вакансиями. Это исследовать элемент и найти число со страницами `<span>40</span>` . Или исследовать элемент кнопку \"Дальше\". `<a class=\"bloko-button\" rel=\"nofollow\" data-qa=\"pager-next\" href=\"/search/vacancy?text=python&amp;area=1&amp;page=1&amp;hhtmFrom=vacancy_search_list\"><span>дальше</span></a>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6649906",
   "metadata": {},
   "source": [
    "##### Как взять URL для нужной вакансии\n",
    "Смотрим элемент через инструменты разработчика\n",
    "`<a class=\"serp-item__title\" data-qa=\"serp-item__title\" target=\"_blank\" href=\"https://hh.ru/analytics_source/vacancy/76390304?from=vacancy_search_list&amp;query=python&amp;requestId=1675239218829a88533135ef1c35229e&amp;totalVacancies=5151&amp;position=46&amp;source=vacancies\">Специалист в отдел мониторинга и автоматизации (VBA и Python)</a>`\n",
    "Берём тэг `'a'` с атрибутом `'data-qa': 'vacancy-serp__vacancy-title'` \n",
    "и берём у этого тега значение атрибута `href`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff340be",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=vac.find_all('a', {'data-qa': 'vacancy-serp__vacancy-title'}, href=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcaa549",
   "metadata": {},
   "source": [
    "##### Точка входа \n",
    "https://hh.ru/vacancy/75538170?from=vacancy_search_list&query=python\n",
    "У нас есть ссылка со списком вакансий (20 шт на странице). В ссылке прописывается та вакансия, которую мы хотим в качестве параметра: `query=python`. \n",
    "\n",
    "https://hh.ru/search/vacancy?text=Python&from=suggest_post&salary=&area=1&ored_clusters=true&enable_snippets=true\n",
    "Ссылка на поиск по вакансии. Здесь также присутствует наша вакансия в виде параметра. `text=Python`\n",
    "\n",
    "Много разных параметров в ссылке но их не желательно удалять. Браузер так не делает. Лучше максимально подделоваться под браузер. \n",
    "\n",
    "Один из вариантов как можно указать ссылку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f53999d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancy = 'python'\n",
    "url = f'https://hh.ru/search/vacancy?text={vacancy}&from=suggest_post&salary=&area=1&ored_clusters=true&enable_snippets=true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f68ab",
   "metadata": {},
   "source": [
    "Ньюанс связанный с количеством страниц. на странице 50 вакансий, но мы получаем не больше 20. Переделали страницу и сделали динамическую выдачу данных. Остальные 30 страниц выдачи получаются с помощью динамики. \n",
    "\n",
    "В настройках страницы ставим показывать 20 страниц и не теряем динамического контента. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314e8e4",
   "metadata": {},
   "source": [
    "Посмотреть куда мы пришли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "628cf2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://spb.hh.ru/search/vacancy?search_field=name&search_field=company_name&search_field=description&search_field=name&search_field=company_name&search_field=description&search_field=name&search_field=company_name&search_field=description&search_field=name&search_field=company_name&search_field=description&text=python&text=python&text=python&text=python&page=4&hhtmFrom=vacancy_search_list&search_field=name&search_field=company_name&search_field=description&text=python'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.url "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b82e7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Работа над парсером самостоятельная\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b65a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html5lib\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: webencodings in c:\\users\\777\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\777\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from html5lib) (1.16.0)\n",
      "Installing collected packages: html5lib\n",
      "Successfully installed html5lib-1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "! pip install html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f60b2a",
   "metadata": {},
   "source": [
    "Загружаем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7523447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59b1ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "vacancy = 'python'\n",
    "url = f'https://hh.ru/search/vacancy?text={vacancy}&from=suggest_post&salary=&area=1&ored_clusters=true&enable_snippets=true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0e380",
   "metadata": {},
   "source": [
    "В хед хантер (HH) если не передать юзер агент в заголовках, то он нам даст 404 ошибку. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518c13c",
   "metadata": {},
   "source": [
    "#### Get запрос \n",
    "Выполняем `get` запрос на сайт хедхантер по указанному выше `url` и указываем заголовки ` headers=headers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4be22c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url, headers=headers)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd6a42",
   "metadata": {},
   "source": [
    "#### DOM\n",
    "Получаем DOM . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b83d10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e4f17",
   "metadata": {},
   "source": [
    "Ищем в инструментах разработчика необходимый нам элемент. Текст с профессией. \n",
    "\n",
    "`<a class=\"serp-item__title\" data-qa=\"serp-item__title\" target=\"_blank\" href=\"https://hh.ru/analytics_source/vacancy/68077706?from=vacancy_search_list&amp;query=Python&amp;requestId=1675242703327964ca472d0a0c76effa&amp;totalVacancies=5168&amp;position=0&amp;source=vacancies\">Python разработчик</a>`\n",
    "\n",
    "Разбираем DOM на запчасти. Собираем тег `<a>` с классом `class=\"serp-item__title`. Внутри этого тега у нас есть ссылка на вакансию и название специальности. сама вакансия. 2 единицы информации необходимой нам хронится в каждом элементе списка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05c46a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vacations = soup.find_all('a', {'class': 'serp-item__title'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbec23",
   "metadata": {},
   "source": [
    "У нас собирается список со всеми тегами `<a>` Посмотрим длину списка, она должна быть равной 20, т.к на странице у нас показано 20 вакансий. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80fde471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vacations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b80d9",
   "metadata": {},
   "source": [
    "Посмотрим на то что мы собрали, Посмотрим на самый первый собранный элемент. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "185acd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"serp-item__title\" data-qa=\"serp-item__title\" href=\"https://hh.ru/vacancy/68077706?from=vacancy_search_list&amp;query=python\" target=\"_blank\">Python разработчик</a>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbe518",
   "metadata": {},
   "source": [
    "#### Собираем список со специальностями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed860425",
   "metadata": {},
   "outputs": [],
   "source": [
    "vacs_list = []\n",
    "for el in vacations:\n",
    "    vacs_list.append(el.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5ce7ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python разработчик',\n",
       " 'Python developer',\n",
       " 'Python Devops (DSCore)',\n",
       " 'Senior Python разработчик',\n",
       " 'Старший дата-инженер / Senior Data Engineer',\n",
       " 'Senior ML Engineer',\n",
       " 'DevOps Engineer',\n",
       " 'Продуктовый аналитик в направление «Работодательские сервисы»',\n",
       " 'Middle/Senior продуктовый аналитик в направление монетизации',\n",
       " 'Middle/Senior Продуктовый аналитик в направление работодатели',\n",
       " 'Data Scientist (Senior)',\n",
       " 'Middle / Senior QA в команду безопасности',\n",
       " 'Middle / Senior QA инженер',\n",
       " 'Администратор Clickhouse',\n",
       " 'Build system Python developer',\n",
       " 'Python разработчик',\n",
       " 'Junior web-программист',\n",
       " 'Разработчик Python',\n",
       " 'Разработчик Python',\n",
       " 'Тестировщик ПО (Python)']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aba7cef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79c778a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9474550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ee48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8372f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "437cc4cb",
   "metadata": {},
   "source": [
    "### Пример решения домашнего задания\n",
    "https://github.com/Androkotey\n",
    "\n",
    "Простое и хорошее решение строковомы методами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b47791fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите вакансию для поиска: python\n",
      "Page 0 is being processed...\n",
      "Page 0 done\n",
      "Page 1 is being processed...\n",
      "Page 1 done\n",
      "Page 2 is being processed...\n",
      "Page 2 done\n",
      "Page 3 is being processed...\n",
      "Page 3 done\n",
      "Page 4 is being processed...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'getText'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m vacancy_link \u001b[38;5;241m=\u001b[39m vacancy_title[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m][: vacancy_title[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     58\u001b[0m vacancy_salary \u001b[38;5;241m=\u001b[39m salary_extraction(vacancy\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbloko-header-section-3\u001b[39m\u001b[38;5;124m'\u001b[39m]}))\n\u001b[1;32m---> 59\u001b[0m vacancy_employer \u001b[38;5;241m=\u001b[39m vacancy\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-qa\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvacancy-serp__vacancy-employer\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mgetText()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     60\u001b[0m vacancy_address \u001b[38;5;241m=\u001b[39m vacancy\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-qa\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvacancy-serp__vacancy-address\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mgetText()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m vacancy_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m vacancy_name\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'getText'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Функция для зарплаты\n",
    "def salary_extraction(vacancy_salary):\n",
    "    salary_dict = {'min': None, 'max': None, 'cur': None}\n",
    "\n",
    "    if vacancy_salary:\n",
    "        # делится всё по этому символу `' – '`?\n",
    "        # удаляются пробелы и получаем общую структуру\n",
    "        raw_salary = vacancy_salary.getText().replace(' – ', ' ').replace(' ', '').split()\n",
    "        # Если в списке есть ДО\n",
    "        if raw_salary[0] == 'до':\n",
    "            # до 380 000 руб.\n",
    "            # тогда сэлери макс берётся из первого элемента списка\n",
    "            salary_dict['max'] = int(raw_salary[1])\n",
    "        elif raw_salary[0] == 'от':\n",
    "            # от 50 000 руб.\n",
    "            # берём минимальную зарплату\n",
    "            salary_dict['min'] = int(raw_salary[1])\n",
    "        else:\n",
    "            # 50 000 – 100 000 руб.\n",
    "            salary_dict['min'] = int(raw_salary[0])\n",
    "            salary_dict['max'] = int(raw_salary[1])\n",
    "        salary_dict['cur'] = raw_salary[2].replace('.', '')\n",
    "\n",
    "    return salary_dict\n",
    "\n",
    "\n",
    "main_url = 'https://spb.hh.ru/'\n",
    "\n",
    "params = {'search_field': ['name', 'company_name', 'description']}\n",
    "params['text'] = input('Введите вакансию для поиска: ')\n",
    "max_page = 99999  # не вижу смысла в ограничении количества страниц\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'}\n",
    "page_link = '/search/vacancy'\n",
    "vacancies = []\n",
    "i = 0\n",
    "\n",
    "# Снаружи внешний цикл\n",
    "# который также завязан на следующей странице\n",
    "# if not next_page\n",
    "\n",
    "while True:\n",
    "    response = requests.get(main_url + page_link,\n",
    "                            params=params,\n",
    "                            headers=headers)\n",
    "    html = response.text\n",
    "    soup = bs(html, 'html.parser')\n",
    "    # собираются вакансии\n",
    "    vacancies_soup = soup.find_all('div', {'class': ['vacancy-serp-item-body__main-info']})\n",
    "\n",
    "    print(f'Page {i} is being processed...')\n",
    "    for vacancy in vacancies_soup:\n",
    "\n",
    "        vacancy_data = {'website': 'hh.ru'}\n",
    "\n",
    "        vacancy_title = vacancy.find('a')\n",
    "\n",
    "        vacancy_name = vacancy_title.getText()\n",
    "        vacancy_link = vacancy_title['href'][: vacancy_title['href'].index('?')]\n",
    "        vacancy_salary = salary_extraction(vacancy.find('span', {'class': ['bloko-header-section-3']}))\n",
    "        vacancy_employer = vacancy.find('a', {'data-qa': 'vacancy-serp__vacancy-employer'}).getText().replace('\\xa0', ' ')\n",
    "        vacancy_address = vacancy.find('div', {'data-qa': 'vacancy-serp__vacancy-address'}).getText().replace('\\xa0', ' ')\n",
    "\n",
    "        vacancy_data['name'] = vacancy_name\n",
    "        vacancy_data['link'] = vacancy_link\n",
    "        vacancy_data['salary_min'] = vacancy_salary['min']\n",
    "        vacancy_data['salary_max'] = vacancy_salary['max']\n",
    "        vacancy_data['salary_currency'] = vacancy_salary['cur']\n",
    "        vacancy_data['employer'] = vacancy_employer\n",
    "        vacancy_data['address'] = vacancy_address\n",
    "\n",
    "        vacancies.append(vacancy_data)\n",
    "# проверяем, если мы не нашли кнопку следующая или достигли максимальной страницы\n",
    "# по параметру max_page можно ограничить число страниц для выдачи. \n",
    "    next_page = soup.find('a', {'data-qa': 'pager-next'})\n",
    "    if not next_page or (i == max_page):\n",
    "        break\n",
    "    page_link = next_page['href']\n",
    "    print(f'Page {i} done')\n",
    "    i += 1\n",
    "\n",
    "vacancies_data = pd.DataFrame(data=vacancies)\n",
    "prefix = '_'.join(params['text'].split())\n",
    "vacancies_data.to_csv(f'hh_vacancies_{prefix}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70199fa",
   "metadata": {},
   "source": [
    "##  Пример решения домашнего задания\n",
    "https://github.com/belkanov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7444f823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 13:18:28 | INFO     | job_scraper | --- START\n",
      "2023-02-01 13:18:28 | INFO     | job_scraper | Parse page #1\n",
      "2023-02-01 13:18:30 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:18:30 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:18:30 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:18:31 | INFO     | job_scraper | Parse page #2\n",
      "2023-02-01 13:18:32 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:18:32 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:18:32 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:18:33 | INFO     | job_scraper | Parse page #3\n",
      "2023-02-01 13:18:35 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:18:35 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:18:35 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:18:36 | INFO     | job_scraper | Parse page #4\n",
      "2023-02-01 13:18:38 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:18:38 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:18:38 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:18:39 | INFO     | job_scraper | Parse page #5\n",
      "2023-02-01 13:18:40 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:18:40 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:18:40 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:18:41 | INFO     | job_scraper | Parse page #6\n",
      "2023-02-01 13:18:43 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:18:43 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:18:43 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:18:44 | INFO     | job_scraper | Parse page #7\n",
      "2023-02-01 13:18:46 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:18:46 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:18:46 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:18:47 | INFO     | job_scraper | Parse page #8\n",
      "2023-02-01 13:19:10 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:10 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:10 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:11 | INFO     | job_scraper | Parse page #9\n",
      "2023-02-01 13:19:13 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:13 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:13 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:14 | INFO     | job_scraper | Parse page #10\n",
      "2023-02-01 13:19:16 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:16 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:16 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:17 | INFO     | job_scraper | Parse page #11\n",
      "2023-02-01 13:19:19 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:19 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:19 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:20 | INFO     | job_scraper | Parse page #12\n",
      "2023-02-01 13:19:21 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:21 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:21 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:22 | INFO     | job_scraper | Parse page #13\n",
      "2023-02-01 13:19:24 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:24 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:24 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:25 | INFO     | job_scraper | Parse page #14\n",
      "2023-02-01 13:19:27 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:27 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:27 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:28 | INFO     | job_scraper | Parse page #15\n",
      "2023-02-01 13:19:29 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:29 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:29 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:30 | INFO     | job_scraper | Parse page #16\n",
      "2023-02-01 13:19:33 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:33 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:33 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:34 | INFO     | job_scraper | Parse page #17\n",
      "2023-02-01 13:19:35 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:35 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:35 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:36 | INFO     | job_scraper | Parse page #18\n",
      "2023-02-01 13:19:41 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:41 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:41 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:42 | INFO     | job_scraper | Parse page #19\n",
      "2023-02-01 13:19:44 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:44 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:44 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:45 | INFO     | job_scraper | Parse page #20\n",
      "2023-02-01 13:19:46 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:46 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:46 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:47 | INFO     | job_scraper | Parse page #21\n",
      "2023-02-01 13:19:49 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:49 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:49 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:50 | INFO     | job_scraper | Parse page #22\n",
      "2023-02-01 13:19:52 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:52 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:52 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:53 | INFO     | job_scraper | Parse page #23\n",
      "2023-02-01 13:19:55 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:55 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:55 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:19:56 | INFO     | job_scraper | Parse page #24\n",
      "2023-02-01 13:19:59 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:19:59 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:19:59 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:00 | INFO     | job_scraper | Parse page #25\n",
      "2023-02-01 13:20:03 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:03 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:03 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:04 | INFO     | job_scraper | Parse page #26\n",
      "2023-02-01 13:20:05 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:05 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:05 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:06 | INFO     | job_scraper | Parse page #27\n",
      "2023-02-01 13:20:08 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:08 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:08 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:09 | INFO     | job_scraper | Parse page #28\n",
      "2023-02-01 13:20:11 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:11 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:11 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:12 | INFO     | job_scraper | Parse page #29\n",
      "2023-02-01 13:20:14 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:14 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:14 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:15 | INFO     | job_scraper | Parse page #30\n",
      "2023-02-01 13:20:17 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:17 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:17 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:18 | INFO     | job_scraper | Parse page #31\n",
      "2023-02-01 13:20:19 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 13:20:19 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:19 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:20 | INFO     | job_scraper | Parse page #32\n",
      "2023-02-01 13:20:22 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:22 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:22 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:23 | INFO     | job_scraper | Parse page #33\n",
      "2023-02-01 13:20:25 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:25 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:25 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:26 | INFO     | job_scraper | Parse page #34\n",
      "2023-02-01 13:20:28 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:28 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:28 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:29 | INFO     | job_scraper | Parse page #35\n",
      "2023-02-01 13:20:30 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:30 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:30 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:31 | INFO     | job_scraper | Parse page #36\n",
      "2023-02-01 13:20:32 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:32 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:32 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:33 | INFO     | job_scraper | Parse page #37\n",
      "2023-02-01 13:20:35 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:35 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:35 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:36 | INFO     | job_scraper | Parse page #38\n",
      "2023-02-01 13:20:38 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:38 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:38 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:39 | INFO     | job_scraper | Parse page #39\n",
      "2023-02-01 13:20:41 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:41 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:41 | INFO     | job_scraper | Нашел.\n",
      "2023-02-01 13:20:42 | INFO     | job_scraper | Parse page #40\n",
      "2023-02-01 13:20:44 | INFO     | job_scraper | Нашел 0 вакансий. Обрабатываю...\n",
      "2023-02-01 13:20:44 | INFO     | job_scraper | Ищу следующую страницу...\n",
      "2023-02-01 13:20:44 | INFO     | job_scraper | Видимо это последняя =) Всего обработано 40 страниц\n",
      "2023-02-01 13:20:44 | INFO     | job_scraper | --- END\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы получаем должность)\n",
    "с сайтов HH(обязательно) и/или Superjob(по желанию).\n",
    "Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы).\n",
    "Получившийся список должен содержать в себе минимум:\n",
    "Наименование вакансии.\n",
    "Предлагаемую зарплату (разносим в три поля: минимальная и максимальная и валюта. цифры преобразуем к цифрам).\n",
    "Ссылку на саму вакансию.\n",
    "Сайт, откуда собрана вакансия.\n",
    "По желанию можно добавить ещё параметры вакансии (например, работодателя и расположение).\n",
    "Структура должна быть одинаковая для вакансий с обоих сайтов.\n",
    "Общий результат можно вывести с помощью dataFrame через pandas.\n",
    "Сохраните в json либо csv.\n",
    "\"\"\"\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "import json\n",
    "\n",
    "RE_SALARY = re.compile(r'(?:\\d+\\s*){1,}')\n",
    "RE_CURRENCY = re.compile(r'\\D+')\n",
    "MAIN_URL = 'https://hh.ru'\n",
    "VACANCY_URL = f'{MAIN_URL}/search/vacancy'\n",
    "HEADERS = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'\n",
    "}\n",
    "vacancy_name = 'python'\n",
    "PARAMS = {\n",
    "    'text': vacancy_name\n",
    "}\n",
    "\n",
    "Salary = namedtuple('Salary', (\n",
    "    'min',\n",
    "    'max',\n",
    "    'currency'\n",
    "))\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger('job_scraper')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# если включить - можно увидеть редиректы\n",
    "# requests_log = logging.getLogger(\"urllib3\")\n",
    "# requests_log.setLevel(logging.DEBUG)\n",
    "# requests_log.propagate = True\n",
    "\n",
    "\n",
    "def get_response(url, headers, params=None):\n",
    "    timeouts = (5, 5)  # conn, read\n",
    "    response = requests.get(url,\n",
    "                            headers=headers,\n",
    "                            params=params,\n",
    "                            timeout=timeouts)\n",
    "    if response.ok:\n",
    "        # на случай редиректа hh.ru -> rostov.hh.ru\n",
    "        splitted_response_url = response.url.split('/')\n",
    "        new_main_url = f'{splitted_response_url[0]}//{splitted_response_url[2]}'\n",
    "        return response, new_main_url\n",
    "\n",
    "# Отдельная функция, которая преобразует полученные значения зарплаты к числовому типу данных\n",
    "def get_int(re_match):\n",
    "    return int(re_match.group().replace(' ', ''))\n",
    "\n",
    "# Проверка зарплатных полей на None значения и обработка зарплат\n",
    "def get_salary(tag):\n",
    "    if tag is None:\n",
    "        return Salary(None, None, None)\n",
    "\n",
    "    text = clear_tag_text(tag)\n",
    "    # получалось неплохо через всякие сплиты/слайсы/джоины,\n",
    "    # но потом пришли 'бел. руб.' и все сломалось =)\n",
    "    # поэтому регулярки\n",
    "    if 'от' in text:\n",
    "        re_salary = RE_SALARY.search(text)\n",
    "        salary = get_int(re_salary)\n",
    "        re_currency = RE_CURRENCY.search(text, re_salary.end())\n",
    "        return Salary(salary, None, re_currency.group())\n",
    "    elif 'до' in text:\n",
    "        re_salary = RE_SALARY.search(text)\n",
    "        salary = get_int(re_salary)\n",
    "        re_currency = RE_CURRENCY.search(text, re_salary.end())\n",
    "        return Salary(None, salary, re_currency.group())\n",
    "    else:\n",
    "        re_min_salary = RE_SALARY.search(text)\n",
    "        min_salary = get_int(re_min_salary)\n",
    "        re_max_salary = RE_SALARY.search(text, re_min_salary.end())\n",
    "        max_salary = get_int(re_max_salary)\n",
    "        re_currency = RE_CURRENCY.search(text, re_max_salary.end())\n",
    "        return Salary(min_salary, max_salary, re_currency.group())\n",
    "    \n",
    "    \n",
    "# Функция удаляет символы переноса строчек \n",
    "\n",
    "def clear_tag_text(tag):\n",
    "    text = tag.getText()\n",
    "    text = text.replace('\\n', '')\n",
    "    # внезапно вылезло много пробелов\n",
    "    splitted = [word for word in text.split() if word]\n",
    "    text = ' '.join(splitted)\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_vacancy_link(tag):\n",
    "    link = tag.get('href')\n",
    "    return f'{MAIN_URL}{link}'\n",
    "\n",
    "\n",
    "def save_to_file(data, file_name):\n",
    "    with open(file_name, 'w', encoding='utf8') as f:\n",
    "        f.write(data)\n",
    "\n",
    "# Сам алгоритм перебора данных. \n",
    "# делаем запрос по ссылке\n",
    "# получаем вакансии\n",
    "\n",
    "# Функция с циклом которая перебирает вакансии на одной странице.\n",
    "\n",
    "def parse_response(response, main_url):\n",
    "    vacancies_info = []\n",
    "\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    anchor = soup.find('div', {'class': 'vacancy-serp-content'})\n",
    "\n",
    "    vacancy_results = anchor.find('div', {'data-qa': 'vacancy-serp__results'})\n",
    "    # здесь получаются наши вакансии через find_all()\n",
    "    vacancies = vacancy_results.find_all('div', {'class': 'vacancy-serp-item'})\n",
    "    logger.info('Нашел %d вакансий. Обрабатываю...', len(vacancies))\n",
    "    # начинаем итеррироваться по всему списку, \n",
    "    # заходя внутрь и получая данные по каждой вакансии\n",
    "    for vacancy in vacancies:\n",
    "        title_tag = vacancy.find('a', {'data-qa': 'vacancy-serp__vacancy-title'})\n",
    "        salary_tag = vacancy.find('span', {'data-qa': 'vacancy-serp__vacancy-compensation'})\n",
    "        \n",
    "        # словарик со всей необходимой нам информацией. \n",
    "        # для каждого данного возвращается функция и всё нам \"причёсывает\"\n",
    "        vacancy_info = {\n",
    "            'name': clear_tag_text(title_tag),\n",
    "            'salary': get_salary(salary_tag),\n",
    "            'link': title_tag.get('href'),\n",
    "            'site': main_url,\n",
    "        }\n",
    "        vacancies_info.append(vacancy_info)\n",
    "\n",
    "    return vacancies_info, anchor\n",
    "\n",
    "\n",
    "def main():\n",
    "    all_vacancies = []\n",
    "\n",
    "    page_cnt = 1\n",
    "    url = VACANCY_URL\n",
    "    headers = HEADERS\n",
    "    params = PARAMS\n",
    "    \n",
    "    # Второй главный цикл, который перебирает страницы, \n",
    "    # обрабатывая каждую страницу с помощью функций\n",
    "    while True:\n",
    "        logger.info('Parse page #%d', page_cnt)\n",
    "        response, main_url = get_response(url, headers=headers, params=params)\n",
    "        # если ответ не пришёл\n",
    "        if not response:\n",
    "            logger.error('NO response from %s', url)\n",
    "            raise SystemExit(1)\n",
    "        \n",
    "        try:\n",
    "            vacancies_info, anchor = parse_response(response, main_url)\n",
    "        except ValueError as e:\n",
    "            logger.exception(e)\n",
    "            save_to_file(response.text, 'error_response.html')\n",
    "            raise SystemExit(1)\n",
    "\n",
    "        all_vacancies.extend(vacancies_info)\n",
    "\n",
    "        logger.info('Ищу следующую страницу...')\n",
    "        next_link = anchor.find('a', {'data-qa': 'pager-next'})\n",
    "        # переход по страницам как осуществляется? \n",
    "        # Ищем кнопку следующая на странице и ссылку которая под этой кнопкой бывает\n",
    "        # извлекаем ссылку на следующую страницу\n",
    "        \n",
    "        # дальше по этой ссылке снова выполняем get-запрос.w\n",
    "        if next_link:\n",
    "            logger.info('Нашел.')\n",
    "            url = f'{main_url}{next_link.get(\"href\")}'\n",
    "            params = None\n",
    "            page_cnt += 1\n",
    "            sleep(1)  # не будем спамить запросами\n",
    "        else:\n",
    "            logger.info('Видимо это последняя =) Всего обработано %d страниц',\n",
    "                        page_cnt)\n",
    "            break\n",
    "\n",
    "    with open('vacancies.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_vacancies, f, ensure_ascii=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger.info('--- START')\n",
    "    main()\n",
    "    logger.info('--- END')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0073821",
   "metadata": {},
   "source": [
    "## Пример парсера, но изменилась структура сайта и парсер уже не отрабатывает "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb0cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "headers = {'user-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13B137 Safari/601.1'}\n",
    "url = 'https://msk.hh.ru/search/vacancy'\n",
    "\n",
    "def find_vacations(vac_name):\n",
    "    ### Понимаю, что код с двумя вложенными циклами трудно назвать оптимальным, \n",
    "    ### но на что-то лучшее, к сожалению, не хватило времени. Очень большие выдачи считает\n",
    "    ### довольно долго (5-7 мин)\n",
    "    ### Максимальное число ответов в выдаче, независимо от их реального числа, - не более 2000. Видимо, опять сайт режет?\n",
    "    ### Добавил в выдачу еще и работодателя, хотя этого не было в задании.\n",
    "    params = {'text' : vac_name}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    c = soup.find('h1', {'data-qa': 'bloko-header-3'})\n",
    "    x = re.findall('[0-9]+', c.get_text())\n",
    "    number = int(''.join(map(str, x)))\n",
    "    number = int(number/20) #Нашел на странице строчку, где указано общее число запросов в выдаче, вынул оттуда это число, разделил на 20 (это значение числа вакансий #на странице установил принудительно ниже) и так нашел общее число страниц в выдаче\n",
    "    \n",
    "    lst = []\n",
    "    page  = 0\n",
    "    while page <= number:\n",
    "        params = {'text' : vac_name, 'page' : page, 'items_on_page' : 20}\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        vacations = soup.find_all('div', {'class': 'vacancy-serp-item'})\n",
    "        for vac in vacations:\n",
    "            temp = {}\n",
    "            a=vac.find_all('span', {'class': 'g-user-content'})\n",
    "            for elt in a:\n",
    "                temp['Должность'] = elt.text\n",
    "            b=vac.find_all('span', {'data-qa': 'vacancy-serp__vacancy-compensation'})\n",
    "            for elt in b:\n",
    "                temp['Зарплата'] = elt.text.replace('\\\\u202f','')\n",
    "            c=vac.find_all('a', {'data-qa': 'vacancy-serp__vacancy-employer'}) \n",
    "            for elt in c:\n",
    "                temp['Работодатель'] = elt.text\n",
    "            d=vac.find_all('a', {'data-qa': 'vacancy-serp__vacancy-title'}, href=True) \n",
    "            for elt in d:\n",
    "                temp['Ссылка на вакансию'] = elt['href'] \n",
    "            lst.append(temp)\n",
    "            page += 1\n",
    "    x = pd.DataFrame(lst)\n",
    "\n",
    "    x['Валюта'] = x['Зарплата'].replace('[0-9–дот]','',regex=True)\n",
    "    x['Мин.зарплата'] = np.select([x['Зарплата'].str.contains('от',na=False), x['Зарплата'].str.contains('до',na=False), x['Зарплата'].str.contains('–',na=False)],\n",
    "    [x['Зарплата'].replace('[^\\\\d]','',regex=True),0, x['Зарплата'].replace('–.*','',regex=True)])\n",
    "\n",
    "    x['Макс.зарплата'] = np.select([x['Зарплата'].str.contains('от',na=False), x['Зарплата'].str.contains('до',na=False), x['Зарплата'].str.contains('–',na=False)],\n",
    "    [0, x['Зарплата'].replace('[^\\\\d]','',regex=True), x['Зарплата'].replace('.*–','',regex=True).replace('[^\\\\d]','',regex=True)])\n",
    "    x.fillna('не указана', inplace=True)\n",
    "    x.drop('Зарплата',axis=1,inplace=True)\n",
    "    x = x.reindex(columns=['Должность', 'Работодатель', 'Мин.зарплата', 'Макс.зарплата', 'Валюта', 'Ссылка на вакансию'])\n",
    "    x[['Мин.зарплата', 'Макс.зарплата']] = x[['Мин.зарплата', 'Макс.зарплата']].astype('int')\n",
    "    x.index +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82f0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
