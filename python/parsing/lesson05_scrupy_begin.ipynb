{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0cf82b1",
   "metadata": {},
   "source": [
    "# Методы сбора и обработки данных при помощи Python\n",
    "\n",
    "## Scrapy\n",
    "\n",
    "Оглавление:\n",
    "- Scrapy\n",
    "    >Как устроен Scrapy?\n",
    "\n",
    "    >Установка\n",
    "- Создание поискового робота\n",
    "- Домашнее задание\n",
    "- Используемая литература\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c90ac",
   "metadata": {},
   "source": [
    "### Scrapy\n",
    "Scrapy — один из наиболее популярных и производительных фреймворков Python для получения\n",
    "данных с веб-страниц. Он включает в себя большинство общих функциональных возможностей. Вам\n",
    "не придётся самостоятельно прописывать многие функции. Scrapy позволяет быстро и без труда\n",
    "создать «веб-паука»."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ac039",
   "metadata": {},
   "source": [
    "### Как устроен Scrapy?\n",
    "`Движок Scrapy (Scrapy Engine)`- отвечает за обработку элементов после их извлечения или\n",
    "очистки «пауками». Типичные задачи включают очистку, проверку и сохранение. Например,\n",
    "сохранение элемента в базе данных. Движок отвечает за координацию между всеми компонентами.\n",
    "\n",
    "`Планировщик (Scheduler)` — получает запросы от движка и ставит их в очередь для передачи позже\n",
    "(также в движок), когда движок их запрашивает. По сути, планировщик определяет порядок операций.\n",
    "\n",
    "`Загрузчик (Downloader)` — отвечает за получение веб-страниц и передачу их движку, который, в\n",
    "свою очередь, передаёт их поисковым роботам.\n",
    "\n",
    "`Пауки (Spiders)` — это настраиваемые классы, написанные пользователями Scrapy для парсинга\n",
    "данных. Каждый паук может обрабатывать определённый домен или группу доменов.\n",
    "\n",
    "`Конвейер элементов (Item Pipelines)` — отвечает за обработку элементов после их извлечения или\n",
    "очистки «пауками». Типичные задачи включают очистку, проверку и сохранение. Например,\n",
    "сохранение элемента в базе данных.\n",
    "\n",
    "`Промежуточное программное обеспечение загрузчика (Downloader middlewares)` — отвечают за\n",
    "загрузку разметки сайта. Они предоставляют удобный механизм расширения функциональности\n",
    "Scrapy путём добавления пользовательского кода.\n",
    "\n",
    "`Промежуточное программное обеспечение паука (Spider middlewares)` — отвечают за возврат\n",
    "данных. Они предоставляют удобный механизм для расширения функциональности Scrapy путём\n",
    "добавления пользовательского кода. В Spider middlewares находятся пользовательские заголовки\n",
    "(headers), а также проксирование.\n",
    "\n",
    "`Поток данных в Scrapy` контролируется движком и выглядит следующим образом:\n",
    "1. Движок открывает домен, находит паука, который обрабатывает этот домен, и запрашивает упаука первые URL-адреса для сканирования.\n",
    "2. Движок получает от паука первые URL-адреса для обхода и планирует их в планировщике как запросы.\n",
    "3. Движок запрашивает у планировщика следующие URL-адреса для сканирования.\n",
    "4. Планировщик возвращает следующие URL-адреса для сканирования в движок, а движок отправляет их в загрузчик, проходя через downloader middleware.\n",
    "5. Как только страница завершает загрузку, загрузчик генерирует ответ (с этой страницей) и отправляет его в движок, проходя через downloader middleware.\n",
    "6. Движок получает ответ от загрузчика и отправляет его на обработку пауку, проходя через spider middleware.\n",
    "7. Паук обрабатывает ответ и возвращает очищенные элементы и новые запросы (для последующих) в движок.\n",
    "8. Движок отправляет очищенные элементы (возвращённые пауком) в конвейер элементов изапросы (возвращённые пауком) в планировщик.\n",
    "9. Процесс повторяется (начиная с шага 2) до тех пор, пока не перестанут поступать запросы отпланировщика, и движок не закроет домен."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459e5696",
   "metadata": {},
   "source": [
    "### Установка\n",
    "Пакет Scrapy можно найти в PyPI (Python Package Index, также известен как pip) — поддерживаемом\n",
    "сообществом репозитории для всех вышедших пакетов Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad89428",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bfeff5",
   "metadata": {},
   "source": [
    "После установки создадим проект scrapy. Для этого введем `scrapy startproject` и название проекта\n",
    "`books_scrape` — будем парсить книги с уже знакомого нам сайта. У нас создалась папка books_scrape\n",
    "с вот такой структурой:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663e43a",
   "metadata": {},
   "source": [
    "### Создание поискового робота\n",
    "Затем нам надо создать паука, который будет парсить сайт. Для этого внутри созданной папки\n",
    "books_scrape (корневой для следующей папки books_scrape и файла scrapy.cfg) мы должны ввести\n",
    "команду “scrapy genspider”, потом имя паука, пусть будет “books”, а затем — указать ссылку на сайт,\n",
    "который мы хотим спарсить. Обратите внимание, что ссылка должна быть без https и последней слеш\n",
    "в конце, например, books.toscrape.com.\n",
    "\n",
    "В итоге ввод будет выглядеть так: “scrapy genspider books books.toscrape.com”.\n",
    "\n",
    "Внутри папки books, а затем spiders, у нас появился файл books.py со следующей структурой:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f327f3",
   "metadata": {},
   "source": [
    "Где name — это имя паука, которое мы будем использовать для его запуска. В allowed_domains\n",
    "лежит домен, который паук сможет парсить. Если вы сделаете ссылку toscrape.com, то паук books\n",
    "сможет парсить и сайт quotes.toscrape.com. Запомните, что в allowed domains никогда нельзя\n",
    "оставлять обозначение протоколов http и https. В start_urls лежит ссылка, с которой будет начат\n",
    "парсинг. Так как у нас сайт с протоколом https, добавим “s\" к этой ссылке. В методе parse() вернётся\n",
    "response — то, что будет спарсено из start_urls.\n",
    "\n",
    "Давайте спарсим первую страницу уже известного нам сайта и запустим паука. Для начала получим\n",
    "название, обложку, цену и наличие каждой книги на первой странице. В итоге у нас должен\n",
    "получиться список из двадцати объектов.\n",
    "\n",
    "Создадим переменную books и получим из response все книги сразу. Сначала найдём, где у нас лежат\n",
    "книги. Каждая книга лежит в теге `<li>`, а все эти теги вложены в тег `<ol>` с классом row. Давайте\n",
    "пропишем это в нашем пауке. Для извлечения будем использовать метод xpath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbd932",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = response.xpath(«//ol[@class='row']/li\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d40aa",
   "metadata": {},
   "source": [
    "То есть получаем все теги `<li>`, вложенные в тег `<ol>` с классом row. Теперь у нас есть список книг, по\n",
    "которому мы должны пройти, чтобы извлечь информацию о каждой книге.\n",
    "\n",
    "Создадим цикл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904da70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in books:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a8bb6",
   "metadata": {},
   "source": [
    "И сделаем сразу вывод данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    yield {\n",
    "    'image': book.xpath(),\n",
    "    'title': book.xpath(),\n",
    "    'price': book.xpath(),\n",
    "    'instock': book.xpath()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a4c02",
   "metadata": {},
   "source": [
    "Смотрите, мы обращаемся к каждой книге, то есть проваливаемся в html-разметку конкретно одной\n",
    "книги в цикле for. Теперь запишем пути для каждой переменной. Обратите внимание, что начинать\n",
    "путь надо с точки, таким образом мы говорим scrapy, что нас интересует конкретно этот элемент\n",
    "разметки и его дочерние элементы. Если точку не поставить, Scrapy будет искать xpath по всему\n",
    "сайту и, в данном случае, выведет название, цену, обложку и наличие одной и той же книги 20 раз.\n",
    "\n",
    "Поищем, где у нас лежат все нужные нам переменные. Ссылка на изображение у нас лежит в теге\n",
    "`<div>` с классом image_container.Внутри — тег `<а>`, а внутри него — `<img>`. Из этого тега `<img>` нам\n",
    "надо достать атрибут src. Запишем xpath. Чтобы получить содержимое атрибута после метода xpath,\n",
    "надо использовать метод `get()`. Есть ещё метод `getall()`, к нему мы вернемся чуть позже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'image': book.xpath(«.//div[@class='image_container']/a/img/@src\").get(),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac3150",
   "metadata": {},
   "source": [
    "Как мы помним и видим сейчас на сайте, ссылка на картинку у нас не полная. В предыдущих уроках\n",
    "мы добавляли корневую ссылку руками. В Scrapy есть специальная функция для этого. Чтобы к\n",
    "извлеченной ссылке добавить books.toscrape.com, достаточно способ извлечения обернуть в\n",
    "response.urljoin(), который добавит ссылку из allowed_domains к тому, что мы извлекли:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'image': response.urljoin(book.xpath(«.//div[@class=‘image_container’]/a/img/@src»).get()),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c4d73",
   "metadata": {},
   "source": [
    "Далее нам надо получить название. Оно лежит в теге a, который находится внутри тега h3. Из тега а\n",
    "нам надо получить атрибут title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'title': book.xpath(«.//h3/a/@title\").get(),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb85bc8c",
   "metadata": {},
   "source": [
    "Теперь получаем цену. Она лежит в теге p с классом price_color и нам надо получить текст этго тега. К\n",
    "тексту мы обращаемся с использованием круглых скобок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'price': book.xpath(«.//p[@class='price_color']/text()\").get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b752832",
   "metadata": {},
   "source": [
    "Последнее, что нам надо получить — это информацию о наличии в магазине. Она лежит в теге p с\n",
    "классом instosk и классом available. И нам надо получить текст этого тега. Давайте запишем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d615e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'instock': book.xpath(«.//p[contains(@class, ‘instock’)]/text()»).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc63dc8",
   "metadata": {},
   "source": [
    "Теперь у нас написано всё, чтобы получить список всех 20 книг с первой страницы. Давайте запустим\n",
    "паука. Для этого нам надо, находясь в той же директории, что и файл scrapy.cfg написать команду для\n",
    "запуска парсера scrapy crawl и имя паука, в данном случае — books.\n",
    "\n",
    "Мы видим, что парсер запустился. Вы даже можете видеть, как проскочили какие-то данные. Scrapy\n",
    "хорош тем, что в него уже встроена асинхронность, так что парсинг занимает существенно меньше времени, чем с помощью того же requests. После завершения работы паука мы видим вывод, в\n",
    "котором содержится много разной информации. Тут есть и время завершения работы, и время\n",
    "начала. Основное, что нас интересует — это данные item_scraped_count. Как видите, тут стоит\n",
    "цифра 20, то есть наш парсер собрал 20 элементов.\n",
    "\n",
    "Теперь давайте поднимемся выше и посмотрим, что же у нас собралось. Мы видим словари, везде\n",
    "вроде бы всё правильно, кроме значения instock. Тут у нас символ новой строки вместо текста “in\n",
    "stock”. Вернёмся на сайт и посмотрим, в чём же может быть дело. Как видите, текст состоит не только\n",
    "из букв, но и из пустых строчек. Чтобы собрать всё это, надо использовать метод getall() вместо get().\n",
    "В таком случае нам будет возвращён словарь. Мы сможем этот словарь объединить и удалить\n",
    "переносы строк. Давайте исправим наш код. Сделаем это в отдельной переменной, чтобы\n",
    "возвращать уже чистую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "instock = ''.join(book.xpath(\".//p[contains(@class,'instock')]/text()\").getall()).strip()\n",
    "'instock': instock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd550e91",
   "metadata": {},
   "source": [
    "Снова запускаем паука. Смотрим — собрано 20 элементов, и теперь в ключе instock находится\n",
    "верное значение. Так, отлично. Теперь сохраним данные в файл. Для этого при запуске паука\n",
    "достаточно указать опцию -o и имя файла, в который мы хотим сохранить данные. Давайте сначала\n",
    "сохраним наши данные в csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl books -o books.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3277f899",
   "metadata": {},
   "source": [
    "Проверяем — всё правильно. Теперь сохраним данные в json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl books -o books.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6693e75",
   "metadata": {},
   "source": [
    "Смотрим файл. Видите, у нас тут вместо знака доллара стоит неправильный символ юникода. Чтобы\n",
    "избежать таких ошибок, идём в файл settings.py и добавляем там поддержку юникода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEED_EXPORT_ENCODING = ‘UTF-8’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc3d09",
   "metadata": {},
   "source": [
    "Удалим json и запустим паука ещё раз. Проверяем. Теперь знак доллара указан верно. У нас в файле\n",
    "20 книг с первой страницы сайта. Отлично.\n",
    "\n",
    "Давайте я вам расскажу ещё немного про файл settings.py. Во-первых, тут можно сделать так, чтобы\n",
    "scrapy не следовал указаниям файла robots.txt сайта. Мы об этом файле говорили на прошлых\n",
    "лекциях. Так как мы с вами парсим сайт, у которого нет этого файла, то `ROBOTSTXT_OBEY = true`\n",
    "в настройках нам никак не мешает. Но чаще всего вы будете сталкиваться с тем, что эта настройка\n",
    "будет вам мешать: Scrapy не будет парсить страницы, которые не разрешено парсить в файле\n",
    "`robots.txt`. Чтобы заставить Scrapy парсить то, что нужно вам, измените значение на False.\n",
    "\n",
    "Ещё есть два параметра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a406dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENT_REQUESTS = 32\n",
    "DOWNLOAD_DELAY = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4996f",
   "metadata": {},
   "source": [
    "Первый позволяет ограничить число одновременных запросов к сайту — это к вопросу об этичном\n",
    "парсинге. Второй делает паузу между запросами на столько секунд, на сколько вы укажете. Также в\n",
    "настройках можно менять user agent, включать и отключать куки и многое другое. Подробнее об\n",
    "остальных параметрах и их значениях вы можете почитать, конечно же, в документации.\n",
    "\n",
    "Теперь давайте поговорим о том, как спарсить все книги со всех страниц. Давайте внутри уже\n",
    "существующего паука создадим переменную next_page. В неё положим ссылку на следующую\n",
    "страницу. Она лежит внутри тега а с текстом “next”, внутри тега `<li>` c классом next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = response.xpath(\"//li[@class='next']/a[contains(text(),'next')]/@href\").get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89524909",
   "metadata": {},
   "source": [
    "Если следующая страница существует, будем переходить по ней, используя метод Request, и\n",
    "вызывать метод parse, который мы с вами только что создали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d952b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if next_page:\n",
    "    next_page_link = response.urljoin(next_page)\n",
    "    yield scrapy.Request(url=next_page_link, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5ca49",
   "metadata": {},
   "source": [
    "Запустим парсер. Надо немного подождать. Как видите, у нас собрана 1000 книг. На каждой странице\n",
    "по 0 книг, всего 50 страниц — всё правильно.\n",
    "\n",
    "Ещё у нас на сайте есть возможность провалиться внутрь каждой книги, если перейти по ссылке,\n",
    "которая лежит в названии. Давайте посмотрим, что там внутри. Как видите, тут есть не только\n",
    "название, но и много других разных данных о книге. Очень часто бывает, что надо спарсить всю\n",
    "информацию о товаре, а значит, переходить по ссылке на каждый товар. И никто не отменял того, что\n",
    "надо собрать всю тысячу товаров, а не первые 20 В таком случае нам на помощь придёт\n",
    "специальный шаблон Scrapy.\n",
    "\n",
    "Давайте создадим нового паука и теперь в опции укажем -t (template) и название шаблона — crawl, —\n",
    "а затем имя паука и снова ссылку на главную страницу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c428a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy genspider -t crawl pages books.toscrape.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca655e0",
   "metadata": {},
   "source": [
    "У нас в папке spiders появился ещё один паук. Открываем. Как видите, у него немного другая\n",
    "структура. Тут появилась переменная rules, в которой у нас содержатся разные правила. Мы будем\n",
    "использовать эти правила, чтобы переходить на следующие страницы и проваливаться внутрь\n",
    "каждой книги.\n",
    "\n",
    "Для начала напишем правило перехода на следующую страницу. Для этого внутри объекта Rules\n",
    "запишем xpath для кнопки на следующую страницу. Мы можем её скопировать из предыдущего кода.\n",
    "Меняем `allow` на `restrict_xpaths`, внутри прописываем путь до ссылки `\"//li[@class=‘next']/a\"` и на этом\n",
    "заканчиваем. Судя по этому правилу, парсер будет находить ссылку на следующую страницу,\n",
    "извлекать её самостоятельно — нам не надо извлекать атрибут href и использовать метод get() — и\n",
    "переходить по этой ссылке до тех пор, пока ссылка на странице существует.\n",
    "\n",
    "Теперь напишем правило, по которому мы будем искать на странице ссылку на каждую книгу и\n",
    "переходить по этой ссылке. Создаём новый объект Rule и указываем, где ссылки лежат. Они лежат в\n",
    "теге `<article>` с классом productt_pod, затем тег `<h3>` и тег `<a>`. После извлечения ссылки нам надо\n",
    "парсить страницу книги. Делать это мы будем в уже созданной функции parse_item, так что пишем\n",
    "callback=parse_item, и указываем, что follow=True, то есть парсер должен пройти по этой ссылке и\n",
    "извлечь из неё html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc214bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rule(LinkExtractor(restrict_xpaths=\"//article[@class='product_pod']/h3/a\"),callback='parse_item', follow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c71324",
   "metadata": {},
   "source": [
    "Теперь давайте в функции parse_item будем доставать название книги. Итак, переходим на страницу\n",
    "любой книги. Смотрим, где у нас лежит название. Оно лежит внутри тега `<div>` с классами col-sm-6\n",
    "product_main — в теге `<h1>`. Запишем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a067add",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = response.xpath(«//div[contains(@class,‘product_main’)]/h1/text()»).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e24ef",
   "metadata": {},
   "source": [
    "Воспользуемся уже созданным словарём, запишем туда: `item[‘title’] = title` и сделаем yield item.\n",
    "Теперь всё готово для запуска парсера. Потребуется какое-то время, чтобы парсер обработал все\n",
    "страницы и каждую книгу. В итоге мы видим, что item_scraped_count равняется тысяче: именно\n",
    "столько книг у нас на сайте. Пробежимся по извлечённым книгам — везде есть название. Парсер всё\n",
    "правильно собрал."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48632b1e",
   "metadata": {},
   "source": [
    "### Домашнее задание\n",
    "Доработать паука в имеющемся проекте, чтобы по каждой книге была собрана вся информация с её\n",
    "страницы: Title, Price, In stock, Product Description, UPC, Product Type, Price (excl. tax), Price (incl. tax),\n",
    "Tax, Availability, Number of reviews.\n",
    "\n",
    "### Используемая литература\n",
    "1. Документация Scrapy .\n",
    "2. Работа с файлами и фото на Scrapy .\n",
    "3. Web scraping с помощью Scrapy и Python ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
