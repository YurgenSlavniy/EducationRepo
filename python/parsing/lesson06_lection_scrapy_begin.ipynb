{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12968f38",
   "metadata": {},
   "source": [
    "# Урок 6. Фреймворк Scrapy. Знакомство\n",
    "\n",
    "Узнаем как строятся приложения с использованием данного фреймворка. \n",
    "\n",
    "`Scrapy` - платформа на базе которой можно использовать изученные ранее инструменты: BeautifulSoup, Selenium, Xpath, CSS селекторы. Используе ещё и дополнительные возможности которых очень много здесь реализовано можно создавать очень мощные приложения, которые работают намного быстрее. \n",
    "\n",
    "Сразу начинаем с практики. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2131b5c",
   "metadata": {},
   "source": [
    "Всё начинается с создания проекта. Используем PyCharm. У фреймворка скрапи за директорию в которой он начинает работать берётся директория проекта и директория виртуального окружения. Рекомендуется создавать отдельное виртуальное окружение для текущего проекта. Создаётся отдельный новый проект под текущие задачи с фреймворком скрапи. В момент создания проекта мы используем текущее локальное виртуальное окружение, которое будет скопировано из глобального.  Создаётся новый проект (в пайчарм) в котором нам доступно виртуальное окружение, в котром есть наш интерпритатор и базовый набор библиотек, необходимый для дальнейшей работы. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66739bea",
   "metadata": {},
   "source": [
    "Следующий этап - установка самого Scrapy. \n",
    "- Кликаем на папку venv (ЛП Левая Панель). \n",
    "- Переходим в терминал. ((venv) D:\\ИИ УЧ~БА\\Scrapy_project>)\n",
    "- Вводим команду: pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f4730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb49fc51",
   "metadata": {},
   "source": [
    "Подтягивается огромное количество библиотек. У фреймворка очень много функциональных возможностей. \n",
    "\n",
    "Далее нам надо создать проект с использованием данного фреймворка так чтобы для нас он сформировал уже некоторую готовую структуру. Приложение пишется не с нуля, а придерживаясь определённых правил. Структуру создаваемого проекта можно сравнить с flask или django. \n",
    "\n",
    "В том же самом терминале вводим команду:\n",
    "> scrapy startproject project_name project_dir\n",
    "\n",
    "Мы обращаемся к некому нашему модулю `scrapy`, который стал у нас доступен после установки в нашем виртуальном окружении.  У этого модуля условно говоря вызываем метод `startproject` и дальше внутрь работы данного метода передаём 2 параметра: имя проекта `project_name`, и директория которая будет являться корнем нашего проекта `project_dir`. По умолчанию мы находимся в директории `(venv) D:\\ИИ УЧ~БА\\Scrapy_project`. Можно вместо второго параметра поставить точку, что будет означать выбор директории в которой мы сейчас находимся. Точка указатель на текущий каталог при создании относительного пути. Корневой директорией будет являться `D:\\ИИ УЧ~БА\\Scrapy_project`.\n",
    "\n",
    "> scrapy startproject project_name .\n",
    "\n",
    "Если не указать второй параметр скрапи по умолчанию раскидает файлы проекта и может быть проблемно собрать потом это всё во едино. \n",
    "\n",
    "> (venv) D:\\ИИ УЧ~БА\\Scrapy_project>scrapy startproject project_1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae79861",
   "metadata": {},
   "source": [
    "Появилась директория `project_1` с определённой структурой файлов внутри. Но прежде чем переходить к структуре и написанию кода, нам необходимо сделать ещё один обязательный шаг - создать так называемого паука. \n",
    "\n",
    "Паук это по факту пайтоновский файл, в котором реализован класс. В этом классе будет создаваться логика парсинга. На вход будет подаватьсе респонс при выполнении гет запроса на какую то ссылку, на выходе будем получать структурированные данные. Мы должны будем полученный респонс обработать, пропустить через какие нибудь обработчики, пропустить через определённые селекторы, сформировать структуру и на выход её нам вернуть. Весь этот файл, отвечающий за этот функционал. ВНИМАНИЕ! здесь не включена обработка ДАННЫХ, она должна быть в другом месте. ТОЛЬКО ПАРСИНГ исключительно. \n",
    "\n",
    "> (venv) D:\\ИИ УЧ~БА\\Scrapy_project>scrapy genspider spider_name resorse_name.com\n",
    "\n",
    "обычно название паука `spider_name` по ресурсу с которого происходит парсинг. И второй параметр разрешённый домен в рамках которого будет работать наш паук. Так как мы в примере будем работать с хедхантером, то домен будет `hh.ru`. Без https и слэшей //. Мы указываем домен первого уровня `hh` и домен второго уровня `.ru`. \n",
    "\n",
    "> (venv) D:\\ИИ УЧ~БА\\Scrapy_project>scrapy genspider hhru hh.ru\n",
    "\n",
    "Создаётся наш паук и находится он в созданной директории `spiders`. Файл `hhru.py`. \n",
    "\n",
    "Будем учиться собирать данные с использованием фреймворка `scrapy` для примера используем `hh.ru`, смотреть на его функциональные возможности и особенности реализации, на то как он работает и ка кнадо подходить к парсингу с учётом логики заложенной внутри данного фреймворка.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d6d73",
   "metadata": {},
   "source": [
    "### Структура проекта\n",
    "\n",
    "- dir spiders\n",
    "- __init__.py\n",
    "- items.py\n",
    "- middlewarers.py\n",
    "- pipelines.py\n",
    "- settings.py\n",
    "\n",
    "Посмотрим на файл `settings.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapy settings for project_1 project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = \"project_1\"\n",
    "\n",
    "SPIDER_MODULES = [\"project_1.spiders\"]\n",
    "NEWSPIDER_MODULE = \"project_1.spiders\"\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = \"project_1 (+http://www.yourdomain.com)\"\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "#    \"Accept-Language\": \"en\",\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    \"project_1.middlewares.Project1SpiderMiddleware\": 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    \"project_1.middlewares.Project1DownloaderMiddleware\": 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    \"scrapy.extensions.telnet.TelnetConsole\": None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    \"project_1.pipelines.Project1Pipeline\": 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = \"httpcache\"\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n",
    "\n",
    "# Set settings whose default value is deprecated to a future-proof value\n",
    "REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\n",
    "TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n",
    "FEED_EXPORT_ENCODING = \"utf-8\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a44f65",
   "metadata": {},
   "source": [
    "Настройки `settings.py` представляют собой набор пар ключ - значение. В комментах этого файла есть ссылка на документацию https://docs.scrapy.org/en/latest/topics/settings.html. Рассмотрим только основные вещи, которые нужны в повседневной работе. \n",
    "\n",
    "`BOT_NAME = \"project_1\"` - то имя которое мы присвоили при создании проекта скрапи. \n",
    "\n",
    "`SPIDER_MODULES = [\"project_1.spiders\"]` - список директорий, в которых хронятся наши пауки. Директории указываются относительно корня который мы указали в момент создания нашего проекта. То есть этот путь берётся относительно ` D:\\ИИ УЧ~БА\\Scrapy_project`. Если у нас будет сложный проект и несколько директорий с пауками, будем добавлять директории в этот список. Те директории где хронятся наши пауки. \n",
    "\n",
    "`NEWSPIDER_MODULE = \"project_1.spiders\"` - директория в которой появляются новые пауки после создания через `scrapy genspider`. \n",
    "\n",
    "`USER_AGENT = \"project_1 (+http://www.yourdomain.com)\"` - заполняем из браузера один раз и все наши запросы будут использоваться с использованием данного USER_AGENT. chrome://version -> USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"\n",
    "\n",
    "\n",
    "`ROBOTSTXT_OBEY = True` - файл нужный для краулеров. Поисковые системы приходя на сайт сканят этот файл, в котором хронится информация о том, как парсить сайт. О том какие разделы, какие директории в нём есть, о чём он. Скрапи умеет тоже это использовать для того чтобы собирать информацию по заранее определённому алгоритму. Данный параметр является приоритетным для сбора данных и если он на сайте недоступен, это приведёт к остановке работы. Поэтому рекомендуется его отключать. ROBOTSTXT_OBEY = False\n",
    "\n",
    "`CONCURRENT_REQUESTS = 32` - количество запросов параллельно выполняемых в единицу времени. Параллельных и асинхронных. Следует аккуратно быть с этим параметром иначе можно задолбить сайт запросами. Нужно быть уверенным что сайт приемлет такую нагрузку на себя и не забанит. Запросы от одного клиента. Аккуратный параметр = 8. \n",
    "\n",
    "`DOWNLOAD_DELAY = 3` - задержка в секундах между вышестоящей пачкой запросов CONCURRENT_REQUESTS. Рекомендации с которых стоит начинать CONCURRENT_REQUESTS = 8, 8 запросов через 1.5 секунды DOWNLOAD_DELAY = 1.5 и смотрим как будет справляться сайт с этой нагрузкой.  \n",
    "\n",
    "`COOKIES_ENABLED = False` - Если выставляем True не заморачиваемся с их управлением и ведём себя практически также как и браузер - при первом запросе их получаем с сервера, собираем себе, а дальше ходим по всем запросам. Сервер видит в нас знакомое лицо, потому что перед входом мы уже машем куками и сервер гораздо более охотно с нами взаимодействует. \n",
    "\n",
    "`LOG_ENABLED = True` - включаем логи \n",
    "\n",
    "`LOG_LEVEL = 'DEBUG'` - включаем уровни логов. Стандартные 5 уровней логов WARN ERROR INFO CRITICAL DEBUG. Самый полномасштабный - DEBUG. После полной отладки можно включать уровень логов INFO чтобы логировались только важные события. Сейчас будем смотреть с DEBUG и читать все события. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd80701",
   "metadata": {},
   "source": [
    "## Настройка запуска приложения\n",
    "### Директория spiders\n",
    "\n",
    "В этой директории находятся файлы паука. В нашем случае `hhru.py` и `__init__.py`.  Посмотрим на файл `hhru.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhru.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class HhruSpider(scrapy.Spider):\n",
    "    name = \"hhru\"\n",
    "    allowed_domains = [\"hh.ru\"]\n",
    "    start_urls = [\"http://hh.ru/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb733a5",
   "metadata": {},
   "source": [
    "Внутри находится класс `HhruSpider`, который является наследником некого базового класса `Spider`. Скрапи 100% объектно ориентированный. Здесь нет функционального программирования и отдельно взятых функций, нет объектов болтающихся неизвестно где. Всё построенно на классах и на их объектов. \n",
    "\n",
    "Что есть в нашем классе. В этом классе есть три атрибута: name, allowed_domains, start_urls. \n",
    "\n",
    "`name = \"hhru\"` - наименование паука, строковое свойство, ни на что не влияет. параметр который будет отличать нашего паука от всех других пауков.\n",
    "\n",
    "`allowed_domains = [\"hh.ru\"]` - значение берётся из параметров, когда мы указывали scrapy genspider hhru hh.ru. Это очень значимое - паук будет работать с сылками только в рамках этого домена \"hh.ru\". Если попытаемся сделать ссылку ведущую на внешний ресурс, получим предупреждение. В логах оно также появится. Скрапи туда не пойдёт. \n",
    "\n",
    "`start_urls = [\"http://hh.ru/\"]` - стартовые точки входа. Это список. \n",
    "\n",
    "### Как всё устроено, как всё работает.\n",
    "У нас есть класс, по логике мы должны создать объект этого класса и вызывать методы для работы и функционирования. Но мы этого делать не будем. В скрапи есть огромное количество скрытых реализаций и функций построенных на патернах проектирования. Суть патернов - от конечного пользователя скрывается огромная масса технических сложностей. \n",
    "\n",
    "Когда мы запустим работу нашего проекта, у нас автоматически создастся экземпляр данного класса, произойдёт некая логика инициализации она достаточно глубокая и технически сложная, и только после этого будет выполнен самостоятельно гет запрос по ссылке `start_urls = [\"http://hh.ru/\"]`, будет получен ответ response и этот response упадёт у нас в метод `parse(self, response)`\n",
    "\n",
    "Наша дача вместо заглушки pass написать логику работы с response при выполнении на первую точку входа.  \n",
    "\n",
    "Стартовая ссылка \"http://hh.ru/\". Если руководствоваться логикой по поиску вакансий, которую мы реализовывали ранее. Идём на сайт \"http://hh.ru/\", вводим нашу профессию которую будем искать, выставляем необходимые нам настройки, копируем ссылку - это и будет точкой входа. https://hh.ru/search/vacancy?text=Python+%D1%81%D1%82%D0%B0%D0%B6%D0%B5%D1%80&from=suggest_post&area=1\n",
    "\n",
    "Вставляем эту ссвылку в `start_urls`. Первый гет запрос будет выполнен именно по этой ссылке. И в параметре response мы получим ответ как раз с этой ссылки. Этот респонс будет похож на респонс xpath. Он будет содержать и DOM по которому будем проходиться css селекторами или xpath селекторами без дополнительных преобразований.\n",
    "\n",
    "Выполняется гет запрос, респонс получаем здесь `def parse(self, response)` его ловим и пишем логику определённую. \n",
    "\n",
    "Теперь по поводу списка в `start_urls`. мы можем задать сразу несколько точек входа одновременно. При нашем запросе - python стажёр нам выдаётся 103 вакансии. Сайт даёт 2000  вакансий максимум, он так настроен. Больше не собрать. А иногда при поиске может быть найдено больше 2000 вакансий. Даже если собираем по API тоже не более 2000 вакансий. мы можем настроить фильтры разным способом. С помощью фильтров настраиваем поиск так. чтобы выдавал до 2000 вакансий, а затем в список вставляем разные точки входа с разными фильтрами и собираем все вакансии с разных точек входа.\n",
    "\n",
    "Ещё один жирный + скрапи - его многопоточность. Хоть потоки и скрыты от нас. И когда у нас список точек входа. Скрапи в момент инициализации данного объекта начнёт последоватьельно проходиться по ссылке и для каждой точки входа создавать отдельный поток. Для первой точки входа скрапи выпонит гетзапрос и респонс закинет в метод parse(self, response) который запущен будет в отдельном потоке. И так как точка входа подразумевает под собой дальнейшую объёмную обработку: переключаться по страницам, заходить внутрь каждого объявления, собирать данные по каждой вакансии - поток будет существовать очень долго. \n",
    "\n",
    "Но у нас и вторая точка входа. зачем ждать пока заверщится первый поток, второй вместе с первым начинает работу. В этом потоке вызовим тотже самый метод parse(self, response) в котором уже обрабатываем вторую точку входа. Потоки работают параллельно. \n",
    "\n",
    "Отсюда и вытекает такая ситуация, что в единицу времени выполняется огромное количество запросов по разным потокам. А мы сделаем такую логику, что не только по страницам будем переключаться, а ещё будем заходить внутрь каждой вакансии и данные собирать изнутри. \n",
    "\n",
    "Модуль использует ресурсы нашей машины. поэтому надо быть осторожным с количеством потоков. Зависит от нашего процессора и количества ядер. Это без использования технологии гиперфрезинг, когда создаются виртуальные потоки, когда в рамках одного ядра можно распараллелить вычисления. Физически реальных потоков не больше чем ядер в процессоре. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50201a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhru.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class HhruSpider(scrapy.Spider):\n",
    "    name = \"hhru\"\n",
    "    allowed_domains = [\"hh.ru\"]\n",
    "    start_urls = [\"https://hh.ru/search/vacancy?text=Python+%D1%81%D1%82%D0%B0%D0%B6%D0%B5%D1%80&from=suggest_post&area=1\", \n",
    "                 \"https://hh.ru/search/vacancy?text=datasince&area=1\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        print(response.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7846cfa",
   "metadata": {},
   "source": [
    "Просто распечатаем ссылку, чтобы убедиться что по каждой из них мы прошли. \n",
    "\n",
    "Проект запускается через терминал, через командную строчку\n",
    "> (venv) D:\\ИИ УЧ~БА\\Scrapy_project>scrapy crawl hhru\n",
    "\n",
    "обращаемся к `scrapy` вызываем метод `crawl` и название паука `hhru`. Мы видим все логи:\n",
    "```\n",
    "2023-03-10 14:11:28 [scrapy.core.engine] INFO: Spider opened\n",
    "2023-03-10 14:11:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2023-03-10 14:11:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "```\n",
    "\n",
    "Все наши настройки преобразовались в объект типа словарь, мы видим дополнительные подключаемые модули, всё это дело фиксируется, логируется, и вот пошла работа нашего паука:\n",
    "```\n",
    "2023-03-10 14:11:30 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): publicsuffix.org:443\n",
    "2023-03-10 14:11:31 [urllib3.connectionpool] DEBUG: https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 81590\n",
    "2023-03-10 14:11:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hh.ru/search/vacancy?text=Python+%D1%81%D1%82%D0%B0%D0%B6%D0%B5%D1%80&from=suggest_post&ar\n",
    "ea=1> (referer: None)\n",
    "https://hh.ru/search/vacancy?text=Python+%D1%81%D1%82%D0%B0%D0%B6%D0%B5%D1%80&from=suggest_post&area=1\n",
    "2023-03-10 14:11:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://hh.ru/search/vacancy?text=datasince&area=1> (referer: None)\n",
    "https://hh.ru/search/vacancy?text=datasince&area=1\n",
    "2023-03-10 14:11:32 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "```\n",
    " У нас выполнилось 2 гет запроса со статускодом 200\n",
    "` Crawled (200) <GET https://hh.ru...` И наш вывод - ссылки. Среднее время ответа от сервера 300-400 милисекунд. Это стандартно считается хороший отклик и хороший пинг. \n",
    "\n",
    "Система логов показывает работу самого фреймворка и ошибки надо будет искать иначе. Логические ошибки на уровне пайтона показаны не будут. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c181335",
   "metadata": {},
   "source": [
    "Напишем точку входа самостоятельно. Внутри проекта создаём ещё один файл самостоятельно `runner.py`. В нём подключаем модули, необходимые нам для инициализации проекта. Модули которые отвечают за различный функционал. Импортируем стандартные модули скрапи. Собираем их воедино для создания самостоятельной точки входа. \n",
    "\n",
    "`from twisted.internet import reactor` - что то типа ядра. Именно в нём находится кнопка пуск \n",
    "\n",
    "`from scrapy.crawler import CrawlerRunner` - нужен для создания обвязки, которая будет запускаться в работу. Она будет в себя настройки, пауки, которые будут работать под эгидой этих самых натсроек. На неё будет воздействовать реактор, впускать её в работу.\n",
    "\n",
    "`from scrapy.utils.project import get_project_settings` - Нам нужна функция которая сможет прочитать файл с настройками и конвертировать его в словарь. (То что происходит в логах при запуске) Здесь будет происходить вся инициализация. \n",
    "\n",
    "`from scrapy.utils.log import configure_logging` - подключаем логгер. \n",
    "\n",
    "Части скрапи которые собираются во едино и инициализируют работу. \n",
    "\n",
    "Дальше внесённая в проект наша логика, нам надо подключить классы наших пауков для того чтобы их можно вписать во внутреннюю логику. \n",
    "При подключении указываем путь относительно нашего проекта. Корнем является `project_1`. from jobparser выбираем директорию spiders и выбираем файл hhru и оттуда импортируем наш класс HhruSpider.\n",
    "`from jobparser.spiders.hhru import HhruSpider` \n",
    "\n",
    "После всех импортов собираем воедино всё. `if __name__ == '__main__':`. Сначала включаем логгирование вызыванием функции `configure_logging()`, после включения системы логгирования после того как мы следим за всеми процессами  нам необходимо прочитать наш файл с настройками и сформировать из этого отдельный объект, который нам вернёт функция: `settings = get_project_settings()` .  Следующим этапом мы создаём объект ранера, который будет экземпляром класса ранер: `runner = CrawlerRunner(settings)` Внутрь CrawlerRunner мы как раз и помещаем наш объект с настройками ему в конструктор. Изначально ранер пустой, как капсула и мы закидываем туда начинку. \n",
    "Когда вся эта информация поместилась в объект ранер, мы должны его наполнить исполнителем. Основным исполнителем - наш паук. `runner.crawl(HhruSpider)`. метод crawl внутрь которого запихиваем наш класс паука. \n",
    "\n",
    "Код для объединения (join) нескольких пауков: `d = runner.join()`. Создаём отдельный объект d. \n",
    "\n",
    "запускаем проект `reactor.run()` через reactor запускаем метод run(). \n",
    "\n",
    "Теперь поставив брекпоинт в файле возле `print(response.url)` и запустив через runner.py в режиме отладки можно отлаживать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40794856",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twisted'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# runner.py \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtwisted\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reactor\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrawler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrawlerRunner\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproject\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_project_settings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'twisted'"
     ]
    }
   ],
   "source": [
    "# runner.py \n",
    "\n",
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "from jobparser.spiders.hhru import HhruSpider\n",
    "# from jobparser.spiders.sjru import SJSpider\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    configure_logging()\n",
    "    settings = get_project_settings()\n",
    "    runner = CrawlerRunner(settings)\n",
    "\n",
    "    runner.crawl(HhruSpider)\n",
    "    # runner.crawl(SJSpider)\n",
    "    #\n",
    "    # d = runner.join()\n",
    "    # d.addBoth(lambda _: reactor.stop())\n",
    "\n",
    "    reactor.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dbd746",
   "metadata": {},
   "source": [
    "Мы получили response и теперь наша задача написать логику для его обработки.  Пайтон не строго типизированный язык и мы не должны объявлять тип данных при объявлении переменной. Но мы можем сделать некоторую подсказку для работы с объектами, которые используют много методов. Например словарь. Но мы можем сделать подсказку объявляя переменную. \n",
    "\n",
    "`x: dict`\n",
    "\n",
    "И теперь выбрав переменную и поставив точку IDE подскажет возможные методы и свойства работы со словарём. Так прокатывает с объектами разных классов. Особенно пользуясь разными библиотеками, когда мы знаем к какому классу относится тот или иной объект можно указать данный тип для данного текущего объекта и дальше пользоваться удобно подсказками IDE всплывающего списка. \n",
    "Подключим в наш проект класс, чтобы удобно пользоваться подсказками. Для файла hhru.py\n",
    "\n",
    "`from scrapy.http import HtmlResponse`\n",
    "\n",
    "И в функции укажем что response является объектом класса HtmlResponse\n",
    "\n",
    "`def parse(self, response:HtmlResponse):`\n",
    "\n",
    "Теперь написав `response.` мы видим все методы и свойства которые этот объект поддерживает. \n",
    "\n",
    "Когда у нас на руках готовый DOM мы можем использовать готовый метод Xpath `response.xpath()` где в скобках указываем нужный нам селектор. \n",
    "есть также метод `response.css()`. \n",
    "\n",
    "```\n",
    "def parse(self, response:HtmlResponse):\n",
    "    response.xpath()\n",
    "    response.css()\n",
    "```\n",
    "Теперь нам осталось продумать логику работы нашего приложения. На хед хантере мы будем падать внутрь каждой вакансии и собирать данные оттуда. На каждой странице мы должны будем получить 2 вещи:\n",
    "- сама ссылка на вакансию\n",
    "`link = response.xpath('//a[@data-qa=\"vacancy-serp__vacancy-title\"]/@href')`\n",
    "Но на выходе мы получаем объекты не совсем финальные, мы получаем обёрнутые объекты в дополнительную оболочку с помощью которой и достигается универсальность работы с респонсом.  Чтобы до конца собрать данные и развернуть эти обёртки и получить всю информацию мы к полученному результату применяем метод `getall()`\n",
    "\n",
    "`links = response.xpath('//a[@data-qa=\"serp-item__title\"]/@href').getall()`\n",
    "\n",
    "Данный метод проходится по полученному результату, выдёргивая с каждого объекта те нужные и полезные данные, которые нам необходимы. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96cb1a",
   "metadata": {},
   "source": [
    "В переменной links мы получим список ссылок на вакансии с первой страницы. После того как список ссылок у нас на руках, остаётся его перебрать, выполняя гет запрос на каждую ссылку полученного нами списка. Это делатся стандартным способом. `for link in links:` и дальше начнёися вся суть работы скрапп и вся суть его заложенной логики. На каждой иттерации цикла for у нас будет ссылка link. Мы должны выполнить гет запрос по этой ссылке. Мы обращаемся к нашемо респонсу и вызываем метод `follow(link)` в качестве параметра указываем ссылку куда надо следовать. Метод выполнит запрос по этой ссылке. По умолчанию делается гет зпрос, но это настраивается и указывается через параметр метод. `response.follow(link, method='')`. \n",
    "\n",
    "В чём прелесть скраппи: мало того что он многопоточный, он ещё и асинхронный. Асинхронность для того, чтобы выполнив гет запрос по этой ссылке, мы бы не ждали ответа от сервера (а это около 300 милисекунд),  мы можем поручить ожидание ответа при выполнении гет запроса на эту ссылку другой функции. В концепции асинхронного программирования - callback функции. и здесь есть соответствующий параметр callback `response.follow(link, callback='self.parse_vacancy')` \n",
    "\n",
    "В концепции класса - это у нас будет метод, эту функцию (метод) мы должны создать. Так как функция принимает в себя результат гет запроса, соответственно она должна принимать в себя респонс соответствующего класса. А в функцию выше мы укажем в параметрах то что будем отдавать респонс в функцию ниже `response.follow(link, callback='self.parse_vacancy')`. В параметре мы не вызываем метод, а передаём ссылку на неё.\n",
    "\n",
    "```\n",
    "def parse_vacancy(self, response:HtmlResponse):\n",
    "    pass\n",
    "```\n",
    "метод parse_vacancy будет работать на обработку гетзапроса ссылки link. Мы находясь в методе parse_vacancy уже занимаемся обработкой ответа от страницы с вакансией. Дальше в этом методе будем писать селекторы для получения финальных данных: наименование вакансии, зарплата, описание вакансии, ключевые данные и т.д. потому что мы перешли внутрь вакансии. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    response.follow(link, callback='self.parse_vacancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0467a77",
   "metadata": {},
   "source": [
    "Есть ньюанс. Результат работы методы `parse_vacancy` мы должны возвращать из метода `parse`. Мы должны выйти анружу из `parse` чтобы потом войти в `parse_vacancy`. Поэтому применяется генератор yield. Он нужен для того, чтобы при выходе из метода `parse` когда зайдём на первую итеррацию цикла for и когда выйдем из метода при первой итерации наружу в `parse_vacancy` мы не сломали, что насобирали в `parse` (ссылки на вакансии насобирали). Чтобы сохранилось текущее состояние генератора. И потом при возврате снова в метод `parse` у нас уже запустилась вторая итерация. \n",
    "\n",
    "Метод follow не только возвращает гет запрос по ссылке, и вызывает callback функцию передавая туда респонс при выполнении гет запроса, он ещё параллельно этому реализует вызов метода из которого он был вызван. Его вторая функциональность заключается как раз в этом действии. \n",
    "\n",
    "Это всё что касается перебора ссылок с первой страницы. \n",
    "\n",
    "Так как это генератор, мы вызываем метод, но мы возвращаемся к тому месту, на котором в прошлый раз остановились, а в прошлый раз мы остановились на второй итеррации цикла for. Поэтому дёргается следующая итерация цикла for. `links = response.xpath('//a[@data-qa=\"serp-item__title\"]/@href').getall()` - эта логика заново не отрабатыват. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26344bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    yield response.follow(link, callback='self.parse_vacancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193628b",
   "metadata": {},
   "source": [
    "Эта была только первая страница, а страниц с поиска вакансий около 100. Нам нужно собрать все 100 страниц по 20 вакансий на каждой. снова работаем над методом `parse` который пока что имеет следующий вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03340fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response:HtmlResponse):\n",
    "    links = response.xpath('//a[@data-qa=\"serp-item__title\"]/@href').getall()\n",
    "    for link in links:\n",
    "        yield response.follow(link, callback='self.parse_vacancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0790637",
   "metadata": {},
   "source": [
    "Будем использовать кнопку дальше для перехода на следующую страницу с вакансиями. Из этой кнопки дальше мы будем извлекать соответствующую ссылку. Проведём инспекцию элемента и видим \n",
    "```\n",
    "<a class=\"bloko-button\" rel=\"nofollow\" data-qa=\"pager-next\" href=\"/search/vacancy?text=python&amp;area=1&amp;page=1&amp;hhtmFrom=vacancy_search_list\"><span>дальше</span></a>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = response.xpath('//a[@data-qa=\"pager-next\"]/@href').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbadb3f",
   "metadata": {},
   "source": [
    "Результатом работы xpath будет список, даже если там всего один элемент. Мы получим список из одного объекта. Распоковать этот объект методом get() когда знаем что должен вернуться один объект.\n",
    "\n",
    "У нас на руках ещё одна ссылка. Теперь проверяем что у нас что то собралось.  Проверка `if next page:` И если ссылка есть(то есть есть следующая страница) то `yield response.follow(next_page, callback='self.parse')`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response:HtmlResponse):\n",
    "    next_page = response.xpath('//a[@data-qa=\"pager-next\"]/@href').get()\n",
    "    if next_page:\n",
    "        yield response.follow(next_page, callback='self.parse')\n",
    "        \n",
    "    links = response.xpath('//a[@data-qa=\"serp-item__title\"]/@href').getall()\n",
    "    for link in links:\n",
    "        yield response.follow(link, callback='self.parse_vacancy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080aeb56",
   "metadata": {},
   "source": [
    "У нас есть первый вызов метода parse. На этой страничке мы ищем ссылку next_page. Если эта ссылка находится мы сразу же переходим на следующую страницу. Параллельно этому так как у нас метод `follow` который дёргает сам себя и дальше работа продолжается на том месте на котором остановились `yield response.follow(next_page, callback='self.parse')` и дальше идём и получаем  20 ссылок `links = response.xpath('//a[@data-qa=\"serp-item__title\"]/@href').getall()` и заходим в цикл переберающий ссылки и выбрасовающий в callback функцию по одной ссылке на вакансию.  Параллельно этому по ссылке перехода на следующую страницу во втором потоке. ссылка ещё на 20 вакансий которые многопоточно запускаются `yield response.follow(link, callback='self.parse_vacancy')`.  И вот у нас параллельно работают 2 цикла for каждый из которых создаёт собственные потоки.  В разных потоках будут создаваться экземпляры одного и тогоже метода.  И ак пока не закончатся ссылки на следующую страницу. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd053984",
   "metadata": {},
   "source": [
    "Для сохранения данных отдельный метод будет работать в отдельном потоке создаётся отдельный объект и там формируется структура. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7018d2",
   "metadata": {},
   "source": [
    "Вернёмся к методу `def parse_vacancy(self, response:HtmlResponse):`.\n",
    "Остаётся получить финальные данные. Наиминование вакансии `name = response.css('h1::text').get()` у css немного другаялогика и синтаксис чем у xpath. Берём тег h1 и у него читаем текстовый узел и извлекаем текст методом get().\n",
    "```\n",
    "<h1 data-qa=\"vacancy-title\" class=\"bloko-header-section-1\"><span>Разработчик </span><span class=\"highlighted\">Python</span><span></span></h1>\n",
    "```\n",
    "В режиме отладки удобно обращаться к `response.url` и смотреть где мы находимся на сайте. \n",
    "\n",
    "следующим соберём зарплату. Будем использовать xpath. Попробуем собрать всё текстовое содержимое тега div с учётом того что внутри тега имеются ещё дочерние теги. Для того чтобы собрать текст используем //text(). \n",
    "```\n",
    "<div data-qa=\"vacancy-salary\"><span data-qa=\"vacancy-salary-compensation-type-gross\" class=\"bloko-header-section-2 bloko-header-section-2_lite\">от <!-- -->57&nbsp;700<!-- --> до <!-- -->57&nbsp;700<!-- --> <!-- -->руб.<span class=\"vacancy-salary-compensation-type\"> <!-- -->до вычета налогов</span></span></div>\n",
    "```\n",
    "\n",
    "собираем данные по зарплате `salary = response.xpath('//div[@data-qa=\"vacancy-salary\"]//text()').getall()`\n",
    "\n",
    "как брать ссылку - сам объект. Мы только что перешли по ссылке, ведущей на сам объект (описание вакансии). `url = response.url`. \n",
    "\n",
    "Из этих собранных данных нам нужно теперь сформировать объект с этими данными внутри. Объектом который хронит всю информацию в структурном виде, но пока ещё не обработанном (зарплата приходит в грязном виде, её надо обрабатывать). В пауке мы не занимаемся преобразованием - это не его обязаность и не его работа. Не наружайте принцип solid. Принцип единой ответственности - каждый объект должен заниматься только одной задачей. Задача паука - парсинг, просто сбор данных, без обработки. Обработкой занимается другой объект. Как собрали в таком виде и передаём. Мы должны сформировать объект определённого класса, который описан в `items.py.` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6032ba",
   "metadata": {},
   "outputs": [],
   "source": [
    " def parse_vacancy(self, response:HtmlResponse):\n",
    "        name = response.css(\"h1::text\").get()\n",
    "        salary = response.xpath('//div[@data-qa=\"vacancy-salary\"]//text()').getall()\n",
    "        url = response.url\n",
    "        yield JobparserItem(name=name, salary=salary, url=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b655c7e",
   "metadata": {},
   "source": [
    "### Модуль items.py\n",
    "В файле `items.py` есть объект `JobparserItem`. Его назначение больше чем просто хранение данных, пока что воспринимаем его как контейнер для наших собранных данных. Он реализован как список свойств. Мы берём и говорим, что у нашего будущего Item будут поля name, salary, url, salary_min ... В которых будут хрониться соответствующие наименования. \n",
    "`name = scrapy.Field()` причём мы не указываем какие данные, какого типа, мы просто говорим, что это некий объект класса `scrapy.Field()`. Любое поле будет класса scrapy.Field(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobparserItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    name = scrapy.Field()\n",
    "    salary = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    salary_min = scrapy.Field()\n",
    "    salary_max = scrapy.Field()\n",
    "    currency = scrapy.Field()\n",
    "    _id = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c202d",
   "metadata": {},
   "source": [
    "К пауку нам теперь надо импортировать данный класс. В файле `spiders/hhru.py` делаем соответствующий импорт `from jobparser.items import JobparserItem`. И уже в функции (методе класса) `parse_vacancy` мы обращаемся к подключенному классу. И формируем этот самый объект `JobparserItem(name=name, salary=salary, url=url)` и не смотря на то что конструктор у нас тут не объявлен внутри данного класса, но он наследуется от `scrapy.Item` в котором конструктор этот объявлен. И суть его - он проходит по всем параметрам закинутым в конструктор и из них формирует определённым образом словарь. Поэтому в качестве параметров можем указывать любое количество элементов и все они запишутся в соответствующие свойства.  Главное провести соответствие `(name=name, salary=salary, url=url)`. name=, salary= - это наши свойства, атрибуты класса, а =salary, =name - это собранные нами данные соответствующих переменных. \n",
    "\n",
    "Экземпляр данного класса Item можем дальше передать по конвееру с помощью `yield`. Можно использовать и return, но рекомендации разработчиков скраппи использовать yield в силу многопоточниости и асинхронности. \n",
    "\n",
    "Таким образом формируется структура данных, которая разложена по свойствам отдельно взятого объекта.  \n",
    "\n",
    "Важно понимать что этот сформированный объект мы куда то отправили. `yield JobparserItem(name=name, salary=salary, url=url)`. Вопрос куда. Здесь мы переходим к следующему модулю, который занимается обработой данных. Этот модуль носит название `pipelines.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a367f",
   "metadata": {},
   "source": [
    "### Модуль pipelines.py\n",
    "Подразумивает под собой некий процесс от точки А до точки Б. Нам на вход приходит `Item`. Обратим внимание на `def process_item()` который имеет предопределённый метод, который принимает этот самый `Item`. Метод, а в глубинах скраппи отлавливается объект класса `JobparserItem` в моменте его выхода из паука и дальше передаёт его в popeline создавая предварительно объект данного класса и вызывая на каждый item метод `process_item` созданного объекта и передаёт его в качестве параметра внутрь данного метода. \n",
    "\n",
    "Единственное чтобы эта логика работала и всё было хорошо нам необходимо перейти в настройки `settings.py`, находим информацию о pipeline: \n",
    "\n",
    "`ITEM_PIPELINES = {\n",
    "   'jobparser.pipelines.JobparserPipeline': 300,\n",
    "}`\n",
    "\n",
    "и раскомментим эту информацию. Иначе у скраппи не будет информации о этом приемнике данных. По умолчанию прописан путь до нашего пайплайна `jobparser.pipelines.JobparserPipeline`.  Указание на класс который будет принимать данные от нашего паука. \n",
    "\n",
    "Мы получим item и должны будем его уже обработать\n",
    "```class Project1Pipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        return item\n",
    "```\n",
    "Обработать данные внутри, их переформировать, реконструировать, получить финальную готовую структуру, сложить её в базу данных. Это всё делается в pipeline. Любые обработки, любые дополнительные навязанные функции и т.д. В пауке этого не делаем. \n",
    "    \n",
    "Разработчики реализовали объект Item очень сходим с обычным словарём. Мы с полученым Item, хоть он и экземпляр класса `JobparserItem` можем с ним работать как со словарём. Надо получить наиминование - пишем `Item['name']`. Можно через метод get() который также поддерживают словари `Item.get('name')`.  Нас будет интересовать salary которое надо обработать и получить данные по зарплате. Получили список, закинули его в обработчик и получили что нам надо. Так как надо будет три данных собрать с зарплаты: минимальная, максимальная, валюта - надо будет заранее об этом позаботиться и создать соответствующие поля. На этапе сбора данных не обязательно собирать все поля. Часть полей заполнится после обработки данных в pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51a94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobparserItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    name = scrapy.Field()\n",
    "    salary = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    salary_min = scrapy.Field()\n",
    "    salary_max = scrapy.Field()\n",
    "    currency = scrapy.Field()\n",
    "    _id = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61244470",
   "metadata": {},
   "source": [
    "Например метод обработки зарплаты в pipeline. Он принимает в себя спарсенные данные по зарплате а дальше выделяет оттуда нужные данные и возвращает нам min, max, curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22bb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_salary(self, salary):\n",
    "    pass\n",
    "    return min_salary, max_salary, curr_salary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ae14c",
   "metadata": {},
   "source": [
    "Дальше внутрь нашего Item передаём эти данные в pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(self, item, spider):\n",
    "    item['salary_min'], item['salary_max'], item['currency'] = self.process_salary(item['salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ea215",
   "metadata": {},
   "source": [
    "В конечном счёте наш полученный Item должны сложить в базу данных. Возьмём паймонго. В проект не забываем поставить паймонго. `pip install pymongo`. в терминале ставим пакет. Не забываем импортировать пакет. `from pymongo import MongoClient` Далее у класса `class JobparserPipeline:` у нас не определён конструктор. Сам создадим его. В конструкторе создаём подключение к БД `client = MongoClient('localhost', 27017)` потому что объект пайплайн будет инициализироваться один раз за всё время работы нашего проекта. Дальше создаём базуданных `self.mongobase = client.vacancy3105`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45da0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobparserPipeline:\n",
    "    def __init__(self):\n",
    "        client = MongoClient('localhost', 27017)\n",
    "        self.mongobase = client.vacancy3105"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02829e49",
   "metadata": {},
   "source": [
    "В методе process_item когда это дело всё обработаем перед return item складываем всё это в базу данных. Формируем коллекцию  `collection = self.mongobase[spider.name]` в качестве имени коллекции мы укажем некое свойство паука. У нас в process_item вместе с самим Item приходит и паук. так как может быть несколько пауков, для того чтобы понимать какой паук принёс этот самый item  нам необходимо получать информацию и о пауке. Паук на данный момент является объектом класса hhruSpider и содержит в себе свойства: name = 'hhru', allowed_domains = ['hh.ru'], start_urls = ..., среди которых для нас полезным будет его имя, которое и укажем в `self.mongobase[spider.name]`. \n",
    "Мы теперь к полученной коллекции вызываем метод `insert_one` и закидываем наш Item. `collection.insert_one(item)`. Монго прекрасно работает с объектом Item у скраппи.  Хитрость `spider.name` в том что для каждого паука коллекции будут соответствующие свои коллекции. При нескольких пауках скрапи будет сам раскидывать по разным коллекциям. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3523d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def process_item(self, item, spider):\n",
    "        print()\n",
    "        if spider.name == 'hhru':\n",
    "            item['salary_min'], item['salary_max'], item['currency'] = self.process_salary(item['salary'])\n",
    "        ...\n",
    "        ...\n",
    "\n",
    "        ...\n",
    "\n",
    "        collection = self.mongobase[spider.name]\n",
    "        collection.insert_one(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f32b96",
   "metadata": {},
   "source": [
    "Монго при работе с объектом класса нужен будет айдишник для записи в коллекции. Поэтому не забываем позаботиться о наличии данного поля в itemsPipe `_id = scrapy.Field()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobparserItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    name = scrapy.Field()\n",
    "    salary = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    salary_min = scrapy.Field()\n",
    "    salary_max = scrapy.Field()\n",
    "    currency = scrapy.Field()\n",
    "    _id = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a324b6",
   "metadata": {},
   "source": [
    "# Файлы с урока"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01575612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "\n",
    "# Scrapy settings for jobparser project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'jobparser'\n",
    "\n",
    "SPIDER_MODULES = ['jobparser.spiders']\n",
    "NEWSPIDER_MODULE = 'jobparser.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.61 Safari/537.36'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "LOG_ENABLED = True\n",
    "LOG_LEVEL = \"DEBUG\"     # WARN ERROR INFO CRITICAL\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "CONCURRENT_REQUESTS = 16\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "DOWNLOAD_DELAY = 0\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "COOKIES_ENABLED = True\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'jobparser.middlewares.JobparserSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'jobparser.middlewares.JobparserDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "   'jobparser.pipelines.JobparserPipeline': 300,\n",
    "}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771132d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner.py \n",
    "\n",
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "from jobparser.spiders.hhru import HhruSpider\n",
    "# from jobparser.spiders.sjru import SJSpider\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    configure_logging()\n",
    "    settings = get_project_settings()\n",
    "    runner = CrawlerRunner(settings)\n",
    "\n",
    "    runner.crawl(HhruSpider)\n",
    "    # runner.crawl(SJSpider)\n",
    "    #\n",
    "    # d = runner.join()\n",
    "    # d.addBoth(lambda _: reactor.stop())\n",
    "\n",
    "    reactor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccc139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class JobparserPipeline:\n",
    "    def __init__(self):\n",
    "        client = MongoClient('localhost', 27017)\n",
    "        self.mongobase = client.vacancy3105\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        print()\n",
    "        if spider.name == 'hhru':\n",
    "            item['salary_min'], item['salary_max'], item['currency'] = self.process_salary(item['salary'])\n",
    "        ...\n",
    "        ...\n",
    "\n",
    "        ...\n",
    "\n",
    "        collection = self.mongobase[spider.name]\n",
    "        collection.insert_one(item)\n",
    "        return item\n",
    "\n",
    "    def process_salary(self, salary):\n",
    "        print(salary)\n",
    "        minv = 0\n",
    "        maxv = 100\n",
    "        cur = 'test'\n",
    "        return minv, maxv, cur\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9014a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# middlewares.py\n",
    "\n",
    "# Define here the models for your spider middleware\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "from scrapy import signals\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import is_item, ItemAdapter\n",
    "\n",
    "\n",
    "class JobparserSpiderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the spider middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        # Called for each response that goes through the spider\n",
    "        # middleware and into the spider.\n",
    "\n",
    "        # Should return None or raise an exception.\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        # Called with the results returned from the Spider, after\n",
    "        # it has processed the response.\n",
    "\n",
    "        # Must return an iterable of Request, or item objects.\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        # Called when a spider or process_spider_input() method\n",
    "        # (from other spider middleware) raises an exception.\n",
    "\n",
    "        # Should return either None or an iterable of Request or item objects.\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        # Called with the start requests of the spider, and works\n",
    "        # similarly to the process_spider_output() method, except\n",
    "        # that it doesn’t have a response associated.\n",
    "\n",
    "        # Must return only requests (not items).\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n",
    "class JobparserDownloaderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        # Called for each request that goes through the downloader\n",
    "        # middleware.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this request\n",
    "        # - or return a Response object\n",
    "        # - or return a Request object\n",
    "        # - or raise IgnoreRequest: process_exception() methods of\n",
    "        #   installed downloader middleware will be called\n",
    "        return None\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea77b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class JobparserItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    name = scrapy.Field()\n",
    "    salary = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    salary_min = scrapy.Field()\n",
    "    salary_max = scrapy.Field()\n",
    "    currency = scrapy.Field()\n",
    "    _id = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a78565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/__init__.py\n",
    "\n",
    "# This package will contain the spiders of your Scrapy project\n",
    "#\n",
    "# Please refer to the documentation for information on how to create and manage\n",
    "# your spiders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/hhru.py\n",
    "\n",
    "import scrapy\n",
    "from scrapy.http import HtmlResponse\n",
    "from jobparser.items import JobparserItem\n",
    "\n",
    "class HhruSpider(scrapy.Spider):\n",
    "    name = 'hhru'\n",
    "    allowed_domains = ['hh.ru']\n",
    "    start_urls = ['https://izhevsk.hh.ru/search/vacancy?area=1&search_field=name&search_field=company_name&search_field=description&text=python&from=suggest_post',\n",
    "                  'https://izhevsk.hh.ru/search/vacancy?area=2&search_field=name&search_field=company_name&search_field=description&text=python&from=suggest_post']\n",
    "\n",
    "    def parse(self, response: HtmlResponse):\n",
    "        next_page = response.xpath(\"//a[@data-qa='pager-next']/@href\").get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "        links = response.xpath('//a[@data-qa=\"vacancy-serp__vacancy-title\"]/@href').getall()\n",
    "        for link in links:\n",
    "            yield response.follow(link, callback=self.parse_vacancy)\n",
    "\n",
    "\n",
    "    def parse_vacancy(self, response:HtmlResponse):\n",
    "        name = response.css(\"h1::text\").get()\n",
    "        salary = response.xpath('//div[@data-qa=\"vacancy-salary\"]//text()').getall()\n",
    "        url = response.url\n",
    "        yield JobparserItem(name=name, salary=salary, url=url)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
