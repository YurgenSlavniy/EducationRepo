{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5351f322",
   "metadata": {},
   "source": [
    "# Методы сбора и обработки данных при помощи Python\n",
    "### На этом уроке \n",
    "1.\tНаучимся заполнять формы логинов scrapy\n",
    "2.\tНаучимся отправлять API запросы в Scrapy\n",
    "\n",
    "На прошлом уроке мы научились логиниться на сайт с использованием селениума. Теперь давайте изучим, как это делать при помощи скрайпи. Переходим на наш сайт quotes.toscrape.com/login. Открываем инспектор и переходим на вкладку «Сеть». Заполняем поля admin/admin и нажимаем login. Смотрите, у нас появился запрос с ответом 302, то есть перенаправление, и названием логин. От-крываем запрос, в заголовках вы видите адрес по которому был сделан запрос, метод - Post, нас интересует вкладка запрос.\n",
    "\n",
    "Как видите, вместе с запросом был отправлен массив, в котором есть такие ключи, как username, password и csrf_token. Откуда у нас юзернейм и пароль - понятно. Но давайте посмотрим, где получить этот токен. Снова открываем страницу с логином, проще всего вернуться назад. Смотрим разметку, попробуем ввести первые символы токена.\n",
    "\n",
    "Так, у нас нашелся тег с именем csrf_token. Отлично, это то, что нужно. Давайте еще проверим, появляется ли этот тег с токеном при отключенном JavaScript’е. F1, отключить джаваскрипт, обновляем страницу и ищем csrf_token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f67f81",
   "metadata": {},
   "source": [
    "Отлично, значит, мы можем использовать скрейпи без сплеша. Создадим новый проект\n",
    "\n",
    "> scrapy startproject login\n",
    "\n",
    "> cd login\n",
    "\n",
    "> scrapy genspider quotes quotes.toscrape.com\n",
    "\n",
    "Открываем файли паука. Первое, что надо сделать - импортировать библиотеку, которая позволит нам отправить post запрос.\n",
    "> from scrapy import FormRequest\n",
    "\n",
    "Меняем start_urls\n",
    "\n",
    ">start_urls = ['https://quotes.toscrape.com/login']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1642c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import FormRequest\n",
    "\n",
    "start_urls = ['https://quotes.toscrape.com/login']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832d528",
   "metadata": {},
   "source": [
    "Теперь в методе parse ищем токен\n",
    "\n",
    "`csrf_token = response.xpath(‘//input[@name=\"csrf_token\"]/@value').get()`\n",
    "                            \n",
    "И после того, как токен найден, отправляем запрос с помощью FormRequest\n",
    "```\n",
    "yield FormRequest.from_response(\n",
    "        response,- на какую ссылку отправлять запрос\n",
    "        formxpath=‘//form',- находим форму по xpath, так как у нас всего одна форма на странице, сделаем это так\n",
    "        formdata={\n",
    "            'csrf_token': csrf_token,\n",
    "            'username': 'admin',\n",
    "            'password': 'admin'\n",
    "        },- формируем данные, которые надо отправить\n",
    "        callback=self.after_login- после отправки запроса переходим к выполнению функции after_login\n",
    "    )\n",
    "```\n",
    "\n",
    "Давайте в функции after_login соберем все цитаты с первой страницы, то есть всего 10 штук. Не будем углубляться в сбор текстов и авторов.\n",
    "```\n",
    "def after_login(self, response):\n",
    "    quotes = response.xpath(\"//div[@class='quote']\")\n",
    "    print(f'Scrapy crawled {len(quotes)} quotes’)\n",
    "```\n",
    "\n",
    "Запускаем паука - всё верно. Но может быть такая ситуация, что токен появляется только после выполнения джава скриптаи сам логин выполняется при помощи javascript’а. Тогда вам надо использовать Splash. Давайте представим, что наш сайт именно такой и напишем еще один скрипт.\n",
    "\n",
    "Создадим нового паука \n",
    "> scrapy genspider js_login\n",
    "\n",
    "Импортируем библиотеки\n",
    "\n",
    "`from scrapy_splash import SplashRequest, SplashFormRequest`\n",
    "\n",
    "Удаляем start_urls. Теперь напишем скрипт для открытия страницы, точно такой же, какой мы писали на предыдущих уроках\n",
    "```\n",
    "script = '''\n",
    "        function main(splash, args)\n",
    "          assert(splash:go(args.url))\n",
    "          assert(splash:wait(0.5))\n",
    "          return splash:html()\n",
    "```\n",
    "\n",
    "затем создадим метод start_requests и внутри него вызовем сплеш реквест\n",
    "\n",
    "```\n",
    "def start_requests(self):\n",
    "        yield SplashRequest(\n",
    "            url='https://quotes.toscrape.com/login',\n",
    "            endpoint='execute',\n",
    "            args = {\n",
    "                'lua_source': self.script\n",
    "            },\n",
    "            callback=self.parse\n",
    "        )\n",
    "```\n",
    "\n",
    "Всё то же самое, что мы уже делали. Затем переходим к методу parse. Сначала находим наш токен. \n",
    "\n",
    "`csrf_token = response.xpath(‘//input[@name=\"csrf_token\"]/@value').get()`\n",
    "\n",
    "Затем отправляем форму при помощи класса SplashFormRequest\n",
    "```\n",
    "yield SplashFormRequest.from_response(\n",
    "            response,\n",
    "            formxpath='//form',\n",
    "            formdata={\n",
    "                'csrf_token': csrf_token,\n",
    "                'username': 'admin',\n",
    "                'password': 'admin'\n",
    "            },\n",
    "            callback=self.after_login\n",
    "        )\n",
    "```\n",
    "\n",
    "По сути, кроме класса отправка формы ничем не отличается от предыдущего скрипта. И наконец метод after_login, который мы просто скопируем из предыдущего скрипта.\n",
    "```\n",
    "def after_login(self, response):\n",
    "    quotes = response.xpath(\"//div[@class='quote']\")\n",
    "    print(f'Scrapy crawled {len(quotes)} quotes’)\n",
    "```\n",
    "\n",
    "Вот таким образом мы можем логиниться на сайт при помощи скрейпи.\n",
    "\n",
    "Последнее, что нам осталось затронуть - это работу с API при помощи Scrapy. Помните, на прошлом уроке, когда мы разбирались с бесконечным скролом в селениуме и отправляли команды в консоль, у нас там появлялся метод api? Сейчас вам еще раз покажу.\n",
    "Открываем https://quotes.toscrape.com/scroll и вкладку сеть в инспекторе. Включаем XHR, чтобы по-смотреть только апи запросы. Вообще апи позволяет легко и просто получать нужные данные легальным способом. Всегда сначала проверяйте эту вкладку при работе с сайтом, а уже потом пробйуте писать свой парсер. Бывает так, что по апи отдаются не все нужные данные, приходится переходить по ссылкам дальше, собирать айдишники и искать новые запросы. Но даже это проще, чем писать парсер html.\n",
    "Пролистаем страницу чуть вниз, чтобы подгрузились новые данные и видим, что у нас есть GET запросы\n",
    "\n",
    "Давайте посмотрим какой-нибудь. Видим, что в ответ приходит json, в котором есть несколько ключей, в том числе ключ quotes, в котором лежат словари с данными о цитатах, авторах и тегах.\n",
    "\n",
    "Отлично, теперь осталось понять, что меняется в запросе. Мы видим, что первый и второй запрос отличаются номером страницы в конце ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd9d30",
   "metadata": {},
   "source": [
    "Отлично, давайте создадим новый проект\n",
    "\n",
    "> scrapy startproject api\n",
    "\n",
    "> cd api\n",
    "\n",
    "> scrapy genspider quotes quotes.toscrape.com\n",
    "\n",
    "Открываем файл паука. В начале импортируем json, так как нам придется с ним работать, когда мы получим ответ от api.\n",
    "\n",
    "`import json`\n",
    "\n",
    "Затем изменим ссылку в start_urls на первую ссылку апи запроса\n",
    "\n",
    "`start_urls = [‘http://quotes.toscrape.com/api/quotes?page=1']`\n",
    "\n",
    "Теперь извлечем json из ответа\n",
    "\n",
    "`resp = json.loads(response.body)`\n",
    "\n",
    "Получим цитаты\n",
    "`\n",
    "`quotes = resp.get(‘quotes')`\n",
    "\n",
    "И вытащим данные из цитат. \n",
    "\n",
    "```\n",
    "for quote in quotes:\n",
    "Давайте посмотрим. Автор у нас лежит по ключу автор, а затем ключ имя\n",
    "yield {\n",
    "            'author': quote.get('author').get('name'),\n",
    "            'tags': quote.get('tags'),\n",
    "            'quote_text': quote.get('text')\n",
    "        }\n",
    "```\n",
    "\n",
    "Теги у нас лежат по ключу tags. Сама цитата лежит в ключе text.\n",
    "Теперь нам надо понять, каким образом мы получим все цитаты. Посмотрим внимательнее на ключи в ответе апи.\n",
    "Тут есть ключ has_next. Возможно, на последней странице этот ключ поменяет свое значение на false. Давайте изменим наш запрос. Добавим 10 страницу, так как из предыдущих уроков мы знаем, что страниц всего 10. \n",
    "В обычной жизни, когда вы не знаете точное количество страниц, вам бы, скорее всего, пришлось писать цикл while True и проверять этот ключ до тех пор, пока он не станет false. И еще надеяться, что разработчики не забыли и поменяли этот ключ на последней странице. У нас же с вами идеальные условия, так что меняем запрос и смотрим.\n",
    "Видим, что тут значение ключа has_next изменилось\n",
    "\n",
    "Отлично, давайте писать код. Сначала создадим переменную has_next и получим для нее значение\n",
    "\n",
    "`has_next = resp.get(‘has_next’)`\n",
    "\n",
    "Если эта переменная равна true, то делаем запрос по апи еще раз, только меняя номер страницы. Номер страницы мы будем так же получать из нашего ответа.\n",
    "```\n",
    "if has_next:\n",
    "    next_page_number = resp.get('page') + 1\n",
    "    yield scrapy.Request(\n",
    "        url=f'http://quotes.toscrape.com/api/quotes?page={next_page_number}',\n",
    "        callback=self.parse\n",
    "    )\n",
    "```\n",
    "\n",
    "Вот и всё. Запускаем пука. Ждем. В item_scraped_count собралась тысяча документов. Прекрасно. Мы с вами научились работать с апи.\n",
    "\n",
    "На этом наш курс подходит к концу. Я надеюсь, что вам понравился парсинг. Этих основ более чем достаточно для самостоятельно дальнейшего погружения в мир автоматического сбора данных. Ну, а при любых трудностях можете смело обращаться к Интернету, StackOverflow и ко мне =)\n",
    "\n",
    "### Домашнее задание\n",
    "Залогиниться на сайте, используя Scrapy. Вывести сообщение, которое появляется после логина (связка логин/пароль может быть любой).\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
